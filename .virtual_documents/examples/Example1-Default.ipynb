get_ipython().run_line_magic("cd", " -q ..")
get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")
get_ipython().run_line_magic("matplotlib", " inline")


import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)


import numpy as np
import pandas as pd
import torch
from torch import cuda
from torch.utils.data import DataLoader, Dataset
from tqdm.notebook import tqdm
from transformers import AutoModel, AutoTokenizer, BertTokenizer


from datasets import load_dataset

# getting newsgroups data from huggingface
train_data = pd.DataFrame(
    data=load_dataset("rungalileo/20_Newsgroups_Fixed", split="train")
)
test_data = pd.DataFrame(
    data=load_dataset("rungalileo/20_Newsgroups_Fixed", split="test")
)

train_data.drop(columns=["id"], axis=1, inplace=True)
test_data.drop(columns=["id"], axis=1, inplace=True)

# setting up pytorch device
if cuda.is_available():
    device = "cuda"
elif torch.has_mps:
    device = "mps"
else:
    device = "cpu"
    
device


class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l1 = AutoModel.from_pretrained("bert-base-cased")
        self.l2 = torch.nn.Linear(768, 20)

    def forward(self, ids, mask):
        output_1 = self.l1(ids, mask)
        output = self.l2(output_1[0][:, 0, :])  # use just the [CLS] output embedding
        return output


model = BERTClass()
model.to(device)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def clean_text(text):
    text = str(text)
    # text = text[text.index("\n\n") + 2 :]
    text = text.replace("\n", " ")
    text = text.replace("    ", " ")
    text = text.replace("   ", " ")
    text = text.replace("  ", " ")
    text = text.strip()
    return text


train_data.text = train_data.text.apply(lambda x: clean_text(x))
test_data.text = test_data.text.apply(lambda x: clean_text(x))


# convert labels to numeric
label_list = list(train_data.label.unique())

train_data.label = train_data.label.apply(lambda x: label_list.index(x))
test_data.label = test_data.label.apply(lambda x: label_list.index(x))

train_data


label_list


# Creating the loss function and optimizer
loss_function = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)


class EncodedSet(Dataset):
    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len):
        self.len = len(dataframe)
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len
        print(self.len)

    def __getitem__(self, index):
        text = str(self.data.text[index])
        inputs = self.tokenizer.encode_plus(
            text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_token_type_ids=True,
        )
        ids = inputs["input_ids"]
        mask = inputs["attention_mask"]

        return {
            "ids": torch.tensor(ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
            "targets": torch.tensor(self.data.label[index], dtype=torch.long),
        }

    def __len__(self):
        return self.len

train_data.reset_index(drop=True)
test_data.reset_index(drop=True)

train_set = EncodedSet(train_data, tokenizer, 256)
test_set = EncodedSet(test_data[:1000], tokenizer, 256)

train_params = {"batch_size": 16, "shuffle": True, "num_workers": 0}

test_params = {"batch_size": 2, "shuffle": True, "num_workers": 0}

# put everything into data loaders
train_loader = DataLoader(train_set, **train_params)
test_loader = DataLoader(test_set, **test_params)


def train(epoch):
    model.train()

    loss_history = []
    for _, data in tqdm(enumerate(train_loader, start=0), total=len(train_loader), desc=f"Epoch {epoch}"):
        ids = data["ids"].to(device, dtype=torch.long)
        mask = data["mask"].to(device, dtype=torch.long)
        targets = data["targets"].to(device, dtype=torch.long)

        outputs = model(ids, mask).squeeze()

        optimizer.zero_grad()
        loss = loss_function(outputs, targets)
        if _ % 100 == 0:
            print(f"Epoch: {epoch}, Loss:  {loss.item()}")
        loss_history.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    #         torch.cuda.empty_cache()
    return loss_history


losses = []
for epoch in range(1):
    losses.extend(train(epoch))


encodings = {}
for index, entry in enumerate(label_list):
    encodings[entry] = index
encodings


from tx2.dashboard import Dashboard
from tx2.wrapper import Wrapper


wrapper = Wrapper(
    train_texts=train_data.text,
    train_labels=train_data.label,
    test_texts=test_data.text[:2000],
    test_labels=test_data.label[:2000],
    encodings=encodings,
    classifier=model,
    language_model=model.l1,
    tokenizer=tokenizer,
    overwrite=True,
)
wrapper.prepare()


get_ipython().run_line_magic("matplotlib", " agg")
import matplotlib.pyplot as plt

dash = Dashboard(wrapper)
dash.render()


# wrapper.recompute_visual_clusterings("KMeans", clustering_args=dict(n_clusters=18))
# wrapper.recompute_visual_clusterings("OPTICS", clustering_args=dict())
# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())


wrapper.classify(["testing"])


wrapper.soft_classify(["testing"])


wrapper.embed(["testing"])


# cached data:
# wrapper.embeddings_training
# wrapper.embeddings_testing
# wrapper.projector
# wrapper.projections_training
# wrapper.projections_testing
# wrapper.salience_maps
# wrapper.clusters
# wrapper.cluster_profiles
# wrapper.cluster_words_freqs
# wrapper.cluster_class_word_sets

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Custom Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the custom approach as described in the \"Basic Usage\" docs. To demonstrate, we simply take the default functions for each and manually define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through sklearn via below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1= self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :]) # use just the [CLS] output embedding\n",
    "        return output\n",
    "    \n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text[text.index(\"\\n\\n\")+2:]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rows = []\n",
    "for i in range(len(train_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(train_data[\"data\"][i])\n",
    "    row[\"target\"] = train_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    train_rows.append(row)\n",
    "train_df = pd.DataFrame(train_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = []\n",
    "for i in range(len(test_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(test_data[\"data\"][i])\n",
    "    row[\"target\"] = test_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    test_rows.append(row)\n",
    "test_df = pd.DataFrame(test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11296\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "train_set = EncodedSet(train_df, tokenizer, 256)\n",
    "test_set = EncodedSet(test_df[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {'batch_size': 16,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 2,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    loss_history = []\n",
    "    for _,data in tqdm(enumerate(train_loader, 0), total=len(train_loader), desc=f\"Epoch {epoch}\"):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _%100==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c18b36c6304ee2b7100dd5fda65027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 0'), FloatProgress(value=0.0, max=706.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  3.1250662803649902\n",
      "Epoch: 0, Loss:  2.4787516593933105\n",
      "Epoch: 0, Loss:  1.00886070728302\n",
      "Epoch: 0, Loss:  1.0574833154678345\n",
      "Epoch: 0, Loss:  1.0020537376403809\n",
      "Epoch: 0, Loss:  1.3189597129821777\n",
      "Epoch: 0, Loss:  0.8657354116439819\n",
      "Epoch: 0, Loss:  1.140439748764038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = {}\n",
    "for index, entry in enumerate(train_data[\"target_names\"]):\n",
    "    encodings[entry] = index\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tx2.wrapper import Wrapper\n",
    "from tx2.dashboard import Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tx2 import utils\n",
    "\n",
    "def custom_encoding_function(text):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(encoded[\"input_ids\"], device=device),\n",
    "        \"attention_mask\": torch.tensor(encoded[\"attention_mask\"], device=device),\n",
    "    }\n",
    "\n",
    "def custom_classification_function(inputs):\n",
    "    return torch.argmax(model(inputs[\"input_ids\"], inputs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "def custom_embedding_function(inputs):\n",
    "    return model.l1(inputs[\"input_ids\"], inputs[\"attention_mask\"])[0][:, 0, :]  # [CLS] token embedding\n",
    "\n",
    "def custom_soft_classification_function(inputs):\n",
    "    return model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cache path found\n",
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:cached version 'data/custom_cache/predictions.json' found\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:cached version 'data/custom_cache/embedding_training.json' found\n",
      "INFO:root:cached version 'data/custom_cache/embedding_testing.json' found\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:cached version 'data/custom_cache/projections_training.json' found\n",
      "INFO:root:cached version 'data/custom_cache/projections_testing.json' found\n",
      "INFO:root:cached version 'data/custom_cache/projector.pkl.gz' found\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:cached version 'data/custom_cache/salience.pkl.gz' found\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:cached version 'data/custom_cache/cluster_profiles.pkl.gz' found\n",
      "INFO:root:cached version 'data/custom_cache/clusters.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_labels.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_words.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_class_words.json' found\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_df=train_df, \n",
    "    test_df=test_df[:2000], \n",
    "    encodings=encodings, \n",
    "    input_col_name=\"text\", \n",
    "    target_col_name=\"target\", \n",
    "    cache_path=\"data/custom_cache\",\n",
    "    overwrite=False\n",
    ")\n",
    "wrapper.encode_function = custom_encoding_function\n",
    "wrapper.classification_function = custom_classification_function\n",
    "wrapper.soft_classification_function = custom_soft_classification_function\n",
    "wrapper.embedding_function = custom_embedding_function\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dbb589855444c891a671999ca5222b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "dash = Dashboard(wrapper, show_wordclouds=True)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper.recompute_visual_clusterings(dbscan_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.274680256843567,\n",
       "  -0.7278291583061218,\n",
       "  -0.5961906313896179,\n",
       "  -0.7103647589683533,\n",
       "  0.06959213316440582,\n",
       "  -0.3663700819015503,\n",
       "  0.845853865146637,\n",
       "  1.6216564178466797,\n",
       "  0.9479706883430481,\n",
       "  -0.49676382541656494,\n",
       "  -1.1853022575378418,\n",
       "  -0.8583256006240845,\n",
       "  0.9677442312240601,\n",
       "  0.6286348104476929,\n",
       "  -0.207204669713974,\n",
       "  0.024738451465964317,\n",
       "  -0.07436274737119675,\n",
       "  -1.0065152645111084,\n",
       "  -0.15594995021820068,\n",
       "  -1.3550989627838135]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.2659216523170471,\n",
       "  0.38137850165367126,\n",
       "  -0.04486016556620598,\n",
       "  -0.8704298734664917,\n",
       "  -0.8540533781051636,\n",
       "  0.07706466317176819,\n",
       "  -0.27307143807411194,\n",
       "  0.3006805181503296,\n",
       "  0.39764225482940674,\n",
       "  -1.4018787145614624,\n",
       "  0.19122491776943207,\n",
       "  -0.16736215353012085,\n",
       "  -0.03104136325418949,\n",
       "  0.20715676248073578,\n",
       "  -0.8335903286933899,\n",
       "  0.5720852613449097,\n",
       "  0.3918491005897522,\n",
       "  0.007387924008071423,\n",
       "  0.45158833265304565,\n",
       "  0.12926512956619263,\n",
       "  -0.20284724235534668,\n",
       "  -0.04778043180704117,\n",
       "  0.6477566361427307,\n",
       "  -1.0826544761657715,\n",
       "  -0.012331757694482803,\n",
       "  -0.8028337955474854,\n",
       "  -0.12404359877109528,\n",
       "  0.7824350595474243,\n",
       "  -0.17032234370708466,\n",
       "  0.6228926777839661,\n",
       "  -0.0209670253098011,\n",
       "  0.8089502453804016,\n",
       "  0.2907910943031311,\n",
       "  -0.7856619358062744,\n",
       "  0.07611598819494247,\n",
       "  -0.8406522274017334,\n",
       "  0.02481873705983162,\n",
       "  0.40354597568511963,\n",
       "  -0.19055405259132385,\n",
       "  -0.0044200224801898,\n",
       "  -0.254119336605072,\n",
       "  -1.407018780708313,\n",
       "  0.7128382921218872,\n",
       "  0.4632900357246399,\n",
       "  0.7170988917350769,\n",
       "  0.5461415648460388,\n",
       "  -0.30304786562919617,\n",
       "  -0.030608780682086945,\n",
       "  -0.4255700707435608,\n",
       "  -0.1390867531299591,\n",
       "  -0.1771538257598877,\n",
       "  0.1347309648990631,\n",
       "  -0.6360343098640442,\n",
       "  1.317735195159912,\n",
       "  -0.7105672359466553,\n",
       "  -0.2354573905467987,\n",
       "  0.3484898507595062,\n",
       "  0.06303907930850983,\n",
       "  0.09918534755706787,\n",
       "  0.43477514386177063,\n",
       "  -0.3954809010028839,\n",
       "  0.00017757085151970387,\n",
       "  0.5359960794448853,\n",
       "  -0.32405412197113037,\n",
       "  0.3642837107181549,\n",
       "  -0.18386337161064148,\n",
       "  0.19124363362789154,\n",
       "  -0.32065489888191223,\n",
       "  -0.15745244920253754,\n",
       "  -0.6582170128822327,\n",
       "  0.20769894123077393,\n",
       "  0.04761604592204094,\n",
       "  0.5525169372558594,\n",
       "  0.4016570448875427,\n",
       "  0.5162539482116699,\n",
       "  -0.06983271986246109,\n",
       "  0.1296120584011078,\n",
       "  -0.6440904140472412,\n",
       "  0.03186437115073204,\n",
       "  0.6648545861244202,\n",
       "  0.7027866840362549,\n",
       "  0.4849835932254791,\n",
       "  -0.47393110394477844,\n",
       "  -0.034544069319963455,\n",
       "  0.007189668249338865,\n",
       "  -0.5631494522094727,\n",
       "  0.42950958013534546,\n",
       "  -0.48875874280929565,\n",
       "  -0.09051654487848282,\n",
       "  -0.12609679996967316,\n",
       "  -0.19429507851600647,\n",
       "  0.07540503144264221,\n",
       "  -1.1302354335784912,\n",
       "  0.1726861447095871,\n",
       "  -0.14436624944210052,\n",
       "  0.2658878564834595,\n",
       "  0.5250715613365173,\n",
       "  0.5367235541343689,\n",
       "  3.1754143238067627,\n",
       "  -0.08895736187696457,\n",
       "  0.08702458441257477,\n",
       "  -0.9722351431846619,\n",
       "  0.1526191234588623,\n",
       "  -0.2948012948036194,\n",
       "  0.5159106254577637,\n",
       "  -0.6361156105995178,\n",
       "  -0.036204028874635696,\n",
       "  -0.05327813699841499,\n",
       "  0.6486550569534302,\n",
       "  0.6927029490470886,\n",
       "  0.49658483266830444,\n",
       "  -0.24149392545223236,\n",
       "  0.5096801519393921,\n",
       "  0.9100965261459351,\n",
       "  0.2731000781059265,\n",
       "  0.1702711284160614,\n",
       "  -0.6422144174575806,\n",
       "  0.4380118250846863,\n",
       "  -0.5248681306838989,\n",
       "  0.09155849367380142,\n",
       "  0.563450813293457,\n",
       "  0.4463476240634918,\n",
       "  1.169932246208191,\n",
       "  -0.6423324346542358,\n",
       "  -0.5275729298591614,\n",
       "  -0.6002805233001709,\n",
       "  -0.21006499230861664,\n",
       "  -0.3135540783405304,\n",
       "  0.02585785463452339,\n",
       "  -0.42637622356414795,\n",
       "  -0.6523085832595825,\n",
       "  0.10084953904151917,\n",
       "  -0.6985015869140625,\n",
       "  -0.16847562789916992,\n",
       "  0.0864361822605133,\n",
       "  0.239823579788208,\n",
       "  -1.0967153310775757,\n",
       "  0.248581662774086,\n",
       "  -2.450028896331787,\n",
       "  0.13407178223133087,\n",
       "  -0.5618208646774292,\n",
       "  -0.04004904627799988,\n",
       "  -0.01695127598941326,\n",
       "  -0.4592221975326538,\n",
       "  -0.5169054865837097,\n",
       "  0.8774344325065613,\n",
       "  0.872680127620697,\n",
       "  0.2289307713508606,\n",
       "  0.19411982595920563,\n",
       "  -0.2413574904203415,\n",
       "  1.3959273099899292,\n",
       "  1.0784939527511597,\n",
       "  -0.20936286449432373,\n",
       "  0.48675301671028137,\n",
       "  0.3555550277233124,\n",
       "  -0.7086731791496277,\n",
       "  -0.03972606733441353,\n",
       "  -0.5233350992202759,\n",
       "  -0.27849942445755005,\n",
       "  1.003583550453186,\n",
       "  -0.04393612965941429,\n",
       "  -0.3495709002017975,\n",
       "  -0.20060862600803375,\n",
       "  0.19719916582107544,\n",
       "  1.1925747394561768,\n",
       "  -0.7113004922866821,\n",
       "  0.575481116771698,\n",
       "  -0.5770688652992249,\n",
       "  0.5764034390449524,\n",
       "  0.553798258304596,\n",
       "  0.44035643339157104,\n",
       "  -0.7723311185836792,\n",
       "  -0.8147573471069336,\n",
       "  1.5596915483474731,\n",
       "  0.043146226555109024,\n",
       "  0.15032154321670532,\n",
       "  -0.4937763512134552,\n",
       "  0.5771908760070801,\n",
       "  -0.4861961007118225,\n",
       "  -0.09171725064516068,\n",
       "  -0.8005693554878235,\n",
       "  1.1400145292282104,\n",
       "  0.315947562456131,\n",
       "  -0.22666332125663757,\n",
       "  -0.6644637584686279,\n",
       "  -0.027930138632655144,\n",
       "  0.01993047073483467,\n",
       "  -0.0883978009223938,\n",
       "  0.7877101302146912,\n",
       "  -0.011999017558991909,\n",
       "  0.02127234824001789,\n",
       "  -0.4325181841850281,\n",
       "  -0.30503079295158386,\n",
       "  0.06344784051179886,\n",
       "  0.2751242220401764,\n",
       "  -0.5697043538093567,\n",
       "  -0.07426142692565918,\n",
       "  0.644000768661499,\n",
       "  -0.9631829261779785,\n",
       "  -0.19040626287460327,\n",
       "  -0.06127405911684036,\n",
       "  0.21783754229545593,\n",
       "  -1.0075995922088623,\n",
       "  -0.05295148119330406,\n",
       "  1.0991843938827515,\n",
       "  -0.3584323823451996,\n",
       "  -0.142536923289299,\n",
       "  0.11933821439743042,\n",
       "  0.32732218503952026,\n",
       "  0.24121709167957306,\n",
       "  -0.20950597524642944,\n",
       "  0.6252950429916382,\n",
       "  0.4615071713924408,\n",
       "  0.6960494518280029,\n",
       "  0.2249688357114792,\n",
       "  -0.8241540193557739,\n",
       "  0.4806581139564514,\n",
       "  0.22373345494270325,\n",
       "  0.13621865212917328,\n",
       "  0.5155930519104004,\n",
       "  -0.852088987827301,\n",
       "  -0.6282359957695007,\n",
       "  -1.1155911684036255,\n",
       "  -0.4847099483013153,\n",
       "  0.7703961133956909,\n",
       "  -0.01741301454603672,\n",
       "  0.623882532119751,\n",
       "  -0.853807806968689,\n",
       "  0.3297392725944519,\n",
       "  0.231063112616539,\n",
       "  0.20693936944007874,\n",
       "  0.7657411098480225,\n",
       "  -0.004288881551474333,\n",
       "  -0.5588797330856323,\n",
       "  0.4349226951599121,\n",
       "  -0.19022995233535767,\n",
       "  -1.1228184700012207,\n",
       "  1.2010668516159058,\n",
       "  -0.3055647313594818,\n",
       "  -0.014616734348237514,\n",
       "  0.06733832508325577,\n",
       "  -0.9135932326316833,\n",
       "  0.717740535736084,\n",
       "  0.7332141995429993,\n",
       "  -0.02177213318645954,\n",
       "  -0.08316284418106079,\n",
       "  0.5998499989509583,\n",
       "  -0.048085276037454605,\n",
       "  0.02527502551674843,\n",
       "  -0.25874805450439453,\n",
       "  0.5367496013641357,\n",
       "  -0.23665457963943481,\n",
       "  0.3781035840511322,\n",
       "  -0.7946584224700928,\n",
       "  0.41816818714141846,\n",
       "  -0.4270715117454529,\n",
       "  0.49732914566993713,\n",
       "  -0.7784280180931091,\n",
       "  0.16026608645915985,\n",
       "  -1.022326111793518,\n",
       "  0.08538872003555298,\n",
       "  -1.329710602760315,\n",
       "  0.29581737518310547,\n",
       "  -0.14172467589378357,\n",
       "  -0.22814929485321045,\n",
       "  -0.2181110978126526,\n",
       "  -0.5362197756767273,\n",
       "  0.32655417919158936,\n",
       "  0.2222515195608139,\n",
       "  -0.11795247346162796,\n",
       "  -0.21007129549980164,\n",
       "  -0.9905583262443542,\n",
       "  -0.45371004939079285,\n",
       "  0.027186306193470955,\n",
       "  0.36741602420806885,\n",
       "  0.06676528602838516,\n",
       "  0.7204890251159668,\n",
       "  -0.9280092120170593,\n",
       "  -0.08149226754903793,\n",
       "  0.189218208193779,\n",
       "  -0.22413653135299683,\n",
       "  -0.12427505105733871,\n",
       "  -0.18557102978229523,\n",
       "  0.3148443102836609,\n",
       "  0.3418164551258087,\n",
       "  0.6798467636108398,\n",
       "  0.39306700229644775,\n",
       "  -0.9237082004547119,\n",
       "  -0.3379925787448883,\n",
       "  2.1379642486572266,\n",
       "  1.0301252603530884,\n",
       "  -0.13120995461940765,\n",
       "  0.270844042301178,\n",
       "  -0.24527864158153534,\n",
       "  -0.17698022723197937,\n",
       "  -0.592531144618988,\n",
       "  -0.04947078227996826,\n",
       "  0.0840660110116005,\n",
       "  -0.39251336455345154,\n",
       "  -0.5106673836708069,\n",
       "  0.22860029339790344,\n",
       "  0.2588082253932953,\n",
       "  -0.9662436842918396,\n",
       "  -0.10533981025218964,\n",
       "  -0.9427086710929871,\n",
       "  0.39800363779067993,\n",
       "  -0.17460647225379944,\n",
       "  0.5734262466430664,\n",
       "  -0.251616895198822,\n",
       "  -0.76189786195755,\n",
       "  0.2708747684955597,\n",
       "  -0.45290377736091614,\n",
       "  -0.09103553742170334,\n",
       "  -0.00690776202827692,\n",
       "  -0.40012499690055847,\n",
       "  -0.08342903852462769,\n",
       "  -0.5394442081451416,\n",
       "  0.9768339395523071,\n",
       "  -0.06819917261600494,\n",
       "  -0.24242457747459412,\n",
       "  0.35010871291160583,\n",
       "  0.2641657590866089,\n",
       "  -0.28417158126831055,\n",
       "  -0.36698049306869507,\n",
       "  -1.035037875175476,\n",
       "  0.12612247467041016,\n",
       "  0.5230520367622375,\n",
       "  0.013232178054749966,\n",
       "  -1.4159423112869263,\n",
       "  1.1547651290893555,\n",
       "  -0.13507331907749176,\n",
       "  -0.29526224732398987,\n",
       "  -0.6405667662620544,\n",
       "  0.7410721182823181,\n",
       "  0.027971168980002403,\n",
       "  0.10841548442840576,\n",
       "  0.4173315167427063,\n",
       "  -0.040864575654268265,\n",
       "  -0.19489935040473938,\n",
       "  -0.46621912717819214,\n",
       "  0.9665723443031311,\n",
       "  0.6321106553077698,\n",
       "  0.06680114567279816,\n",
       "  0.14982131123542786,\n",
       "  0.4657164216041565,\n",
       "  0.38847973942756653,\n",
       "  0.29238030314445496,\n",
       "  -0.54771488904953,\n",
       "  1.0260961055755615,\n",
       "  -0.3058488070964813,\n",
       "  0.721298098564148,\n",
       "  -0.6415988206863403,\n",
       "  -1.1677570343017578,\n",
       "  0.5513172149658203,\n",
       "  0.7722039818763733,\n",
       "  -0.34187084436416626,\n",
       "  -0.40639039874076843,\n",
       "  -0.0724862813949585,\n",
       "  -0.07490687817335129,\n",
       "  0.49423202872276306,\n",
       "  -0.5386857986450195,\n",
       "  -0.9656047821044922,\n",
       "  0.776760995388031,\n",
       "  -0.06262960284948349,\n",
       "  0.3682810962200165,\n",
       "  0.06641993671655655,\n",
       "  -0.2607279121875763,\n",
       "  -0.014069548808038235,\n",
       "  0.6264200210571289,\n",
       "  -0.08991669863462448,\n",
       "  -0.039394132792949677,\n",
       "  -0.19200749695301056,\n",
       "  -0.16328111290931702,\n",
       "  0.4389214515686035,\n",
       "  0.3475152850151062,\n",
       "  -0.5957750678062439,\n",
       "  0.7191671133041382,\n",
       "  0.7975821495056152,\n",
       "  0.3648269474506378,\n",
       "  -0.25357410311698914,\n",
       "  -0.028177766129374504,\n",
       "  0.11729741841554642,\n",
       "  0.3690453767776489,\n",
       "  0.06782644987106323,\n",
       "  0.20979952812194824,\n",
       "  -0.1573060005903244,\n",
       "  0.6706221103668213,\n",
       "  -0.12899550795555115,\n",
       "  -1.0889991521835327,\n",
       "  0.2841164469718933,\n",
       "  0.7573808431625366,\n",
       "  -0.03453053906559944,\n",
       "  0.1312873214483261,\n",
       "  -0.37198907136917114,\n",
       "  -0.13660715520381927,\n",
       "  -0.05610351264476776,\n",
       "  0.6183323860168457,\n",
       "  0.204417884349823,\n",
       "  0.22255705296993256,\n",
       "  -0.608475387096405,\n",
       "  -0.277224063873291,\n",
       "  0.30102285742759705,\n",
       "  -0.16833804547786713,\n",
       "  0.19776180386543274,\n",
       "  -0.2545749545097351,\n",
       "  1.499752163887024,\n",
       "  -0.3427667021751404,\n",
       "  -0.27770349383354187,\n",
       "  -0.5872185826301575,\n",
       "  0.026926543563604355,\n",
       "  0.48562756180763245,\n",
       "  -0.30149364471435547,\n",
       "  -0.3780354857444763,\n",
       "  -0.6158023476600647,\n",
       "  0.09375699609518051,\n",
       "  -0.6505525708198547,\n",
       "  -0.23901014029979706,\n",
       "  -0.29798853397369385,\n",
       "  0.5534766912460327,\n",
       "  0.0018448978662490845,\n",
       "  0.020876072347164154,\n",
       "  -0.09497763216495514,\n",
       "  0.5462908148765564,\n",
       "  -1.3709559440612793,\n",
       "  0.35754257440567017,\n",
       "  0.06359362602233887,\n",
       "  0.3841097354888916,\n",
       "  -0.3759379982948303,\n",
       "  -0.12432847172021866,\n",
       "  0.18042291700839996,\n",
       "  -0.009447317570447922,\n",
       "  0.5575748085975647,\n",
       "  0.3125819265842438,\n",
       "  0.06828469783067703,\n",
       "  -0.13314762711524963,\n",
       "  0.4895821213722229,\n",
       "  -0.35673701763153076,\n",
       "  -1.1220264434814453,\n",
       "  0.29497823119163513,\n",
       "  2.101321220397949,\n",
       "  -0.09056928008794785,\n",
       "  0.09355010837316513,\n",
       "  -0.45223483443260193,\n",
       "  -0.44167283177375793,\n",
       "  -0.3720436096191406,\n",
       "  -0.023506494238972664,\n",
       "  -0.6560002565383911,\n",
       "  0.12898650765419006,\n",
       "  0.5756826400756836,\n",
       "  -0.6693305373191833,\n",
       "  -1.005649447441101,\n",
       "  0.1916056126356125,\n",
       "  -0.018354976549744606,\n",
       "  -0.32261228561401367,\n",
       "  0.6773999333381653,\n",
       "  -0.13881650567054749,\n",
       "  0.8288539052009583,\n",
       "  0.10406302660703659,\n",
       "  -0.2051362246274948,\n",
       "  0.23768359422683716,\n",
       "  0.09652423113584518,\n",
       "  0.41026759147644043,\n",
       "  -0.036516617983579636,\n",
       "  0.11062364280223846,\n",
       "  0.9965822696685791,\n",
       "  0.7995328307151794,\n",
       "  -0.41529086232185364,\n",
       "  0.1557336300611496,\n",
       "  -0.5079215168952942,\n",
       "  -0.15306031703948975,\n",
       "  -0.17859064042568207,\n",
       "  -0.3558792173862457,\n",
       "  -0.7727926969528198,\n",
       "  0.5238412022590637,\n",
       "  -0.7004234194755554,\n",
       "  0.4779089093208313,\n",
       "  0.753928542137146,\n",
       "  -0.06936538219451904,\n",
       "  0.48962172865867615,\n",
       "  -0.08741683512926102,\n",
       "  -0.027341481298208237,\n",
       "  0.25100943446159363,\n",
       "  -0.7284023761749268,\n",
       "  0.5176091194152832,\n",
       "  0.23776787519454956,\n",
       "  0.17950314283370972,\n",
       "  0.47707435488700867,\n",
       "  0.1401124745607376,\n",
       "  0.06885883212089539,\n",
       "  0.28179290890693665,\n",
       "  0.27996429800987244,\n",
       "  0.9054209589958191,\n",
       "  -0.75190269947052,\n",
       "  0.2131679654121399,\n",
       "  0.700835645198822,\n",
       "  -0.08849231898784637,\n",
       "  -0.16629070043563843,\n",
       "  -0.12674245238304138,\n",
       "  -0.42990756034851074,\n",
       "  -0.3589460551738739,\n",
       "  -0.215152308344841,\n",
       "  -0.05538027733564377,\n",
       "  -0.4055336117744446,\n",
       "  0.22069516777992249,\n",
       "  -0.25368228554725647,\n",
       "  0.4667356312274933,\n",
       "  -0.42625847458839417,\n",
       "  0.5969623923301697,\n",
       "  0.003756002988666296,\n",
       "  -0.0680563822388649,\n",
       "  0.15061113238334656,\n",
       "  -0.8300287127494812,\n",
       "  0.23607592284679413,\n",
       "  0.9382079243659973,\n",
       "  -0.6759952902793884,\n",
       "  0.26504358649253845,\n",
       "  -0.037982091307640076,\n",
       "  -0.8678779602050781,\n",
       "  0.0013517345068976283,\n",
       "  -0.5713436603546143,\n",
       "  0.42157256603240967,\n",
       "  -0.16448131203651428,\n",
       "  -0.14673371613025665,\n",
       "  0.6329652070999146,\n",
       "  0.16557203233242035,\n",
       "  0.5838711857795715,\n",
       "  0.03278767317533493,\n",
       "  -0.4973331093788147,\n",
       "  0.6052947044372559,\n",
       "  -0.5118674635887146,\n",
       "  -0.35423997044563293,\n",
       "  -0.06687607616186142,\n",
       "  -0.5274540781974792,\n",
       "  -0.22544510662555695,\n",
       "  -0.1742940992116928,\n",
       "  0.673364520072937,\n",
       "  -0.19331075251102448,\n",
       "  -0.7598954439163208,\n",
       "  0.06620730459690094,\n",
       "  0.10200229287147522,\n",
       "  -0.08352258056402206,\n",
       "  -0.11841315776109695,\n",
       "  0.1439165472984314,\n",
       "  -0.016295574605464935,\n",
       "  -0.07612577825784683,\n",
       "  0.31964582204818726,\n",
       "  0.07519341260194778,\n",
       "  -0.2605496346950531,\n",
       "  -0.018617277964949608,\n",
       "  -0.21027173101902008,\n",
       "  -0.4915948808193207,\n",
       "  0.5426371097564697,\n",
       "  -0.8173994421958923,\n",
       "  0.6952602863311768,\n",
       "  1.5866141319274902,\n",
       "  0.6124350428581238,\n",
       "  -0.3054993748664856,\n",
       "  -6.901821613311768,\n",
       "  0.18606609106063843,\n",
       "  0.3314770460128784,\n",
       "  0.17544858157634735,\n",
       "  -0.396089106798172,\n",
       "  -0.554805338382721,\n",
       "  0.06944956630468369,\n",
       "  0.44752785563468933,\n",
       "  -0.07102156430482864,\n",
       "  -0.40611204504966736,\n",
       "  -0.41814613342285156,\n",
       "  -0.04732615128159523,\n",
       "  -1.1221898794174194,\n",
       "  0.3952254056930542,\n",
       "  -0.3195718824863434,\n",
       "  0.08757639676332474,\n",
       "  0.4258606433868408,\n",
       "  -1.4276877641677856,\n",
       "  -0.7673832774162292,\n",
       "  0.23412318527698517,\n",
       "  0.11661756038665771,\n",
       "  -0.21056321263313293,\n",
       "  0.2340705394744873,\n",
       "  0.23882129788398743,\n",
       "  0.2298002541065216,\n",
       "  -0.36516550183296204,\n",
       "  -0.6801757216453552,\n",
       "  -0.41052213311195374,\n",
       "  -0.2803710103034973,\n",
       "  0.36347588896751404,\n",
       "  -0.7137009501457214,\n",
       "  -0.20099598169326782,\n",
       "  0.1779998242855072,\n",
       "  -0.263090580701828,\n",
       "  -0.39874276518821716,\n",
       "  -0.31121641397476196,\n",
       "  0.7151867151260376,\n",
       "  -0.20118393003940582,\n",
       "  1.0815889835357666,\n",
       "  -0.126031756401062,\n",
       "  0.08153124153614044,\n",
       "  -0.7202317714691162,\n",
       "  0.5948665738105774,\n",
       "  0.16259057819843292,\n",
       "  -0.5681498646736145,\n",
       "  -0.039638858288526535,\n",
       "  -0.36618003249168396,\n",
       "  -0.7475693225860596,\n",
       "  0.1601448953151703,\n",
       "  -0.6158109903335571,\n",
       "  1.438932180404663,\n",
       "  0.31001898646354675,\n",
       "  -0.33954471349716187,\n",
       "  -0.24061256647109985,\n",
       "  -0.4281553328037262,\n",
       "  0.24620026350021362,\n",
       "  0.38525402545928955,\n",
       "  0.33323439955711365,\n",
       "  0.4720194339752197,\n",
       "  0.2853662967681885,\n",
       "  -0.8878185153007507,\n",
       "  -0.48224738240242004,\n",
       "  0.6829046607017517,\n",
       "  0.25674042105674744,\n",
       "  -0.3592662513256073,\n",
       "  0.2675623893737793,\n",
       "  0.1712660938501358,\n",
       "  -0.5922044515609741,\n",
       "  -0.11020585149526596,\n",
       "  -0.41272613406181335,\n",
       "  0.11775007843971252,\n",
       "  0.09607122093439102,\n",
       "  -0.894696831703186,\n",
       "  -0.4394049644470215,\n",
       "  -0.01843355782330036,\n",
       "  -0.13879187405109406,\n",
       "  -0.4853432774543762,\n",
       "  -0.0064130378887057304,\n",
       "  0.16936254501342773,\n",
       "  0.6264346241950989,\n",
       "  -0.6791065335273743,\n",
       "  0.6845179200172424,\n",
       "  0.3207862079143524,\n",
       "  0.839804470539093,\n",
       "  -0.038251783698797226,\n",
       "  0.13372685015201569,\n",
       "  -0.6405892968177795,\n",
       "  0.1177494153380394,\n",
       "  -0.4086206257343292,\n",
       "  0.5634811520576477,\n",
       "  0.5248701572418213,\n",
       "  0.5346519351005554,\n",
       "  0.15120501816272736,\n",
       "  0.5310313105583191,\n",
       "  -0.3186365067958832,\n",
       "  0.2591599225997925,\n",
       "  -0.016276679933071136,\n",
       "  -0.5360355973243713,\n",
       "  0.21926626563072205,\n",
       "  -0.1635276824235916,\n",
       "  -0.25581949949264526,\n",
       "  0.40145328640937805,\n",
       "  0.013555937446653843,\n",
       "  -0.5219651460647583,\n",
       "  0.8445917367935181,\n",
       "  0.6729480624198914,\n",
       "  1.822329044342041,\n",
       "  0.42095234990119934,\n",
       "  -0.08323896676301956,\n",
       "  0.7466345429420471,\n",
       "  -0.7776318192481995,\n",
       "  0.06656476110219955,\n",
       "  -1.1134315729141235,\n",
       "  0.20445378124713898,\n",
       "  1.1883732080459595,\n",
       "  0.8730736374855042,\n",
       "  -0.3684697449207306,\n",
       "  -0.23487816751003265,\n",
       "  0.32593879103660583,\n",
       "  -0.2256828248500824,\n",
       "  0.8530217409133911,\n",
       "  0.04793451726436615,\n",
       "  -0.0830393061041832,\n",
       "  0.17866401374340057,\n",
       "  0.4339451491832733,\n",
       "  -0.6558839082717896,\n",
       "  -0.02361476607620716,\n",
       "  -0.7880495190620422,\n",
       "  0.011305240914225578,\n",
       "  -0.32863160967826843,\n",
       "  -0.03391837328672409,\n",
       "  0.11021295189857483,\n",
       "  -0.7787360548973083,\n",
       "  -0.07574210315942764,\n",
       "  0.005855909548699856,\n",
       "  -1.0949901342391968,\n",
       "  0.09701758623123169,\n",
       "  0.5988472104072571,\n",
       "  0.561527669429779,\n",
       "  0.6872393488883972,\n",
       "  0.17551273107528687,\n",
       "  0.8997986316680908,\n",
       "  -0.07985664904117584,\n",
       "  -0.4921325147151947,\n",
       "  0.4992198646068573,\n",
       "  -0.6628105044364929,\n",
       "  0.11652333289384842,\n",
       "  0.3133540153503418,\n",
       "  -1.151768684387207,\n",
       "  -0.17633120715618134,\n",
       "  0.5999781489372253,\n",
       "  -0.20471340417861938,\n",
       "  -0.7941346764564514,\n",
       "  -0.3031436502933502,\n",
       "  -0.08241572976112366,\n",
       "  -0.9264982342720032,\n",
       "  -0.2222728729248047,\n",
       "  -0.3238370418548584,\n",
       "  -0.7104605436325073,\n",
       "  0.8730823993682861,\n",
       "  0.9320602416992188,\n",
       "  0.8408662676811218,\n",
       "  -0.5690397620201111,\n",
       "  -0.6034623980522156,\n",
       "  0.019691117107868195,\n",
       "  -0.30083367228507996,\n",
       "  -0.4054086208343506,\n",
       "  -0.17068170011043549,\n",
       "  0.4325747489929199,\n",
       "  0.5786721706390381,\n",
       "  0.14237534999847412,\n",
       "  -0.037798743695020676,\n",
       "  0.34299007058143616,\n",
       "  -0.4303964674472809,\n",
       "  0.23093348741531372,\n",
       "  0.07782471925020218,\n",
       "  0.2417709231376648,\n",
       "  -0.688822865486145,\n",
       "  -1.3099241256713867,\n",
       "  -0.37342724204063416,\n",
       "  -0.37909629940986633,\n",
       "  0.2161446064710617,\n",
       "  0.1862780898809433,\n",
       "  -0.2758235037326813,\n",
       "  0.38128626346588135,\n",
       "  -0.4681520164012909,\n",
       "  0.396647185087204,\n",
       "  0.6776564121246338,\n",
       "  0.6041076183319092,\n",
       "  0.6546014547348022,\n",
       "  -0.6601353883743286,\n",
       "  -0.3362329304218292,\n",
       "  -1.2359275817871094,\n",
       "  -0.5219667553901672,\n",
       "  0.614685595035553,\n",
       "  -0.38565516471862793,\n",
       "  -0.10962504893541336,\n",
       "  0.625419020652771,\n",
       "  -0.10116216540336609,\n",
       "  0.6257575750350952,\n",
       "  0.37992340326309204,\n",
       "  0.2509784400463104,\n",
       "  0.017778703942894936,\n",
       "  0.28458595275878906,\n",
       "  0.03390589356422424,\n",
       "  -0.07440434396266937,\n",
       "  0.24076399207115173,\n",
       "  0.7300758361816406,\n",
       "  -0.8488668203353882,\n",
       "  0.6640687584877014,\n",
       "  -0.2800278067588806]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Custom Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the custom approach as described in the \"Basic Usage\" docs. To demonstrate, we simply take the default functions for each and manually define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through sklearn via below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1= self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :]) # use just the [CLS] output embedding\n",
    "        return output\n",
    "    \n",
    "model = BERTClass()\n",
    "model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text[text.index(\"\\n\\n\")+2:]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rows = []\n",
    "for i in range(len(train_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(train_data[\"data\"][i])\n",
    "    row[\"target\"] = train_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    train_rows.append(row)\n",
    "train_df = pd.DataFrame(train_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = []\n",
    "for i in range(len(test_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(test_data[\"data\"][i])\n",
    "    row[\"target\"] = test_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    test_rows.append(row)\n",
    "test_df = pd.DataFrame(test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11296\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "train_set = EncodedSet(train_df, tokenizer, 256)\n",
    "test_set = EncodedSet(test_df[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {'batch_size': 16,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 2,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    loss_history = []\n",
    "    for _,data in tqdm(enumerate(train_loader, 0), total=len(train_loader), desc=f\"Epoch {epoch}\"):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _%100==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8288a367b7b406e938f3f83a4ededb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/706 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/adapd_nlp_hf/lib/python3.8/site-packages/transformers-4.1.1-py3.8.egg/transformers/tokenization_utils_base.py:2173: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  3.045555830001831\n",
      "Epoch: 0, Loss:  2.6691184043884277\n",
      "Epoch: 0, Loss:  1.8494617938995361\n",
      "Epoch: 0, Loss:  0.3816988170146942\n",
      "Epoch: 0, Loss:  1.6430333852767944\n",
      "Epoch: 0, Loss:  0.8961666822433472\n",
      "Epoch: 0, Loss:  1.0292330980300903\n",
      "Epoch: 0, Loss:  0.6516829133033752\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = {}\n",
    "for index, entry in enumerate(train_data[\"target_names\"]):\n",
    "    encodings[entry] = index\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tx2.wrapper import Wrapper\n",
    "from tx2.dashboard import Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tx2 import utils\n",
    "\n",
    "def custom_encoding_function(text):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(encoded[\"input_ids\"], device=utils.get_device()),\n",
    "        \"attention_mask\": torch.tensor(encoded[\"attention_mask\"], device=utils.get_device()),\n",
    "    }\n",
    "\n",
    "def custom_classification_function(inputs):\n",
    "    return torch.argmax(model(inputs[\"input_ids\"], inputs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "def custom_embedding_function(inputs):\n",
    "    return model.l1(inputs[\"input_ids\"], inputs[\"attention_mask\"])[0][:, 0, :]  # [CLS] token embedding\n",
    "\n",
    "def custom_soft_classification_function(inputs):\n",
    "    return model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:cached version 'data/custom_cache/predictions.json' found\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:cached version 'data/custom_cache/embedding_training.json' found\n",
      "INFO:root:cached version 'data/custom_cache/embedding_testing.json' found\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:cached version 'data/custom_cache/projections_training.json' found\n",
      "INFO:root:cached version 'data/custom_cache/projections_testing.json' found\n",
      "INFO:root:cached version 'data/custom_cache/projector.pkl.gz' found\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:cached version 'data/custom_cache/salience.pkl.gz' found\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:cached version 'data/custom_cache/cluster_profiles.pkl.gz' found\n",
      "INFO:root:cached version 'data/custom_cache/clusters.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_labels.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_words.json' found\n",
      "INFO:root:cached version 'data/custom_cache/cluster_class_words.json' found\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_df=train_df, \n",
    "    test_df=test_df[:2000], \n",
    "    encodings=encodings, \n",
    "    input_col_name=\"text\", \n",
    "    target_col_name=\"target\", \n",
    "    cache_path=\"data/custom_cache\",\n",
    "    overwrite=False\n",
    ")\n",
    "wrapper.encode_function = custom_encoding_function\n",
    "wrapper.classification_function = custom_classification_function\n",
    "wrapper.soft_classification_function = custom_soft_classification_function\n",
    "wrapper.embedding_function = custom_embedding_function\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a105c652f0547bcb0678b7141ac76fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(childâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "dash = Dashboard(wrapper, show_wordclouds=True)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper.recompute_visual_clusterings(dbscan_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.1586105823516846,\n",
       "  -0.269292950630188,\n",
       "  0.32233911752700806,\n",
       "  0.05956360697746277,\n",
       "  0.470281183719635,\n",
       "  -0.1995173841714859,\n",
       "  0.812308669090271,\n",
       "  0.11527691781520844,\n",
       "  -0.5965903997421265,\n",
       "  -0.0914115458726883,\n",
       "  -0.3710917830467224,\n",
       "  -0.022948797792196274,\n",
       "  0.31187009811401367,\n",
       "  1.1063690185546875,\n",
       "  -0.029917806386947632,\n",
       "  0.11398419737815857,\n",
       "  -0.5523065328598022,\n",
       "  -0.5899699330329895,\n",
       "  0.39079421758651733,\n",
       "  -0.7065202593803406]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.17447194457054138,\n",
       "  0.9931786060333252,\n",
       "  -0.01606195978820324,\n",
       "  -1.1096956729888916,\n",
       "  -0.3842974305152893,\n",
       "  0.5603211522102356,\n",
       "  0.14243218302726746,\n",
       "  0.15506093204021454,\n",
       "  -0.6247150301933289,\n",
       "  -1.0153950452804565,\n",
       "  -0.06342215836048126,\n",
       "  1.103736400604248,\n",
       "  0.04490356147289276,\n",
       "  1.1352771520614624,\n",
       "  -0.8774974942207336,\n",
       "  -0.5332215428352356,\n",
       "  0.1610693484544754,\n",
       "  0.3871626853942871,\n",
       "  1.369088053703308,\n",
       "  0.40271803736686707,\n",
       "  0.030687661841511726,\n",
       "  -0.8779581189155579,\n",
       "  0.9278408288955688,\n",
       "  0.008592884987592697,\n",
       "  -0.29590949416160583,\n",
       "  -0.8164458274841309,\n",
       "  0.37475383281707764,\n",
       "  0.6363185048103333,\n",
       "  0.9378421902656555,\n",
       "  0.6158270239830017,\n",
       "  -0.2505626678466797,\n",
       "  0.20758314430713654,\n",
       "  -0.5005321502685547,\n",
       "  0.2038734406232834,\n",
       "  -0.3629128336906433,\n",
       "  0.08684615045785904,\n",
       "  0.41998451948165894,\n",
       "  0.3224567770957947,\n",
       "  -0.5328847765922546,\n",
       "  -0.13581889867782593,\n",
       "  -1.3324553966522217,\n",
       "  -0.08679085969924927,\n",
       "  -0.041342999786138535,\n",
       "  0.14681130647659302,\n",
       "  0.45271867513656616,\n",
       "  0.2081996500492096,\n",
       "  -0.03336434066295624,\n",
       "  -0.07037477195262909,\n",
       "  -0.5470823049545288,\n",
       "  0.7366995215415955,\n",
       "  -0.5309891104698181,\n",
       "  0.19492778182029724,\n",
       "  0.28021135926246643,\n",
       "  0.8350492119789124,\n",
       "  -0.03163520619273186,\n",
       "  0.4679824709892273,\n",
       "  -0.46160537004470825,\n",
       "  -0.08470966666936874,\n",
       "  -0.13442149758338928,\n",
       "  0.19235439598560333,\n",
       "  -0.482310026884079,\n",
       "  0.05594436824321747,\n",
       "  0.22652176022529602,\n",
       "  0.8020638227462769,\n",
       "  0.41763338446617126,\n",
       "  -1.1770814657211304,\n",
       "  0.335496723651886,\n",
       "  -0.12085524201393127,\n",
       "  0.46551063656806946,\n",
       "  -0.5231508612632751,\n",
       "  0.7037779688835144,\n",
       "  0.38519811630249023,\n",
       "  0.42972105741500854,\n",
       "  0.10947079211473465,\n",
       "  0.8262370228767395,\n",
       "  -0.26015061140060425,\n",
       "  -0.4327068030834198,\n",
       "  -0.6303264498710632,\n",
       "  -0.4159628748893738,\n",
       "  -0.19922272861003876,\n",
       "  0.6424369215965271,\n",
       "  0.1412624716758728,\n",
       "  -1.2794616222381592,\n",
       "  0.3998928368091583,\n",
       "  0.3230050802230835,\n",
       "  0.3865028917789459,\n",
       "  -0.42862433195114136,\n",
       "  -0.43742311000823975,\n",
       "  0.16150414943695068,\n",
       "  0.7465774416923523,\n",
       "  -0.37901249527931213,\n",
       "  0.6837053298950195,\n",
       "  -0.7549005150794983,\n",
       "  0.08670233935117722,\n",
       "  0.1109144538640976,\n",
       "  0.4023873209953308,\n",
       "  0.37095341086387634,\n",
       "  0.14000339806079865,\n",
       "  3.1535892486572266,\n",
       "  0.05918503925204277,\n",
       "  0.1276606023311615,\n",
       "  -0.4817197620868683,\n",
       "  0.3797084391117096,\n",
       "  -0.47914206981658936,\n",
       "  -0.036525074392557144,\n",
       "  -0.5158683061599731,\n",
       "  -0.4157560467720032,\n",
       "  -1.0562947988510132,\n",
       "  0.24589471518993378,\n",
       "  0.5273638367652893,\n",
       "  0.5532650351524353,\n",
       "  -0.2406286746263504,\n",
       "  0.1562686264514923,\n",
       "  0.937317430973053,\n",
       "  0.32264336943626404,\n",
       "  -0.2978161871433258,\n",
       "  -0.29441148042678833,\n",
       "  -0.004161017015576363,\n",
       "  -0.7891985177993774,\n",
       "  0.10122374445199966,\n",
       "  0.19726751744747162,\n",
       "  0.31435105204582214,\n",
       "  1.4059010744094849,\n",
       "  0.013723642565310001,\n",
       "  -0.45044541358947754,\n",
       "  0.47805100679397583,\n",
       "  -0.34118959307670593,\n",
       "  0.47770237922668457,\n",
       "  0.2364143580198288,\n",
       "  -0.6547777056694031,\n",
       "  -0.4691643714904785,\n",
       "  -0.9099711775779724,\n",
       "  -0.26363396644592285,\n",
       "  -0.40346965193748474,\n",
       "  0.15759022533893585,\n",
       "  0.10193274915218353,\n",
       "  -0.03427693247795105,\n",
       "  -0.12188457697629929,\n",
       "  -2.38875675201416,\n",
       "  0.010376502759754658,\n",
       "  -0.2047455757856369,\n",
       "  -0.020027801394462585,\n",
       "  0.44132480025291443,\n",
       "  -0.6285865306854248,\n",
       "  0.27894487977027893,\n",
       "  2.0470690727233887,\n",
       "  -0.3355099558830261,\n",
       "  0.447989284992218,\n",
       "  0.35894161462783813,\n",
       "  -0.5732185244560242,\n",
       "  0.5560122728347778,\n",
       "  1.027663230895996,\n",
       "  0.5665638446807861,\n",
       "  0.6338019967079163,\n",
       "  0.5810181498527527,\n",
       "  -0.6976680755615234,\n",
       "  -0.012523488141596317,\n",
       "  -0.24979569017887115,\n",
       "  0.10700575262308121,\n",
       "  0.19872969388961792,\n",
       "  0.39630958437919617,\n",
       "  0.006738101597875357,\n",
       "  -0.6101914048194885,\n",
       "  0.03693898767232895,\n",
       "  1.2264373302459717,\n",
       "  -0.7957552075386047,\n",
       "  1.3847672939300537,\n",
       "  -0.413113534450531,\n",
       "  0.2111729234457016,\n",
       "  0.6423460841178894,\n",
       "  0.2824894189834595,\n",
       "  -1.0180517435073853,\n",
       "  -1.0055947303771973,\n",
       "  0.6106128692626953,\n",
       "  0.7373060584068298,\n",
       "  0.3140738904476166,\n",
       "  -0.011516731232404709,\n",
       "  0.6867774724960327,\n",
       "  -0.05904248356819153,\n",
       "  -0.6351730823516846,\n",
       "  -0.8837601542472839,\n",
       "  0.2291548103094101,\n",
       "  -0.8291718363761902,\n",
       "  -0.21364305913448334,\n",
       "  -0.44188112020492554,\n",
       "  -0.2819543182849884,\n",
       "  0.29403430223464966,\n",
       "  0.9488955140113831,\n",
       "  -0.07117122411727905,\n",
       "  -0.2545236051082611,\n",
       "  0.6600366234779358,\n",
       "  1.0242992639541626,\n",
       "  0.0627773106098175,\n",
       "  0.504940927028656,\n",
       "  0.5304730534553528,\n",
       "  0.27662789821624756,\n",
       "  0.5538923144340515,\n",
       "  -0.7235219478607178,\n",
       "  -1.2297255992889404,\n",
       "  -0.14191046357154846,\n",
       "  0.08388377726078033,\n",
       "  -0.1198362410068512,\n",
       "  -0.25736308097839355,\n",
       "  -0.46681347489356995,\n",
       "  0.8892740607261658,\n",
       "  -0.26652777194976807,\n",
       "  -0.17716610431671143,\n",
       "  -0.16838152706623077,\n",
       "  1.0070797204971313,\n",
       "  -1.002861499786377,\n",
       "  -0.5301644802093506,\n",
       "  -0.2298016995191574,\n",
       "  0.4220446050167084,\n",
       "  0.6296281814575195,\n",
       "  -0.020556772127747536,\n",
       "  -0.7030135989189148,\n",
       "  0.14351069927215576,\n",
       "  0.4477115869522095,\n",
       "  -0.32364073395729065,\n",
       "  0.5061472654342651,\n",
       "  -0.7674485445022583,\n",
       "  -0.3314192593097687,\n",
       "  0.2851583659648895,\n",
       "  -0.9017206430435181,\n",
       "  0.1292905956506729,\n",
       "  0.2886447608470917,\n",
       "  0.6205490231513977,\n",
       "  0.10580254346132278,\n",
       "  0.5327110290527344,\n",
       "  -0.33101874589920044,\n",
       "  0.05359344184398651,\n",
       "  0.6761738657951355,\n",
       "  0.2673135995864868,\n",
       "  -0.8775195479393005,\n",
       "  0.3025021553039551,\n",
       "  0.5937346816062927,\n",
       "  -0.8672523498535156,\n",
       "  -0.09181896597146988,\n",
       "  -0.43080586194992065,\n",
       "  0.013896534219384193,\n",
       "  -0.5620359778404236,\n",
       "  -1.050611972808838,\n",
       "  1.09260892868042,\n",
       "  -0.049326468259096146,\n",
       "  0.16163749992847443,\n",
       "  -0.4206584393978119,\n",
       "  0.1677015721797943,\n",
       "  0.395331472158432,\n",
       "  0.15289722383022308,\n",
       "  -0.6723751425743103,\n",
       "  0.18745963275432587,\n",
       "  -0.49422961473464966,\n",
       "  0.05728761479258537,\n",
       "  -1.1049270629882812,\n",
       "  0.1933731585741043,\n",
       "  0.1283654272556305,\n",
       "  0.3046776056289673,\n",
       "  -1.2655420303344727,\n",
       "  0.4664666950702667,\n",
       "  -1.8052520751953125,\n",
       "  0.5238243341445923,\n",
       "  -2.001229763031006,\n",
       "  0.6616763472557068,\n",
       "  0.9586389064788818,\n",
       "  -0.27737221121788025,\n",
       "  -0.6339710354804993,\n",
       "  -0.26960286498069763,\n",
       "  -0.25645893812179565,\n",
       "  0.31149670481681824,\n",
       "  0.8986833691596985,\n",
       "  0.7126682996749878,\n",
       "  -0.24751275777816772,\n",
       "  0.004562539979815483,\n",
       "  0.46298688650131226,\n",
       "  0.3651752173900604,\n",
       "  0.9559521079063416,\n",
       "  0.13006436824798584,\n",
       "  0.03587528318166733,\n",
       "  -0.015038170851767063,\n",
       "  -0.09595463424921036,\n",
       "  -0.5511395335197449,\n",
       "  0.5164914727210999,\n",
       "  -1.544126272201538,\n",
       "  0.014842800796031952,\n",
       "  -0.032529402524232864,\n",
       "  0.766880989074707,\n",
       "  1.3251945972442627,\n",
       "  -0.7056003212928772,\n",
       "  -0.1632910519838333,\n",
       "  2.393805980682373,\n",
       "  -0.6804120540618896,\n",
       "  -0.5704535841941833,\n",
       "  -0.43553662300109863,\n",
       "  -0.9912399649620056,\n",
       "  -0.1292363554239273,\n",
       "  -1.1146618127822876,\n",
       "  0.02465674839913845,\n",
       "  -0.4437408745288849,\n",
       "  -0.6846582889556885,\n",
       "  -0.23855772614479065,\n",
       "  1.140925407409668,\n",
       "  -0.4464455246925354,\n",
       "  -0.7160587310791016,\n",
       "  0.48668867349624634,\n",
       "  -0.4226023256778717,\n",
       "  -0.3042236864566803,\n",
       "  0.8331106901168823,\n",
       "  -0.03794030845165253,\n",
       "  -0.6661574840545654,\n",
       "  0.06052007898688316,\n",
       "  0.18450455367565155,\n",
       "  0.4153918921947479,\n",
       "  -0.4044310748577118,\n",
       "  -0.7528190612792969,\n",
       "  -1.425600290298462,\n",
       "  -0.1368524581193924,\n",
       "  -1.1605318784713745,\n",
       "  0.039502814412117004,\n",
       "  -0.5408617854118347,\n",
       "  -0.1439281553030014,\n",
       "  0.14619120955467224,\n",
       "  -0.3090745806694031,\n",
       "  -0.6650052666664124,\n",
       "  0.20274662971496582,\n",
       "  -0.6435030102729797,\n",
       "  -0.23881226778030396,\n",
       "  0.19467578828334808,\n",
       "  0.04547618702054024,\n",
       "  -1.461729645729065,\n",
       "  0.7888852953910828,\n",
       "  -1.091856837272644,\n",
       "  -0.6507018208503723,\n",
       "  0.07499120384454727,\n",
       "  0.1658397614955902,\n",
       "  -0.012384156696498394,\n",
       "  0.29443028569221497,\n",
       "  -0.4967494606971741,\n",
       "  0.7612356543540955,\n",
       "  0.4633369743824005,\n",
       "  -0.5244778990745544,\n",
       "  0.8598725199699402,\n",
       "  0.7843013405799866,\n",
       "  0.0023558130487799644,\n",
       "  -0.4273424744606018,\n",
       "  -1.074775218963623,\n",
       "  -0.7609919905662537,\n",
       "  0.5726380944252014,\n",
       "  -0.12056750804185867,\n",
       "  -0.6857165694236755,\n",
       "  -0.12740448117256165,\n",
       "  0.1263592541217804,\n",
       "  -1.1900132894515991,\n",
       "  -0.11010370403528214,\n",
       "  0.1259753406047821,\n",
       "  1.1147451400756836,\n",
       "  -0.31308749318122864,\n",
       "  -0.029726248234510422,\n",
       "  -0.2783880829811096,\n",
       "  -0.03563770279288292,\n",
       "  0.19048097729682922,\n",
       "  0.36949673295021057,\n",
       "  -1.1622521877288818,\n",
       "  1.2105635404586792,\n",
       "  -0.19985777139663696,\n",
       "  0.3690374195575714,\n",
       "  -0.5718274116516113,\n",
       "  0.29213008284568787,\n",
       "  -0.6946905255317688,\n",
       "  0.05538859963417053,\n",
       "  0.431803435087204,\n",
       "  0.9919177293777466,\n",
       "  0.46938225626945496,\n",
       "  0.6088768243789673,\n",
       "  0.41955533623695374,\n",
       "  -0.23640857636928558,\n",
       "  -1.107866644859314,\n",
       "  0.3526592254638672,\n",
       "  -0.28077563643455505,\n",
       "  0.16244181990623474,\n",
       "  -0.8627232313156128,\n",
       "  0.04471481591463089,\n",
       "  -0.6618359088897705,\n",
       "  0.7783628106117249,\n",
       "  0.40865474939346313,\n",
       "  -0.40315717458724976,\n",
       "  -0.17780600488185883,\n",
       "  0.4592714011669159,\n",
       "  0.5755608677864075,\n",
       "  -0.11219781637191772,\n",
       "  0.06367155909538269,\n",
       "  0.21599231660366058,\n",
       "  -0.840652585029602,\n",
       "  -0.06859604269266129,\n",
       "  -0.06912700831890106,\n",
       "  -0.4249669909477234,\n",
       "  0.010332001373171806,\n",
       "  0.5788914561271667,\n",
       "  0.5867595076560974,\n",
       "  0.7962981462478638,\n",
       "  0.024042580276727676,\n",
       "  -0.531493604183197,\n",
       "  0.32169926166534424,\n",
       "  -0.3789735436439514,\n",
       "  0.049892596900463104,\n",
       "  -0.3251331150531769,\n",
       "  0.3615425229072571,\n",
       "  0.18193645775318146,\n",
       "  -0.14238928258419037,\n",
       "  -1.3034194707870483,\n",
       "  0.4179003834724426,\n",
       "  0.07829535752534866,\n",
       "  -0.6069260239601135,\n",
       "  -0.1851552426815033,\n",
       "  -0.1140112578868866,\n",
       "  0.629127025604248,\n",
       "  -1.2934969663619995,\n",
       "  -0.13718700408935547,\n",
       "  0.21924364566802979,\n",
       "  0.6442474722862244,\n",
       "  -0.33548757433891296,\n",
       "  0.4477500915527344,\n",
       "  0.33753278851509094,\n",
       "  -0.30483579635620117,\n",
       "  -1.052783489227295,\n",
       "  0.5436803698539734,\n",
       "  0.05432148650288582,\n",
       "  0.2446892410516739,\n",
       "  -0.24490603804588318,\n",
       "  1.2147117853164673,\n",
       "  1.0905193090438843,\n",
       "  -0.17267902195453644,\n",
       "  0.341429740190506,\n",
       "  0.3195245862007141,\n",
       "  0.2207491546869278,\n",
       "  0.07615368068218231,\n",
       "  0.49290427565574646,\n",
       "  -0.018807703629136086,\n",
       "  -0.5262554883956909,\n",
       "  -0.6750630736351013,\n",
       "  1.8594334125518799,\n",
       "  -0.9113317131996155,\n",
       "  -0.2780172526836395,\n",
       "  -0.23705312609672546,\n",
       "  0.46321263909339905,\n",
       "  -0.04392422363162041,\n",
       "  -0.6236043572425842,\n",
       "  -0.31021323800086975,\n",
       "  0.10447155684232712,\n",
       "  -0.20009520649909973,\n",
       "  -0.332324743270874,\n",
       "  0.08498861640691757,\n",
       "  0.09240160137414932,\n",
       "  0.06519606709480286,\n",
       "  -0.021211158484220505,\n",
       "  -0.16646525263786316,\n",
       "  0.09243866801261902,\n",
       "  -0.3085486888885498,\n",
       "  0.2740122675895691,\n",
       "  -0.022601475939154625,\n",
       "  0.33380967378616333,\n",
       "  -1.0336127281188965,\n",
       "  0.6025402545928955,\n",
       "  0.17698132991790771,\n",
       "  0.07538141310214996,\n",
       "  1.1883946657180786,\n",
       "  0.27663400769233704,\n",
       "  -0.09674888104200363,\n",
       "  0.3736954927444458,\n",
       "  0.08019594103097916,\n",
       "  0.040222592651844025,\n",
       "  -0.03560342639684677,\n",
       "  0.5327646136283875,\n",
       "  0.4091106057167053,\n",
       "  -0.1786222904920578,\n",
       "  -0.9598503708839417,\n",
       "  1.339028239250183,\n",
       "  -0.036174140870571136,\n",
       "  -0.8878045678138733,\n",
       "  0.36204299330711365,\n",
       "  0.8872060179710388,\n",
       "  -0.2160300463438034,\n",
       "  0.841917872428894,\n",
       "  0.05845651775598526,\n",
       "  0.6471024751663208,\n",
       "  0.7850382924079895,\n",
       "  0.4905646741390228,\n",
       "  0.08693554997444153,\n",
       "  -0.25528740882873535,\n",
       "  0.5158706307411194,\n",
       "  0.6478238701820374,\n",
       "  0.4021035134792328,\n",
       "  0.04950559511780739,\n",
       "  -0.742408812046051,\n",
       "  0.059911198914051056,\n",
       "  0.12259162962436676,\n",
       "  0.003428540425375104,\n",
       "  -0.8130874037742615,\n",
       "  -0.5433285236358643,\n",
       "  -0.6805672645568848,\n",
       "  -0.38964393734931946,\n",
       "  -0.7499992251396179,\n",
       "  0.09123953431844711,\n",
       "  0.04211583733558655,\n",
       "  0.051993176341056824,\n",
       "  -0.0661124736070633,\n",
       "  0.4476603865623474,\n",
       "  -0.12657512724399567,\n",
       "  -0.1303730309009552,\n",
       "  -0.0620608888566494,\n",
       "  0.24504001438617706,\n",
       "  0.2403959333896637,\n",
       "  -0.14507609605789185,\n",
       "  0.39759740233421326,\n",
       "  0.5909735560417175,\n",
       "  -0.2800925672054291,\n",
       "  0.012437217868864536,\n",
       "  -0.9036535620689392,\n",
       "  0.20139147341251373,\n",
       "  -0.08244319260120392,\n",
       "  0.2368771880865097,\n",
       "  -0.7003673911094666,\n",
       "  -0.6904047727584839,\n",
       "  -0.7984455227851868,\n",
       "  -0.0851982980966568,\n",
       "  -0.089396633207798,\n",
       "  -0.10147594660520554,\n",
       "  0.2696930766105652,\n",
       "  0.37684008479118347,\n",
       "  -0.01283667515963316,\n",
       "  -0.3478902578353882,\n",
       "  0.23722116649150848,\n",
       "  0.0746794119477272,\n",
       "  -0.21982340514659882,\n",
       "  0.9274489879608154,\n",
       "  0.6919798254966736,\n",
       "  -0.7449237108230591,\n",
       "  0.24474216997623444,\n",
       "  0.26077544689178467,\n",
       "  -0.7197049260139465,\n",
       "  -0.6293529868125916,\n",
       "  0.9048378467559814,\n",
       "  -0.32770785689353943,\n",
       "  -0.11383596062660217,\n",
       "  -0.341692179441452,\n",
       "  0.15532024204730988,\n",
       "  0.11466606706380844,\n",
       "  -0.1474786251783371,\n",
       "  0.9360966086387634,\n",
       "  0.7716594934463501,\n",
       "  0.14946432411670685,\n",
       "  0.23020006716251373,\n",
       "  -0.3616488575935364,\n",
       "  -1.036294937133789,\n",
       "  -0.18926815688610077,\n",
       "  0.7866480946540833,\n",
       "  0.9432981610298157,\n",
       "  -0.20832715928554535,\n",
       "  -5.806033134460449,\n",
       "  -0.0585019625723362,\n",
       "  0.774641752243042,\n",
       "  -0.2734185755252838,\n",
       "  0.011340550146996975,\n",
       "  -0.32030123472213745,\n",
       "  -0.8135960698127747,\n",
       "  0.22555962204933167,\n",
       "  -0.5529108643531799,\n",
       "  0.1817733496427536,\n",
       "  -0.1373545229434967,\n",
       "  0.8409803509712219,\n",
       "  -2.0206570625305176,\n",
       "  0.5529946684837341,\n",
       "  -0.7590022087097168,\n",
       "  -0.15112249553203583,\n",
       "  0.06096339970827103,\n",
       "  -1.0952755212783813,\n",
       "  -0.7024580836296082,\n",
       "  -0.48860299587249756,\n",
       "  -0.4699653089046478,\n",
       "  0.36145302653312683,\n",
       "  0.376304030418396,\n",
       "  0.9350889921188354,\n",
       "  0.6974533200263977,\n",
       "  1.4956376552581787,\n",
       "  0.5572872757911682,\n",
       "  0.218179851770401,\n",
       "  0.1201702132821083,\n",
       "  -0.662882924079895,\n",
       "  0.020128434523940086,\n",
       "  0.6065220236778259,\n",
       "  0.9201573729515076,\n",
       "  -0.5089097023010254,\n",
       "  -0.39464446902275085,\n",
       "  -0.41600438952445984,\n",
       "  0.04889177531003952,\n",
       "  0.21014352142810822,\n",
       "  0.7102362513542175,\n",
       "  -1.194907546043396,\n",
       "  0.2598299980163574,\n",
       "  0.41992300748825073,\n",
       "  0.7734778523445129,\n",
       "  0.8575215339660645,\n",
       "  -0.6360518932342529,\n",
       "  0.1884421855211258,\n",
       "  0.009042586199939251,\n",
       "  -1.0158895254135132,\n",
       "  0.5727096796035767,\n",
       "  0.05042896419763565,\n",
       "  1.1736760139465332,\n",
       "  -0.15159845352172852,\n",
       "  -0.9664084911346436,\n",
       "  -0.7782920598983765,\n",
       "  -0.39316797256469727,\n",
       "  0.5544114112854004,\n",
       "  -0.02635628543794155,\n",
       "  1.5552514791488647,\n",
       "  0.9117303490638733,\n",
       "  -0.524489164352417,\n",
       "  -0.9882389307022095,\n",
       "  0.11766627430915833,\n",
       "  0.668460488319397,\n",
       "  -0.19337303936481476,\n",
       "  -0.17331966757774353,\n",
       "  0.4355056583881378,\n",
       "  -0.7115711569786072,\n",
       "  -0.7836660742759705,\n",
       "  0.9739810228347778,\n",
       "  -0.2775784134864807,\n",
       "  -0.14055180549621582,\n",
       "  -0.14779207110404968,\n",
       "  0.1639489382505417,\n",
       "  -0.1933194249868393,\n",
       "  -0.2803347110748291,\n",
       "  -0.4691576063632965,\n",
       "  -0.25159355998039246,\n",
       "  -1.0067356824874878,\n",
       "  -0.5211124420166016,\n",
       "  0.269984632730484,\n",
       "  -1.1616638898849487,\n",
       "  -0.2100149244070053,\n",
       "  0.41219061613082886,\n",
       "  -0.9069850444793701,\n",
       "  -0.4539511501789093,\n",
       "  0.1204465925693512,\n",
       "  -1.3274333477020264,\n",
       "  0.03595525771379471,\n",
       "  -0.22862891852855682,\n",
       "  -0.6070247292518616,\n",
       "  0.4371277987957001,\n",
       "  -0.7316824793815613,\n",
       "  0.3665028214454651,\n",
       "  1.1474800109863281,\n",
       "  -0.08162926137447357,\n",
       "  0.7994849681854248,\n",
       "  -0.6781668663024902,\n",
       "  -0.03809574618935585,\n",
       "  -0.5375702977180481,\n",
       "  -0.034330133348703384,\n",
       "  0.4439272880554199,\n",
       "  0.13346096873283386,\n",
       "  -0.5879133343696594,\n",
       "  -0.08813273161649704,\n",
       "  -0.5732664465904236,\n",
       "  0.7275698781013489,\n",
       "  2.0034983158111572,\n",
       "  -0.42400461435317993,\n",
       "  -0.3026866018772125,\n",
       "  0.8928820490837097,\n",
       "  0.1004442423582077,\n",
       "  0.36145010590553284,\n",
       "  -0.5491225719451904,\n",
       "  0.770147442817688,\n",
       "  1.1268426179885864,\n",
       "  1.1656471490859985,\n",
       "  -0.442290723323822,\n",
       "  -0.4387323260307312,\n",
       "  0.9836454391479492,\n",
       "  -0.4926570951938629,\n",
       "  -0.17008481919765472,\n",
       "  -0.7430698275566101,\n",
       "  -0.09789586067199707,\n",
       "  0.09629576653242111,\n",
       "  -0.7048373818397522,\n",
       "  0.3999813199043274,\n",
       "  0.6414588093757629,\n",
       "  -1.4140033721923828,\n",
       "  -1.357211947441101,\n",
       "  -0.31974008679389954,\n",
       "  -0.24563303589820862,\n",
       "  0.09814215451478958,\n",
       "  -0.21018874645233154,\n",
       "  0.10163687914609909,\n",
       "  -0.21403475105762482,\n",
       "  -0.16963741183280945,\n",
       "  -0.1615188717842102,\n",
       "  -0.5751153230667114,\n",
       "  0.03550706058740616,\n",
       "  0.5093631744384766,\n",
       "  -0.16494594514369965,\n",
       "  1.0025625228881836,\n",
       "  0.2809365689754486,\n",
       "  -0.12866273522377014,\n",
       "  -0.06709787994623184,\n",
       "  0.05206717550754547,\n",
       "  -0.3632459342479706,\n",
       "  0.2396993637084961,\n",
       "  -1.1304354667663574,\n",
       "  0.6214728951454163,\n",
       "  -0.6428390741348267,\n",
       "  -0.591816782951355,\n",
       "  0.171854630112648,\n",
       "  0.02398885041475296,\n",
       "  0.2690393626689911,\n",
       "  -0.7410790920257568,\n",
       "  0.10024825483560562,\n",
       "  0.7518476247787476,\n",
       "  -0.9439442157745361,\n",
       "  0.5811437964439392,\n",
       "  1.2157480716705322,\n",
       "  -0.18807636201381683,\n",
       "  -0.11906000226736069,\n",
       "  0.15877896547317505,\n",
       "  -0.07381569594144821,\n",
       "  -0.18190006911754608,\n",
       "  -0.21241162717342377,\n",
       "  -0.006350275594741106,\n",
       "  -0.2108793556690216,\n",
       "  0.9674453735351562,\n",
       "  -0.4430045783519745,\n",
       "  -0.682498037815094,\n",
       "  0.11770045012235641,\n",
       "  0.7041776776313782,\n",
       "  0.299269437789917,\n",
       "  -0.36989787220954895,\n",
       "  -0.35297903418540955,\n",
       "  -0.14483200013637543,\n",
       "  0.19040875136852264,\n",
       "  0.6239108443260193,\n",
       "  -0.5479503870010376,\n",
       "  0.41308578848838806,\n",
       "  -0.2542884051799774,\n",
       "  -0.12874732911586761,\n",
       "  0.3710668683052063,\n",
       "  0.44351905584335327,\n",
       "  -0.05747271701693535,\n",
       "  0.5829998254776001,\n",
       "  0.4470652937889099,\n",
       "  0.8166115880012512,\n",
       "  0.11332467198371887,\n",
       "  -0.6929972171783447,\n",
       "  -0.6042528748512268,\n",
       "  -0.32267728447914124,\n",
       "  0.38008907437324524,\n",
       "  -0.407785564661026,\n",
       "  0.32454824447631836,\n",
       "  -0.07382999360561371,\n",
       "  -0.25588905811309814,\n",
       "  0.7277262806892395,\n",
       "  0.4275507628917694,\n",
       "  0.5700197815895081,\n",
       "  0.4108925759792328,\n",
       "  -0.8894989490509033,\n",
       "  -0.29842761158943176,\n",
       "  0.42399945855140686,\n",
       "  -0.13283611834049225,\n",
       "  0.3178425431251526,\n",
       "  -0.2699935734272003,\n",
       "  0.02457543835043907,\n",
       "  0.6499971151351929]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

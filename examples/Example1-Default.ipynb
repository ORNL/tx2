{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Default Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the default approach as described in the \"Basic Usage\" docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LRU_CACHE_CAPACITY'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through huggingfacevia below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
      "Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# getting newsgroups data from huggingface\n",
    "train_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"train\"))\n",
    "test_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"test\"))\n",
    "\n",
    "# setting up pytorch device\n",
    "if cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1 = self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :])  # use just the [CLS] output embedding\n",
    "        return output\n",
    "\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # text = text[text.index(\"\\n\\n\") + 2 :]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you have Weitek's address/phone number? I'd...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye) DN...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>Wouldn't this require a hyper-sphere. In 3-spa...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      I was wondering if anyone out there could enli...      7   \n",
       "1      A fair number of brave souls who upgraded thei...      4   \n",
       "2      well folks, my mac plus finally gave up the gh...      4   \n",
       "3      Do you have Weitek's address/phone number? I'd...      1   \n",
       "4      From article <C5owCB.n3p@world.std.com>, by to...     14   \n",
       "...                                                  ...    ...   \n",
       "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye) DN...     13   \n",
       "11310  I have a (very old) Mac 512k and a Mac Plus, b...      4   \n",
       "11311  I just installed a DX2-66 CPU in a clone mothe...      3   \n",
       "11312  Wouldn't this require a hyper-sphere. In 3-spa...      1   \n",
       "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...      8   \n",
       "\n",
       "                     label_text  \n",
       "0                     rec.autos  \n",
       "1         comp.sys.mac.hardware  \n",
       "2         comp.sys.mac.hardware  \n",
       "3                 comp.graphics  \n",
       "4                     sci.space  \n",
       "...                         ...  \n",
       "11309                   sci.med  \n",
       "11310     comp.sys.mac.hardware  \n",
       "11311  comp.sys.ibm.pc.hardware  \n",
       "11312             comp.graphics  \n",
       "11313           rec.motorcycles  \n",
       "\n",
       "[11014 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean long white space or extensive character returns\n",
    "train_data.text = train_data.text.apply(lambda x: clean_text(x))\n",
    "test_data.text = test_data.text.apply(lambda x: clean_text(x))\n",
    "\n",
    "# remove empty entries or trivially short ones\n",
    "train_cleaned = train_data[train_data[\"text\"].str.len() > 1]\n",
    "test_cleaned = test_data[test_data[\"text\"].str.len() > 1]\n",
    "train_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11014\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.data.label[index], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "train_cleaned.reset_index(drop=True, inplace=True)\n",
    "test_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_set = EncodedSet(train_cleaned, tokenizer, 256)\n",
    "test_set = EncodedSet(test_cleaned[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {\"batch_size\": 16, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "test_params = {\"batch_size\": 2, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    for _, data in tqdm(\n",
    "        enumerate(train_loader, start=0), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "    ):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _ % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss:  {loss.item()}\")\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    #         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d5dd1212974a7f8cf9e3bed124c9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  3.1381747722625732\n",
      "CPU times: user 7 µs, sys: 5 µs, total: 12 µs\n",
      "Wall time: 23.4 µs\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))\n",
    "\n",
    "%time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = (\n",
    "    train_cleaned[[\"label\", \"label_text\"]]\n",
    "    .groupby([\"label_text\"])\n",
    "    .apply(lambda x: x[\"label\"].tolist()[0])\n",
    "    .to_dict()\n",
    ")\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tx2.dashboard import Dashboard\n",
    "from tx2.wrapper import Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # shouldn't be necessary since done in wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cache path found\n",
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:Running classifier...\n",
      "INFO:root:Saving predictions...\n",
      "INFO:root:Writing to data/predictions.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:Embedding training and testing datasets\n",
      "INFO:root:Saving embeddings...\n",
      "INFO:root:Writing to data/embedding_training.json\n",
      "INFO:root:Writing to data/embedding_testing.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:Training projector...\n",
      "INFO:root:Applying projector to test dataset...\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Writing to data/projections_training.json\n",
      "INFO:root:Writing to data/projections_testing.json\n",
      "INFO:root:Writing to data/projector.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:Computing salience maps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57170e8076424633add393ac31d041d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving salience maps...\n",
      "INFO:root:Writing to data/salience.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:Clustering projections...\n",
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 0 ns, total: 14 µs\n",
      "Wall time: 30.3 µs\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_texts=train_cleaned.text,\n",
    "    train_labels=train_cleaned.label,\n",
    "    test_texts=test_cleaned.text[:2000],\n",
    "    test_labels=test_cleaned.label[:2000],\n",
    "    encodings=encodings,\n",
    "    classifier=model,\n",
    "    language_model=model.l1,\n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True,\n",
    ")\n",
    "wrapper.batch_size = 16\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper._compute_all_salience_maps_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8398c1ace3455c8be7f38f58195a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dash = Dashboard(wrapper)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper.recompute_visual_clusterings(\"KMeans\", clustering_args=dict(n_clusters=18))\n",
    "# wrapper.recompute_visual_clusterings(\"OPTICS\", clustering_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.70252525806427,\n",
       "  -0.08774121105670929,\n",
       "  0.03329361230134964,\n",
       "  0.6910033822059631,\n",
       "  -0.2096623331308365,\n",
       "  0.3286273181438446,\n",
       "  0.13128194212913513,\n",
       "  -0.0030859168618917465,\n",
       "  -0.12238069623708725,\n",
       "  -0.311109334230423,\n",
       "  -0.08581492304801941,\n",
       "  -0.02155875787138939,\n",
       "  -0.2893389165401459,\n",
       "  -0.3918139934539795,\n",
       "  -0.20700697600841522,\n",
       "  -0.036795392632484436,\n",
       "  -0.231439471244812,\n",
       "  0.09039005637168884,\n",
       "  0.10014926642179489,\n",
       "  0.6179112195968628]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22336895763874054,\n",
       "  0.14740251004695892,\n",
       "  0.240580216050148,\n",
       "  -0.3657938241958618,\n",
       "  -0.37757351994514465,\n",
       "  -0.02837028168141842,\n",
       "  0.21536007523536682,\n",
       "  -0.24178506433963776,\n",
       "  0.33886128664016724,\n",
       "  -0.9810981750488281,\n",
       "  -0.3445986807346344,\n",
       "  -0.02593773603439331,\n",
       "  -0.09564787894487381,\n",
       "  0.16875332593917847,\n",
       "  -0.3423798084259033,\n",
       "  0.1058882474899292,\n",
       "  0.27200406789779663,\n",
       "  0.2061891406774521,\n",
       "  0.15039069950580597,\n",
       "  -0.23673740029335022,\n",
       "  0.1307438611984253,\n",
       "  -0.07121182233095169,\n",
       "  0.5284308195114136,\n",
       "  -0.3411671817302704,\n",
       "  -0.19292786717414856,\n",
       "  0.016751376911997795,\n",
       "  0.19145603477954865,\n",
       "  0.03140030428767204,\n",
       "  -0.022005822509527206,\n",
       "  0.3931223452091217,\n",
       "  -0.0787348598241806,\n",
       "  0.38846680521965027,\n",
       "  -0.0737343281507492,\n",
       "  0.18222106993198395,\n",
       "  -0.03844716399908066,\n",
       "  -0.023472195491194725,\n",
       "  -0.010447225533425808,\n",
       "  -0.15724509954452515,\n",
       "  -0.09711393713951111,\n",
       "  -0.08561110496520996,\n",
       "  -0.4862777292728424,\n",
       "  0.05628576502203941,\n",
       "  0.42431625723838806,\n",
       "  0.11583810299634933,\n",
       "  0.07442230731248856,\n",
       "  -0.2502947747707367,\n",
       "  0.0948840081691742,\n",
       "  0.017859285697340965,\n",
       "  -0.23295843601226807,\n",
       "  -0.06428752094507217,\n",
       "  -0.22786307334899902,\n",
       "  0.042634956538677216,\n",
       "  0.12281488627195358,\n",
       "  0.08734536170959473,\n",
       "  0.2044624537229538,\n",
       "  0.09596917778253555,\n",
       "  -0.3487134575843811,\n",
       "  0.0771859884262085,\n",
       "  -0.3221743404865265,\n",
       "  0.27271783351898193,\n",
       "  -0.25484102964401245,\n",
       "  -0.26814335584640503,\n",
       "  0.2632361352443695,\n",
       "  0.366881787776947,\n",
       "  -0.07855904847383499,\n",
       "  -0.07930311560630798,\n",
       "  -0.05209667608141899,\n",
       "  -0.1786235123872757,\n",
       "  -0.3590669631958008,\n",
       "  -0.3675737679004669,\n",
       "  0.009804649278521538,\n",
       "  0.06866677850484848,\n",
       "  0.11976784467697144,\n",
       "  0.9298439621925354,\n",
       "  0.5671759843826294,\n",
       "  -0.6006792783737183,\n",
       "  0.33135971426963806,\n",
       "  0.21180741488933563,\n",
       "  -0.08499118685722351,\n",
       "  0.03820904716849327,\n",
       "  0.19504934549331665,\n",
       "  0.011450178921222687,\n",
       "  -0.17806053161621094,\n",
       "  -0.15849460661411285,\n",
       "  -0.2204594910144806,\n",
       "  -0.00038302945904433727,\n",
       "  0.03736715763807297,\n",
       "  -0.023844311013817787,\n",
       "  -0.0906578078866005,\n",
       "  0.019568607211112976,\n",
       "  0.22945047914981842,\n",
       "  0.016045991331338882,\n",
       "  -0.5351395010948181,\n",
       "  0.2395244538784027,\n",
       "  -0.053837936371564865,\n",
       "  0.2799275517463684,\n",
       "  -0.07928276807069778,\n",
       "  0.5037716627120972,\n",
       "  6.369478225708008,\n",
       "  -0.10875559598207474,\n",
       "  0.1416933238506317,\n",
       "  0.04872528463602066,\n",
       "  0.09751524776220322,\n",
       "  0.0004574625345412642,\n",
       "  0.24217216670513153,\n",
       "  -0.39722028374671936,\n",
       "  0.0034557473845779896,\n",
       "  -0.4449271559715271,\n",
       "  0.19286766648292542,\n",
       "  0.4745340347290039,\n",
       "  0.7392034530639648,\n",
       "  0.039624880999326706,\n",
       "  0.3819548189640045,\n",
       "  0.039833977818489075,\n",
       "  0.106351338326931,\n",
       "  -0.33630043268203735,\n",
       "  -0.1920425444841385,\n",
       "  0.20056799054145813,\n",
       "  -0.1698632538318634,\n",
       "  -0.20307601988315582,\n",
       "  -0.050296518951654434,\n",
       "  0.231478750705719,\n",
       "  1.3401100635528564,\n",
       "  0.36420318484306335,\n",
       "  -0.4410886764526367,\n",
       "  -0.1272355318069458,\n",
       "  0.08399824053049088,\n",
       "  -0.08161423355340958,\n",
       "  0.15196718275547028,\n",
       "  -0.2637327015399933,\n",
       "  -0.5644196271896362,\n",
       "  -0.20070405304431915,\n",
       "  -0.12300293892621994,\n",
       "  0.1811957061290741,\n",
       "  -0.08541636168956757,\n",
       "  0.23337575793266296,\n",
       "  -0.22013616561889648,\n",
       "  -0.2188984602689743,\n",
       "  -0.6806115508079529,\n",
       "  -0.02529023215174675,\n",
       "  0.08850932866334915,\n",
       "  -0.06968377530574799,\n",
       "  -0.015386510640382767,\n",
       "  -0.1790289431810379,\n",
       "  -0.3766893446445465,\n",
       "  2.7362306118011475,\n",
       "  -0.028970563784241676,\n",
       "  -0.22966039180755615,\n",
       "  -0.10357251763343811,\n",
       "  -0.1366093009710312,\n",
       "  0.348478764295578,\n",
       "  0.16259102523326874,\n",
       "  -0.22604872286319733,\n",
       "  -0.055558402091264725,\n",
       "  0.01855296455323696,\n",
       "  -0.43508434295654297,\n",
       "  0.19553492963314056,\n",
       "  -0.1293623000383377,\n",
       "  -0.11606346815824509,\n",
       "  0.07679424434900284,\n",
       "  -0.5463919639587402,\n",
       "  -0.18154104053974152,\n",
       "  -0.31811508536338806,\n",
       "  0.36354121565818787,\n",
       "  0.3858555555343628,\n",
       "  -0.1429259479045868,\n",
       "  0.2729838490486145,\n",
       "  -0.6076833605766296,\n",
       "  0.12585480511188507,\n",
       "  0.2674447000026703,\n",
       "  -0.16418898105621338,\n",
       "  -0.3604217767715454,\n",
       "  -2.3702969551086426,\n",
       "  0.346323698759079,\n",
       "  -0.10945973545312881,\n",
       "  0.24689899384975433,\n",
       "  0.24594920873641968,\n",
       "  0.11259628087282181,\n",
       "  -0.10657987743616104,\n",
       "  0.039337754249572754,\n",
       "  -0.19291426241397858,\n",
       "  0.33158078789711,\n",
       "  0.026588957756757736,\n",
       "  -0.13487665355205536,\n",
       "  -0.3814489245414734,\n",
       "  -0.008473997935652733,\n",
       "  0.0687284767627716,\n",
       "  -0.3537646532058716,\n",
       "  -0.05290568247437477,\n",
       "  -0.10681840777397156,\n",
       "  0.11191556602716446,\n",
       "  0.044443488121032715,\n",
       "  -0.20161999762058258,\n",
       "  0.09931596368551254,\n",
       "  0.34546607732772827,\n",
       "  -0.06542392075061798,\n",
       "  0.13838748633861542,\n",
       "  -0.3025483787059784,\n",
       "  0.15197984874248505,\n",
       "  0.048611678183078766,\n",
       "  -0.2041170746088028,\n",
       "  -0.06115128844976425,\n",
       "  -0.5253164172172546,\n",
       "  0.3086462616920471,\n",
       "  0.3585171699523926,\n",
       "  0.1463293731212616,\n",
       "  0.023034755140542984,\n",
       "  -0.20253488421440125,\n",
       "  0.3517303168773651,\n",
       "  -0.0804566815495491,\n",
       "  -0.40714117884635925,\n",
       "  0.04305499047040939,\n",
       "  0.2147919237613678,\n",
       "  0.25527605414390564,\n",
       "  0.5634976029396057,\n",
       "  -0.35741353034973145,\n",
       "  0.17011147737503052,\n",
       "  0.3018186688423157,\n",
       "  0.09086382389068604,\n",
       "  0.013855097815394402,\n",
       "  -0.10050582140684128,\n",
       "  -0.00200836849398911,\n",
       "  -0.15314742922782898,\n",
       "  -0.011222678236663342,\n",
       "  0.3075198233127594,\n",
       "  -0.07414336502552032,\n",
       "  0.18028943240642548,\n",
       "  0.07937242835760117,\n",
       "  0.1951104998588562,\n",
       "  -0.2454419583082199,\n",
       "  0.24816836416721344,\n",
       "  0.23609930276870728,\n",
       "  -0.0024263127706944942,\n",
       "  -0.2434786558151245,\n",
       "  0.01718699187040329,\n",
       "  -0.044297654181718826,\n",
       "  -0.219014510512352,\n",
       "  0.22432754933834076,\n",
       "  0.01171473041176796,\n",
       "  0.04760037362575531,\n",
       "  -0.057439032942056656,\n",
       "  -0.20469027757644653,\n",
       "  0.0775236040353775,\n",
       "  -0.08998081833124161,\n",
       "  -0.007496945559978485,\n",
       "  0.2719199061393738,\n",
       "  0.352726012468338,\n",
       "  0.2077394425868988,\n",
       "  0.2606114149093628,\n",
       "  -0.15208926796913147,\n",
       "  0.895992636680603,\n",
       "  -0.021606914699077606,\n",
       "  -0.24840378761291504,\n",
       "  -0.3918832242488861,\n",
       "  -0.06675134599208832,\n",
       "  0.19218777120113373,\n",
       "  -0.13709941506385803,\n",
       "  -1.722700834274292,\n",
       "  0.04439287260174751,\n",
       "  -0.3715816140174866,\n",
       "  0.2049800157546997,\n",
       "  -3.0520009994506836,\n",
       "  0.21461480855941772,\n",
       "  0.11009793728590012,\n",
       "  0.05929240584373474,\n",
       "  0.04503012076020241,\n",
       "  0.3585793077945709,\n",
       "  0.04608310014009476,\n",
       "  -0.09845612198114395,\n",
       "  0.22741670906543732,\n",
       "  -0.25237590074539185,\n",
       "  0.08849963545799255,\n",
       "  -0.5784815549850464,\n",
       "  0.1108604371547699,\n",
       "  0.025160739198327065,\n",
       "  0.21003422141075134,\n",
       "  0.09219948947429657,\n",
       "  -0.06883697956800461,\n",
       "  0.02719077840447426,\n",
       "  -0.15669316053390503,\n",
       "  -0.02862056903541088,\n",
       "  0.25864943861961365,\n",
       "  -0.4379468262195587,\n",
       "  0.30441778898239136,\n",
       "  -0.3010883629322052,\n",
       "  0.2962630093097687,\n",
       "  0.6019259691238403,\n",
       "  0.147074356675148,\n",
       "  0.015200660564005375,\n",
       "  4.073275089263916,\n",
       "  0.2059394270181656,\n",
       "  -0.09832310676574707,\n",
       "  0.21068468689918518,\n",
       "  -0.4240049421787262,\n",
       "  -0.26875653862953186,\n",
       "  0.021945681422948837,\n",
       "  -0.5804668068885803,\n",
       "  -0.2477228343486786,\n",
       "  -0.20495131611824036,\n",
       "  -0.14111587405204773,\n",
       "  0.24184057116508484,\n",
       "  -0.13118237257003784,\n",
       "  -0.7489919066429138,\n",
       "  0.09182831645011902,\n",
       "  -0.3126537799835205,\n",
       "  -0.2711324393749237,\n",
       "  0.3132290542125702,\n",
       "  0.28940168023109436,\n",
       "  -0.8144950270652771,\n",
       "  0.23519504070281982,\n",
       "  0.09796515107154846,\n",
       "  0.03871453180909157,\n",
       "  -0.11675482243299484,\n",
       "  -0.21829617023468018,\n",
       "  -0.49591609835624695,\n",
       "  -0.4057682156562805,\n",
       "  -0.2282477468252182,\n",
       "  0.625264585018158,\n",
       "  -0.1148955449461937,\n",
       "  -1.0362849235534668,\n",
       "  0.37711161375045776,\n",
       "  -0.06446781754493713,\n",
       "  -0.3760254979133606,\n",
       "  -0.33370766043663025,\n",
       "  -0.036469466984272,\n",
       "  0.09958626329898834,\n",
       "  0.09540697187185287,\n",
       "  -0.10744378715753555,\n",
       "  -0.16362756490707397,\n",
       "  0.10733065009117126,\n",
       "  -0.2390376329421997,\n",
       "  -0.053031403571367264,\n",
       "  -0.5615189671516418,\n",
       "  0.04093289002776146,\n",
       "  -0.17136703431606293,\n",
       "  0.006556368432939053,\n",
       "  -0.34423375129699707,\n",
       "  0.33153626322746277,\n",
       "  -0.050476882606744766,\n",
       "  -0.0456882081925869,\n",
       "  -0.042579520493745804,\n",
       "  0.3156668543815613,\n",
       "  -0.004076903220266104,\n",
       "  0.07120488584041595,\n",
       "  0.27665042877197266,\n",
       "  0.36698243021965027,\n",
       "  0.16394849121570587,\n",
       "  -0.010619395412504673,\n",
       "  0.20113694667816162,\n",
       "  0.08311715722084045,\n",
       "  0.33446529507637024,\n",
       "  -0.07440559566020966,\n",
       "  0.04257596284151077,\n",
       "  0.1616639345884323,\n",
       "  0.042227353900671005,\n",
       "  0.10309629142284393,\n",
       "  -0.25887319445610046,\n",
       "  -0.2799502909183502,\n",
       "  -0.12377835810184479,\n",
       "  0.24502171576023102,\n",
       "  -0.030728962272405624,\n",
       "  -2.0515975952148438,\n",
       "  0.5446759462356567,\n",
       "  0.1660798192024231,\n",
       "  0.06618291139602661,\n",
       "  -0.20876701176166534,\n",
       "  -0.2975539565086365,\n",
       "  -0.06591879576444626,\n",
       "  -0.06998743116855621,\n",
       "  0.4471389651298523,\n",
       "  0.27148354053497314,\n",
       "  0.38768312335014343,\n",
       "  0.050008922815322876,\n",
       "  0.1449798196554184,\n",
       "  -0.24939538538455963,\n",
       "  -0.18481488525867462,\n",
       "  0.26002123951911926,\n",
       "  0.7309338450431824,\n",
       "  -0.1908637285232544,\n",
       "  -0.12086040526628494,\n",
       "  -0.29428496956825256,\n",
       "  0.2029840350151062,\n",
       "  0.5127744674682617,\n",
       "  0.04882161691784859,\n",
       "  0.03632368519902229,\n",
       "  -0.03795444220304489,\n",
       "  0.0570162758231163,\n",
       "  -0.3530772626399994,\n",
       "  -0.2600468099117279,\n",
       "  0.02558976411819458,\n",
       "  0.481987327337265,\n",
       "  -0.23851969838142395,\n",
       "  0.053207240998744965,\n",
       "  -0.0992901474237442,\n",
       "  -0.21974457800388336,\n",
       "  0.07711745798587799,\n",
       "  -0.0724732056260109,\n",
       "  0.227423757314682,\n",
       "  0.37555161118507385,\n",
       "  -0.5821549296379089,\n",
       "  -0.2759298086166382,\n",
       "  0.2888561189174652,\n",
       "  -0.42804625630378723,\n",
       "  0.08991479873657227,\n",
       "  -0.09715867042541504,\n",
       "  -0.053957659751176834,\n",
       "  0.018872302025556564,\n",
       "  -0.14844630658626556,\n",
       "  -1.6140402555465698,\n",
       "  0.013644031248986721,\n",
       "  -0.17028409242630005,\n",
       "  -0.24518747627735138,\n",
       "  -0.2509000599384308,\n",
       "  -0.25825849175453186,\n",
       "  0.20544780790805817,\n",
       "  -0.04068613424897194,\n",
       "  0.014420970343053341,\n",
       "  0.18535706400871277,\n",
       "  0.24215617775917053,\n",
       "  0.2138786017894745,\n",
       "  -0.01805885322391987,\n",
       "  0.09168034791946411,\n",
       "  -0.0011974532390013337,\n",
       "  -0.6292529106140137,\n",
       "  -0.09730281680822372,\n",
       "  0.19591261446475983,\n",
       "  0.07122278213500977,\n",
       "  -0.24492284655570984,\n",
       "  0.2398938238620758,\n",
       "  0.436998188495636,\n",
       "  -0.510779857635498,\n",
       "  -0.030680522322654724,\n",
       "  0.08849441260099411,\n",
       "  -0.36646226048469543,\n",
       "  -0.26844021677970886,\n",
       "  -0.1451343595981598,\n",
       "  -0.087521031498909,\n",
       "  -0.37561342120170593,\n",
       "  0.12796548008918762,\n",
       "  5.279024124145508,\n",
       "  -0.46436911821365356,\n",
       "  0.40178874135017395,\n",
       "  -0.0667356625199318,\n",
       "  -0.23608005046844482,\n",
       "  0.07180527597665787,\n",
       "  0.003957087639719248,\n",
       "  -0.23573186993598938,\n",
       "  0.7087140083312988,\n",
       "  0.08416381478309631,\n",
       "  -0.30598804354667664,\n",
       "  -0.38289615511894226,\n",
       "  0.08756222575902939,\n",
       "  -0.2656835615634918,\n",
       "  -0.468847393989563,\n",
       "  0.5891886949539185,\n",
       "  0.48189687728881836,\n",
       "  0.018071472644805908,\n",
       "  -0.13798373937606812,\n",
       "  -0.45806968212127686,\n",
       "  -0.22840453684329987,\n",
       "  -0.13279113173484802,\n",
       "  0.2890258729457855,\n",
       "  -0.0983775183558464,\n",
       "  -0.003096083179116249,\n",
       "  0.11005320399999619,\n",
       "  0.04684165120124817,\n",
       "  -0.24772413074970245,\n",
       "  -0.25268110632896423,\n",
       "  -0.551250696182251,\n",
       "  -0.4553067684173584,\n",
       "  0.0873100608587265,\n",
       "  -0.010366866365075111,\n",
       "  0.09937278181314468,\n",
       "  0.17264559864997864,\n",
       "  -0.2589137554168701,\n",
       "  0.26454371213912964,\n",
       "  0.01139400340616703,\n",
       "  0.12661902606487274,\n",
       "  0.037841398268938065,\n",
       "  0.21742232143878937,\n",
       "  -0.03245485946536064,\n",
       "  0.9129306674003601,\n",
       "  -0.22696202993392944,\n",
       "  0.6827805638313293,\n",
       "  0.17079856991767883,\n",
       "  -0.09364239871501923,\n",
       "  -0.20079900324344635,\n",
       "  0.08401007950305939,\n",
       "  0.5979773998260498,\n",
       "  0.26535600423812866,\n",
       "  0.44375020265579224,\n",
       "  0.32992807030677795,\n",
       "  -0.24312888085842133,\n",
       "  0.19153156876564026,\n",
       "  0.27941256761550903,\n",
       "  -0.018677063286304474,\n",
       "  -0.4274774491786957,\n",
       "  -0.06081341952085495,\n",
       "  0.1775340735912323,\n",
       "  -0.24959242343902588,\n",
       "  -0.09303038567304611,\n",
       "  -0.09846062958240509,\n",
       "  -0.12613578140735626,\n",
       "  0.2794888913631439,\n",
       "  -0.14563530683517456,\n",
       "  0.3424358367919922,\n",
       "  -0.1571909636259079,\n",
       "  -0.16999778151512146,\n",
       "  -0.29515957832336426,\n",
       "  -0.08421314507722855,\n",
       "  0.32786402106285095,\n",
       "  -0.39575618505477905,\n",
       "  -0.05643673613667488,\n",
       "  0.21005235612392426,\n",
       "  -0.11479989439249039,\n",
       "  -0.34008580446243286,\n",
       "  -0.11830156296491623,\n",
       "  -0.06051449850201607,\n",
       "  0.060985222458839417,\n",
       "  -0.06404094398021698,\n",
       "  0.0643882155418396,\n",
       "  0.09807409346103668,\n",
       "  -0.29856282472610474,\n",
       "  0.22916296124458313,\n",
       "  0.04429050534963608,\n",
       "  -0.6117652654647827,\n",
       "  -0.47092941403388977,\n",
       "  0.17326243221759796,\n",
       "  0.3423256576061249,\n",
       "  -0.14437486231327057,\n",
       "  -0.006296235602349043,\n",
       "  -0.2147475779056549,\n",
       "  -0.373251736164093,\n",
       "  -0.028155464679002762,\n",
       "  0.17137522995471954,\n",
       "  -0.060499779880046844,\n",
       "  -0.160186767578125,\n",
       "  -0.12453427165746689,\n",
       "  0.42856451869010925,\n",
       "  0.2802847921848297,\n",
       "  0.23315510153770447,\n",
       "  0.03134774789214134,\n",
       "  0.03769713267683983,\n",
       "  -0.03338547796010971,\n",
       "  0.1538269817829132,\n",
       "  0.11292169243097305,\n",
       "  0.06274638324975967,\n",
       "  0.05727037042379379,\n",
       "  0.01334167830646038,\n",
       "  0.4293939471244812,\n",
       "  0.13517916202545166,\n",
       "  0.04623075947165489,\n",
       "  -0.2172391563653946,\n",
       "  0.1799410879611969,\n",
       "  0.1238914430141449,\n",
       "  0.20943868160247803,\n",
       "  -0.3388269543647766,\n",
       "  -7.1219563484191895,\n",
       "  0.043910738080739975,\n",
       "  -0.058724693953990936,\n",
       "  0.25308188796043396,\n",
       "  -0.30798089504241943,\n",
       "  -0.3938092887401581,\n",
       "  0.2199382781982422,\n",
       "  0.07030060887336731,\n",
       "  0.1951795220375061,\n",
       "  0.23096834123134613,\n",
       "  -0.29600897431373596,\n",
       "  -0.22928179800510406,\n",
       "  -2.066899061203003,\n",
       "  0.17040173709392548,\n",
       "  -0.23896314203739166,\n",
       "  -0.09689678251743317,\n",
       "  0.03125382214784622,\n",
       "  -0.9744086265563965,\n",
       "  -0.2883668541908264,\n",
       "  0.4743227958679199,\n",
       "  -0.2015887349843979,\n",
       "  0.052730850875377655,\n",
       "  0.27221983671188354,\n",
       "  0.35693424940109253,\n",
       "  -0.026011183857917786,\n",
       "  0.37189626693725586,\n",
       "  0.020759498700499535,\n",
       "  -0.1349090039730072,\n",
       "  -0.16166608035564423,\n",
       "  -0.20395895838737488,\n",
       "  -0.14274996519088745,\n",
       "  0.037083160132169724,\n",
       "  0.3855258822441101,\n",
       "  -0.3890123963356018,\n",
       "  -0.19893671572208405,\n",
       "  -0.1924058347940445,\n",
       "  0.03886658698320389,\n",
       "  -0.021559489890933037,\n",
       "  0.052433986216783524,\n",
       "  -0.3599354922771454,\n",
       "  -0.08929159492254257,\n",
       "  -0.008810669183731079,\n",
       "  0.4221988022327423,\n",
       "  0.12119881808757782,\n",
       "  -0.577661395072937,\n",
       "  -0.2685125470161438,\n",
       "  -0.4336092472076416,\n",
       "  -2.1073498725891113,\n",
       "  0.3477879762649536,\n",
       "  -0.3043898940086365,\n",
       "  0.585471510887146,\n",
       "  0.4278703033924103,\n",
       "  -0.343013197183609,\n",
       "  0.09986734390258789,\n",
       "  -0.350171834230423,\n",
       "  -0.1253969967365265,\n",
       "  0.0003185273380950093,\n",
       "  0.23262692987918854,\n",
       "  0.34367749094963074,\n",
       "  -0.4631279706954956,\n",
       "  -0.5980153679847717,\n",
       "  0.007200243417173624,\n",
       "  0.03653121739625931,\n",
       "  -0.043554309755563736,\n",
       "  -0.0449039563536644,\n",
       "  -0.045544061809778214,\n",
       "  -0.6824705004692078,\n",
       "  0.05264248698949814,\n",
       "  0.16175353527069092,\n",
       "  -0.08200810849666595,\n",
       "  0.05354122817516327,\n",
       "  -0.1341138333082199,\n",
       "  -0.4058241844177246,\n",
       "  0.10215524584054947,\n",
       "  0.00034791557118296623,\n",
       "  0.003382177324965596,\n",
       "  0.16311341524124146,\n",
       "  -0.11166113615036011,\n",
       "  -0.154036283493042,\n",
       "  0.1859486848115921,\n",
       "  -0.2323894500732422,\n",
       "  -0.1366141140460968,\n",
       "  0.38236203789711,\n",
       "  0.2807565927505493,\n",
       "  -0.030482912436127663,\n",
       "  0.04763701185584068,\n",
       "  -0.36608952283859253,\n",
       "  0.1987907588481903,\n",
       "  0.10331416875123978,\n",
       "  0.33274880051612854,\n",
       "  -0.18804532289505005,\n",
       "  -0.03520640358328819,\n",
       "  -0.032214321196079254,\n",
       "  0.029825357720255852,\n",
       "  -0.08237835764884949,\n",
       "  0.36488181352615356,\n",
       "  0.00939459353685379,\n",
       "  0.015518737025558949,\n",
       "  -0.4267636835575104,\n",
       "  -0.12939997017383575,\n",
       "  -0.06223178282380104,\n",
       "  0.22658458352088928,\n",
       "  0.011430365964770317,\n",
       "  -0.5648561716079712,\n",
       "  0.10124287754297256,\n",
       "  0.07408852130174637,\n",
       "  2.7907848358154297,\n",
       "  -0.07582181692123413,\n",
       "  -0.35848262906074524,\n",
       "  0.10117810219526291,\n",
       "  0.1853293925523758,\n",
       "  0.06836286932229996,\n",
       "  -0.33418354392051697,\n",
       "  0.5970680713653564,\n",
       "  1.5461478233337402,\n",
       "  0.17123347520828247,\n",
       "  -0.37065988779067993,\n",
       "  -0.1718408316373825,\n",
       "  0.27831560373306274,\n",
       "  0.05533670634031296,\n",
       "  0.03814667463302612,\n",
       "  0.5273491144180298,\n",
       "  -0.10372202843427658,\n",
       "  0.04024892672896385,\n",
       "  0.07397404313087463,\n",
       "  -0.20302554965019226,\n",
       "  0.2683877944946289,\n",
       "  -0.305142343044281,\n",
       "  -0.1155967116355896,\n",
       "  -0.1051236167550087,\n",
       "  -0.23183311522006989,\n",
       "  -0.05216100066900253,\n",
       "  -0.4244546592235565,\n",
       "  -0.33858466148376465,\n",
       "  0.014682871289551258,\n",
       "  -0.1381831169128418,\n",
       "  -0.031268078833818436,\n",
       "  0.14091378450393677,\n",
       "  0.02273581176996231,\n",
       "  1.0597997903823853,\n",
       "  -0.12687821686267853,\n",
       "  0.4270680546760559,\n",
       "  0.031140347942709923,\n",
       "  -0.2051699310541153,\n",
       "  0.12190999835729599,\n",
       "  -0.12022299319505692,\n",
       "  -0.1372247338294983,\n",
       "  0.10074654221534729,\n",
       "  -0.7985782027244568,\n",
       "  0.656240701675415,\n",
       "  0.00724755646660924,\n",
       "  0.19914500415325165,\n",
       "  -0.24037568271160126,\n",
       "  -0.15988902747631073,\n",
       "  -0.03395393118262291,\n",
       "  -0.36462756991386414,\n",
       "  -0.1277415007352829,\n",
       "  -0.216185063123703,\n",
       "  -0.3675285875797272,\n",
       "  0.26699382066726685,\n",
       "  0.2562674880027771,\n",
       "  -0.044151850044727325,\n",
       "  -0.29541343450546265,\n",
       "  0.145339697599411,\n",
       "  -0.22958248853683472,\n",
       "  -0.14497798681259155,\n",
       "  0.10745516419410706,\n",
       "  0.10673496127128601,\n",
       "  0.01867067627608776,\n",
       "  0.3060412108898163,\n",
       "  -0.42884278297424316,\n",
       "  -0.01234680600464344,\n",
       "  0.2791489064693451,\n",
       "  -0.08936358988285065,\n",
       "  -0.28195920586586,\n",
       "  -0.4140729010105133,\n",
       "  0.17586320638656616,\n",
       "  -0.06495803594589233,\n",
       "  -2.2368669509887695,\n",
       "  0.2552701532840729,\n",
       "  -0.06851740926504135,\n",
       "  -0.03875264897942543,\n",
       "  -0.24389615654945374,\n",
       "  -0.42118215560913086,\n",
       "  0.23717057704925537,\n",
       "  0.09633471071720123,\n",
       "  -0.1287880539894104,\n",
       "  0.2594984173774719,\n",
       "  0.29861608147621155,\n",
       "  1.7891244888305664,\n",
       "  0.09430627524852753,\n",
       "  -0.2491118162870407,\n",
       "  -0.2500072717666626,\n",
       "  -0.40343335270881653,\n",
       "  0.1587466150522232,\n",
       "  0.13502603769302368,\n",
       "  -0.026078753173351288,\n",
       "  -0.2468750774860382,\n",
       "  -0.30434730648994446,\n",
       "  1.567744493484497,\n",
       "  0.11491361260414124,\n",
       "  0.2718205451965332,\n",
       "  0.27123960852622986,\n",
       "  -0.20121519267559052,\n",
       "  -0.0062218960374593735,\n",
       "  0.06541308760643005,\n",
       "  0.2907083034515381,\n",
       "  0.29823026061058044,\n",
       "  -0.24748368561267853,\n",
       "  0.32527637481689453,\n",
       "  -0.033468734472990036]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tx2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c8ebf232dea6577f6ceae92998c8d758983422845017e0e774d4bf7f66f7ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

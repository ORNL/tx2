{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Default Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the default approach as described in the \"Basic Usage\" docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "# import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through sklearn via below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15371b4053e4474fb14475d122a2e1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1= self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :]) # use just the [CLS] output embedding\n",
    "        return output\n",
    "    \n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text[text.index(\"\\n\\n\")+2:]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rows = []\n",
    "for i in range(len(train_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(train_data[\"data\"][i])\n",
    "    row[\"target\"] = train_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    train_rows.append(row)\n",
    "train_df = pd.DataFrame(train_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = []\n",
    "for i in range(len(test_data[\"data\"])):\n",
    "    row = {}\n",
    "    row[\"text\"] = clean_text(test_data[\"data\"][i])\n",
    "    row[\"target\"] = test_data['target'][i]\n",
    "    if row[\"text\"] == \"\" or row[\"text\"] == \" \": continue\n",
    "    test_rows.append(row)\n",
    "test_df = pd.DataFrame(test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11296\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.target[index], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "train_set = EncodedSet(train_df, tokenizer, 256)\n",
    "test_set = EncodedSet(test_df[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {'batch_size': 16,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 2,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    loss_history = []\n",
    "    for _,data in tqdm(enumerate(train_loader, 0), total=len(train_loader), desc=f\"Epoch {epoch}\"):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _%100==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7227f35917eb420b8c21f29bce51d718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 0'), FloatProgress(value=0.0, max=706.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  3.0206263065338135\n",
      "Epoch: 0, Loss:  2.1934101581573486\n",
      "Epoch: 0, Loss:  1.4502356052398682\n",
      "Epoch: 0, Loss:  1.2169358730316162\n",
      "Epoch: 0, Loss:  0.9175316095352173\n",
      "Epoch: 0, Loss:  0.7663290500640869\n",
      "Epoch: 0, Loss:  0.241717129945755\n",
      "Epoch: 0, Loss:  0.5925803184509277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = {}\n",
    "for index, entry in enumerate(train_data[\"target_names\"]):\n",
    "    encodings[entry] = index\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.20 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9892/1149527777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDashboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lab\\tx2\\tx2\\wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# TODO: not crazy about this, but library agnosticism later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tx2test\\lib\\site-packages\\umap_learn-0.5.2-py3.8.egg\\umap\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimplefilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mumap_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tx2test\\lib\\site-packages\\umap_learn-0.5.2-py3.8.egg\\umap\\umap_.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtril\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_tril\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtriu\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_triu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistances\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tx2test\\lib\\site-packages\\numba-0.54.1-py3.8-win-amd64.egg\\numba\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[0m_ensure_llvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;31m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tx2test\\lib\\site-packages\\numba-0.54.1-py3.8-win-amd64.egg\\numba\\__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.17 or greater\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Numba needs NumPy 1.20 or less\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.20 or less"
     ]
    }
   ],
   "source": [
    "from tx2.wrapper import Wrapper\n",
    "from tx2.dashboard import Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cache path not found, creating...\n",
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:Running classifier...\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "INFO:root:Saving predictions...\n",
      "INFO:root:Writing to data/predictions.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:Embedding training and testing datasets\n",
      "INFO:root:Saving embeddings...\n",
      "INFO:root:Writing to data/embedding_training.json\n",
      "INFO:root:Writing to data/embedding_testing.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:Training projector...\n",
      "INFO:root:Applying projector to test dataset...\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Writing to data/projections_training.json\n",
      "INFO:root:Writing to data/projections_testing.json\n",
      "INFO:root:Writing to data/projector.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:Computing salience maps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc85b28f98034430892239a6203bb6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "INFO:root:Saving salience maps...\n",
      "INFO:root:Writing to data/salience.pkl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:Clustering projections...\n",
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_texts=train_df.text,\n",
    "    train_labels=train_df.target,\n",
    "    test_texts=test_df.text[:2000],\n",
    "    test_labels=test_df.target[:2000],\n",
    "    encodings=encodings, \n",
    "    classifier=model, \n",
    "    language_model=model.l1, \n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True\n",
    ")\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b55a6447c4e76b19f0d13c7a06cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(childâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/81n/miniconda3/envs/tx2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "import matplotlib.pyplot as plt\n",
    "dash = Dashboard(wrapper)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper.recompute_visual_clusterings(\"KMeans\", clustering_args=dict(n_clusters=18))\n",
    "# wrapper.recompute_visual_clusterings(\"OPTICS\", clustering_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.8178814649581909,\n",
       "  -0.1121731549501419,\n",
       "  -0.5409483313560486,\n",
       "  0.21581129729747772,\n",
       "  0.2682485580444336,\n",
       "  -0.6747360229492188,\n",
       "  1.249107003211975,\n",
       "  0.9179522395133972,\n",
       "  1.154718041419983,\n",
       "  -0.9459081292152405,\n",
       "  0.5081962943077087,\n",
       "  -0.27006587386131287,\n",
       "  0.19237199425697327,\n",
       "  -0.34899014234542847,\n",
       "  0.6782020330429077,\n",
       "  0.04995761811733246,\n",
       "  -0.22134341299533844,\n",
       "  -0.2886699438095093,\n",
       "  -0.5444299578666687,\n",
       "  -0.4451519846916199]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6664102077484131,\n",
       "  0.37331387400627136,\n",
       "  0.337298721075058,\n",
       "  -0.8685522079467773,\n",
       "  -0.6003721356391907,\n",
       "  -0.25507545471191406,\n",
       "  0.12168938666582108,\n",
       "  0.04850916191935539,\n",
       "  0.29078394174575806,\n",
       "  -1.1002589464187622,\n",
       "  0.1251598745584488,\n",
       "  0.1908818483352661,\n",
       "  0.017635632306337357,\n",
       "  0.39517685770988464,\n",
       "  -0.3626023530960083,\n",
       "  0.8305122256278992,\n",
       "  0.3448742628097534,\n",
       "  0.6485928893089294,\n",
       "  0.38122469186782837,\n",
       "  -0.3653854727745056,\n",
       "  0.3282410800457001,\n",
       "  -0.651463508605957,\n",
       "  0.9279282093048096,\n",
       "  -0.8284140825271606,\n",
       "  -0.014791159890592098,\n",
       "  -0.04969337582588196,\n",
       "  0.1139974296092987,\n",
       "  0.08380938321352005,\n",
       "  -0.05369633436203003,\n",
       "  0.04894104227423668,\n",
       "  -0.9453793168067932,\n",
       "  0.38021811842918396,\n",
       "  -0.11028444766998291,\n",
       "  0.21194276213645935,\n",
       "  -0.03325001150369644,\n",
       "  -0.30094578862190247,\n",
       "  0.38758620619773865,\n",
       "  -0.05283786728978157,\n",
       "  -0.5122196078300476,\n",
       "  -0.3037961721420288,\n",
       "  -0.9389336109161377,\n",
       "  -0.24832458794116974,\n",
       "  0.5926793813705444,\n",
       "  -0.4574815630912781,\n",
       "  0.29105061292648315,\n",
       "  0.07457920163869858,\n",
       "  0.07914511859416962,\n",
       "  -0.17714892327785492,\n",
       "  -1.0188835859298706,\n",
       "  0.20833194255828857,\n",
       "  -0.32710424065589905,\n",
       "  0.9033678770065308,\n",
       "  0.020552201196551323,\n",
       "  0.8283553123474121,\n",
       "  -0.025891534984111786,\n",
       "  -0.10972100496292114,\n",
       "  -0.2354191690683365,\n",
       "  0.15094193816184998,\n",
       "  -0.17109917104244232,\n",
       "  0.30401504039764404,\n",
       "  -0.3191044330596924,\n",
       "  -0.333404541015625,\n",
       "  0.20229476690292358,\n",
       "  0.6896889805793762,\n",
       "  0.03321113809943199,\n",
       "  -0.36051321029663086,\n",
       "  0.0564553327858448,\n",
       "  -0.7925944328308105,\n",
       "  0.025517065078020096,\n",
       "  0.012500975281000137,\n",
       "  -0.2814570963382721,\n",
       "  0.18232809007167816,\n",
       "  0.5714581608772278,\n",
       "  1.2110038995742798,\n",
       "  1.0180429220199585,\n",
       "  -0.5765681862831116,\n",
       "  -0.0014546735910698771,\n",
       "  -0.13028310239315033,\n",
       "  -0.37283527851104736,\n",
       "  0.3052072823047638,\n",
       "  0.2822440564632416,\n",
       "  0.045827776193618774,\n",
       "  -0.43208861351013184,\n",
       "  -0.2412802278995514,\n",
       "  -0.1549464464187622,\n",
       "  0.38185369968414307,\n",
       "  -0.06335389614105225,\n",
       "  0.2550264298915863,\n",
       "  -0.22305870056152344,\n",
       "  -0.6778595447540283,\n",
       "  -0.4065686762332916,\n",
       "  0.2729409635066986,\n",
       "  -0.9006948471069336,\n",
       "  -0.16366247832775116,\n",
       "  -0.2597365379333496,\n",
       "  0.5724727511405945,\n",
       "  0.1474035084247589,\n",
       "  0.5710075497627258,\n",
       "  5.475398540496826,\n",
       "  0.14800001680850983,\n",
       "  0.20960605144500732,\n",
       "  -0.7366184592247009,\n",
       "  -0.24895165860652924,\n",
       "  -0.5246017575263977,\n",
       "  0.15053978562355042,\n",
       "  -0.6082779765129089,\n",
       "  -0.20753350853919983,\n",
       "  -0.4539145827293396,\n",
       "  0.4122779071331024,\n",
       "  0.653746485710144,\n",
       "  0.625603437423706,\n",
       "  0.2332894504070282,\n",
       "  0.5744681358337402,\n",
       "  0.4220547676086426,\n",
       "  0.4402552545070648,\n",
       "  -0.17048490047454834,\n",
       "  -0.2547055184841156,\n",
       "  -0.370760977268219,\n",
       "  -0.14442409574985504,\n",
       "  -0.2641841769218445,\n",
       "  0.10121960192918777,\n",
       "  0.6248742938041687,\n",
       "  1.57511305809021,\n",
       "  0.28964829444885254,\n",
       "  -1.100856900215149,\n",
       "  0.01137615367770195,\n",
       "  0.47573813796043396,\n",
       "  0.2713125944137573,\n",
       "  0.442862331867218,\n",
       "  0.1207989975810051,\n",
       "  -0.8406433463096619,\n",
       "  0.3659445643424988,\n",
       "  -0.3019793629646301,\n",
       "  0.21943698823451996,\n",
       "  -0.12683922052383423,\n",
       "  0.2734944224357605,\n",
       "  -0.16403186321258545,\n",
       "  0.533710777759552,\n",
       "  -1.6463197469711304,\n",
       "  -0.3435959815979004,\n",
       "  -0.41425326466560364,\n",
       "  -0.4362930953502655,\n",
       "  0.37184837460517883,\n",
       "  -0.7140821218490601,\n",
       "  -0.1610640436410904,\n",
       "  2.5936291217803955,\n",
       "  0.39540743827819824,\n",
       "  0.31644245982170105,\n",
       "  0.27263757586479187,\n",
       "  -1.1364336013793945,\n",
       "  0.7983266711235046,\n",
       "  0.8028755187988281,\n",
       "  -0.9476365447044373,\n",
       "  0.6497874855995178,\n",
       "  0.7518437504768372,\n",
       "  -0.6123835444450378,\n",
       "  -0.43001845479011536,\n",
       "  -0.2748676538467407,\n",
       "  -0.6835700869560242,\n",
       "  0.2831452190876007,\n",
       "  -0.32811129093170166,\n",
       "  -0.08475241810083389,\n",
       "  -0.7808023691177368,\n",
       "  0.2645663022994995,\n",
       "  0.5186124444007874,\n",
       "  -0.26505962014198303,\n",
       "  0.8912575244903564,\n",
       "  -0.7228949069976807,\n",
       "  0.3262900710105896,\n",
       "  -0.3069688677787781,\n",
       "  -0.5772968530654907,\n",
       "  -0.6883891820907593,\n",
       "  -1.561535358428955,\n",
       "  0.9303983449935913,\n",
       "  -0.2131338268518448,\n",
       "  -0.21272119879722595,\n",
       "  0.3340873122215271,\n",
       "  0.7121709585189819,\n",
       "  -0.4775104820728302,\n",
       "  -0.23537896573543549,\n",
       "  -0.5299550890922546,\n",
       "  0.5538855791091919,\n",
       "  0.5047098994255066,\n",
       "  0.01800089329481125,\n",
       "  -0.4168740212917328,\n",
       "  -0.21434926986694336,\n",
       "  0.11289889365434647,\n",
       "  -0.9306021332740784,\n",
       "  -0.601888120174408,\n",
       "  -1.1214185953140259,\n",
       "  0.09955959022045135,\n",
       "  -0.2212885320186615,\n",
       "  0.01913992129266262,\n",
       "  -0.13532628118991852,\n",
       "  0.5912900567054749,\n",
       "  0.2562481164932251,\n",
       "  -0.471919983625412,\n",
       "  -0.20966650545597076,\n",
       "  -0.6498093008995056,\n",
       "  0.047328487038612366,\n",
       "  -0.19534841179847717,\n",
       "  -0.5219071507453918,\n",
       "  0.11453317105770111,\n",
       "  -0.07747969776391983,\n",
       "  0.34181228280067444,\n",
       "  -0.6142288446426392,\n",
       "  0.4895939826965332,\n",
       "  -0.26610076427459717,\n",
       "  1.3045637607574463,\n",
       "  0.1257265955209732,\n",
       "  -0.32893580198287964,\n",
       "  -0.07370675355195999,\n",
       "  0.06627175211906433,\n",
       "  -0.3094308078289032,\n",
       "  0.12716960906982422,\n",
       "  -0.842354416847229,\n",
       "  0.08429162949323654,\n",
       "  0.5694887638092041,\n",
       "  0.1943160742521286,\n",
       "  0.38430774211883545,\n",
       "  -0.47688621282577515,\n",
       "  0.1826484352350235,\n",
       "  -0.7283504605293274,\n",
       "  -0.45825695991516113,\n",
       "  0.3822261095046997,\n",
       "  -0.017054880037903786,\n",
       "  0.6873035430908203,\n",
       "  0.12401460856199265,\n",
       "  0.08154072612524033,\n",
       "  -0.25158894062042236,\n",
       "  0.4861030876636505,\n",
       "  0.8506457805633545,\n",
       "  -0.0774432048201561,\n",
       "  -0.5731611847877502,\n",
       "  0.3374532461166382,\n",
       "  0.07902520895004272,\n",
       "  -1.3224118947982788,\n",
       "  1.1045119762420654,\n",
       "  0.3529776930809021,\n",
       "  -0.1739366054534912,\n",
       "  0.22680366039276123,\n",
       "  -0.846035897731781,\n",
       "  0.6374323964118958,\n",
       "  -0.03318871557712555,\n",
       "  0.2503232955932617,\n",
       "  0.17549125850200653,\n",
       "  0.519474983215332,\n",
       "  0.39894938468933105,\n",
       "  0.279740571975708,\n",
       "  -0.06541263312101364,\n",
       "  0.8238254189491272,\n",
       "  -0.24984607100486755,\n",
       "  -0.07888093590736389,\n",
       "  -0.7120556831359863,\n",
       "  0.25432923436164856,\n",
       "  0.774031937122345,\n",
       "  0.681769073009491,\n",
       "  -1.5790237188339233,\n",
       "  0.42317622900009155,\n",
       "  -1.1369402408599854,\n",
       "  0.3916929066181183,\n",
       "  -3.2233052253723145,\n",
       "  0.3692805767059326,\n",
       "  0.2124013453722,\n",
       "  -0.4361185133457184,\n",
       "  0.1750422716140747,\n",
       "  0.20093081891536713,\n",
       "  0.3422713279724121,\n",
       "  -0.2987052798271179,\n",
       "  -0.03978382796049118,\n",
       "  0.8672553896903992,\n",
       "  -0.5445775985717773,\n",
       "  -0.2854652404785156,\n",
       "  -0.36980700492858887,\n",
       "  0.07942765206098557,\n",
       "  -0.06585191935300827,\n",
       "  -0.34026309847831726,\n",
       "  0.08785777539014816,\n",
       "  -0.28120025992393494,\n",
       "  -0.6775339841842651,\n",
       "  0.40868330001831055,\n",
       "  1.091659426689148,\n",
       "  -1.2449783086776733,\n",
       "  0.38999614119529724,\n",
       "  0.011545948684215546,\n",
       "  0.4597250521183014,\n",
       "  0.6887055039405823,\n",
       "  -0.8144379258155823,\n",
       "  0.037283092737197876,\n",
       "  3.7737128734588623,\n",
       "  -0.096852608025074,\n",
       "  -0.46017226576805115,\n",
       "  -0.017019568011164665,\n",
       "  -1.1247695684432983,\n",
       "  -0.26432397961616516,\n",
       "  -0.06738118827342987,\n",
       "  -0.7704915404319763,\n",
       "  0.070760577917099,\n",
       "  -0.3616270124912262,\n",
       "  -0.13060472905635834,\n",
       "  0.31432491540908813,\n",
       "  0.4544595181941986,\n",
       "  -0.4857627749443054,\n",
       "  0.22905422747135162,\n",
       "  -0.9954020977020264,\n",
       "  -0.24838973581790924,\n",
       "  0.37114569544792175,\n",
       "  0.13247448205947876,\n",
       "  -0.6606546640396118,\n",
       "  0.04138033464550972,\n",
       "  -0.5403993725776672,\n",
       "  -0.12801113724708557,\n",
       "  -0.008054865524172783,\n",
       "  0.4575677812099457,\n",
       "  -0.4326547682285309,\n",
       "  -0.17108739912509918,\n",
       "  -0.922261655330658,\n",
       "  -0.16510897874832153,\n",
       "  -0.2033335268497467,\n",
       "  -0.4169047474861145,\n",
       "  0.4245342016220093,\n",
       "  0.37799885869026184,\n",
       "  -0.647455632686615,\n",
       "  -0.5136665105819702,\n",
       "  -1.0431572198867798,\n",
       "  -0.20368632674217224,\n",
       "  0.2807459533214569,\n",
       "  -0.02067898027598858,\n",
       "  -0.6867409944534302,\n",
       "  0.23525722324848175,\n",
       "  -0.2867315709590912,\n",
       "  -0.11630593985319138,\n",
       "  -0.2421136051416397,\n",
       "  0.13044896721839905,\n",
       "  -0.680706799030304,\n",
       "  0.10218986123800278,\n",
       "  -0.36203300952911377,\n",
       "  0.3957729637622833,\n",
       "  0.027310017496347427,\n",
       "  -0.24116571247577667,\n",
       "  0.0840839222073555,\n",
       "  0.5228548645973206,\n",
       "  0.14343950152397156,\n",
       "  -0.7762009501457214,\n",
       "  -0.2549991011619568,\n",
       "  -0.19300462305545807,\n",
       "  0.71123868227005,\n",
       "  0.2608817517757416,\n",
       "  0.9030780792236328,\n",
       "  0.09953770041465759,\n",
       "  0.34953930974006653,\n",
       "  -0.10079769045114517,\n",
       "  -0.4355911612510681,\n",
       "  0.4772161841392517,\n",
       "  -0.10805711895227432,\n",
       "  0.09675697982311249,\n",
       "  0.09071692079305649,\n",
       "  -0.3937079906463623,\n",
       "  0.014204674400389194,\n",
       "  0.9483192563056946,\n",
       "  -0.29329168796539307,\n",
       "  -1.0797548294067383,\n",
       "  0.9791220426559448,\n",
       "  -0.27922487258911133,\n",
       "  0.5763322710990906,\n",
       "  -0.46859148144721985,\n",
       "  0.523037850856781,\n",
       "  -0.6161646842956543,\n",
       "  0.038386620581150055,\n",
       "  0.16853640973567963,\n",
       "  0.4462053179740906,\n",
       "  0.2654194235801697,\n",
       "  0.335673987865448,\n",
       "  0.20280396938323975,\n",
       "  0.178431898355484,\n",
       "  -0.2700342833995819,\n",
       "  0.06467727571725845,\n",
       "  0.8587360382080078,\n",
       "  -0.3380662202835083,\n",
       "  0.5521110892295837,\n",
       "  0.5577613711357117,\n",
       "  0.5980238914489746,\n",
       "  1.4139328002929688,\n",
       "  0.16830983757972717,\n",
       "  0.202920600771904,\n",
       "  0.07638099789619446,\n",
       "  -0.0652916207909584,\n",
       "  -0.09756142646074295,\n",
       "  -0.34576335549354553,\n",
       "  -0.13019821047782898,\n",
       "  0.4846070110797882,\n",
       "  -0.5009174346923828,\n",
       "  0.22239404916763306,\n",
       "  -0.49964439868927,\n",
       "  -0.6337224841117859,\n",
       "  0.4134974777698517,\n",
       "  0.36644071340560913,\n",
       "  0.5660147070884705,\n",
       "  0.8458536267280579,\n",
       "  -0.4963982105255127,\n",
       "  -0.7409420013427734,\n",
       "  0.2510237395763397,\n",
       "  -0.5348697900772095,\n",
       "  0.09565447270870209,\n",
       "  -0.2614578306674957,\n",
       "  0.3760053515434265,\n",
       "  -0.14182347059249878,\n",
       "  -0.2534395754337311,\n",
       "  -1.5514206886291504,\n",
       "  0.10257599502801895,\n",
       "  0.35353589057922363,\n",
       "  -0.4499528408050537,\n",
       "  -0.06687408685684204,\n",
       "  -0.3458044230937958,\n",
       "  0.3555006980895996,\n",
       "  -0.36736512184143066,\n",
       "  -0.27654385566711426,\n",
       "  -0.06803485006093979,\n",
       "  0.0024010620545595884,\n",
       "  -0.2792879343032837,\n",
       "  -0.10495732724666595,\n",
       "  0.6822336316108704,\n",
       "  0.5034608840942383,\n",
       "  -0.7234092950820923,\n",
       "  0.552311897277832,\n",
       "  0.061906684190034866,\n",
       "  0.11079353839159012,\n",
       "  -1.1896533966064453,\n",
       "  0.6121376156806946,\n",
       "  0.8667239546775818,\n",
       "  -0.7535315155982971,\n",
       "  0.2473176270723343,\n",
       "  0.44193997979164124,\n",
       "  0.5571994185447693,\n",
       "  -0.6156358122825623,\n",
       "  0.4092448353767395,\n",
       "  -0.28754666447639465,\n",
       "  -0.8755388259887695,\n",
       "  0.2212953269481659,\n",
       "  4.015242099761963,\n",
       "  -0.9499169588088989,\n",
       "  0.038978710770606995,\n",
       "  -0.0906473696231842,\n",
       "  -0.11110202223062515,\n",
       "  -1.0306596755981445,\n",
       "  -0.15089982748031616,\n",
       "  -0.46723735332489014,\n",
       "  1.0912202596664429,\n",
       "  -0.2608606815338135,\n",
       "  -0.758277416229248,\n",
       "  -0.5993179082870483,\n",
       "  -0.5542641282081604,\n",
       "  -0.6023985743522644,\n",
       "  -0.27891260385513306,\n",
       "  0.5535054206848145,\n",
       "  -0.08343721926212311,\n",
       "  0.7607680559158325,\n",
       "  0.16676102578639984,\n",
       "  -0.6044574975967407,\n",
       "  0.10399483144283295,\n",
       "  -0.21821941435337067,\n",
       "  0.42729127407073975,\n",
       "  -0.23035898804664612,\n",
       "  0.1509423851966858,\n",
       "  0.884285569190979,\n",
       "  0.2537215054035187,\n",
       "  -0.18303662538528442,\n",
       "  0.36322522163391113,\n",
       "  -0.32644540071487427,\n",
       "  -0.716954231262207,\n",
       "  0.33335819840431213,\n",
       "  0.06254447251558304,\n",
       "  -0.3353778123855591,\n",
       "  0.7887704372406006,\n",
       "  -0.49687814712524414,\n",
       "  0.47979241609573364,\n",
       "  -0.17004495859146118,\n",
       "  -0.02539960853755474,\n",
       "  0.28223752975463867,\n",
       "  0.8991106748580933,\n",
       "  0.21786049008369446,\n",
       "  1.32724130153656,\n",
       "  -0.4597925543785095,\n",
       "  0.8857175707817078,\n",
       "  0.4449489414691925,\n",
       "  -0.033436160534620285,\n",
       "  -0.26820406317710876,\n",
       "  -0.6150285601615906,\n",
       "  0.5035116672515869,\n",
       "  0.5973960757255554,\n",
       "  0.35886940360069275,\n",
       "  0.3019285798072815,\n",
       "  -0.5490908026695251,\n",
       "  0.08958102017641068,\n",
       "  0.9428814053535461,\n",
       "  -0.47260016202926636,\n",
       "  -0.5206637978553772,\n",
       "  -0.1115475594997406,\n",
       "  0.38512182235717773,\n",
       "  -0.6558445692062378,\n",
       "  -0.0799376368522644,\n",
       "  0.4492967426776886,\n",
       "  -0.4863613545894623,\n",
       "  -0.36463892459869385,\n",
       "  -0.04530109837651253,\n",
       "  0.3919910788536072,\n",
       "  -0.2919635772705078,\n",
       "  -0.31702378392219543,\n",
       "  0.06853244453668594,\n",
       "  -0.22465121746063232,\n",
       "  0.422122985124588,\n",
       "  -0.5938765406608582,\n",
       "  0.8851484656333923,\n",
       "  0.35828813910484314,\n",
       "  -0.3613705635070801,\n",
       "  -0.07975196838378906,\n",
       "  0.14756576716899872,\n",
       "  -0.2984780967235565,\n",
       "  0.24149104952812195,\n",
       "  -0.5833868980407715,\n",
       "  0.029073117300868034,\n",
       "  0.09110092371702194,\n",
       "  -0.2737034857273102,\n",
       "  0.8396980166435242,\n",
       "  0.13212457299232483,\n",
       "  -0.9900987148284912,\n",
       "  -0.19821546971797943,\n",
       "  0.1698628067970276,\n",
       "  0.3286958336830139,\n",
       "  -0.19004929065704346,\n",
       "  -0.1346089094877243,\n",
       "  -0.05077623948454857,\n",
       "  -0.9283325672149658,\n",
       "  -0.19280923902988434,\n",
       "  0.20183511078357697,\n",
       "  0.11517808586359024,\n",
       "  0.3328392207622528,\n",
       "  -0.1237107515335083,\n",
       "  0.16437631845474243,\n",
       "  -0.24938562512397766,\n",
       "  0.03792940080165863,\n",
       "  0.16389884054660797,\n",
       "  -0.09709237515926361,\n",
       "  -0.5412594676017761,\n",
       "  -0.07175884395837784,\n",
       "  0.4143776297569275,\n",
       "  0.26162347197532654,\n",
       "  -0.25746601819992065,\n",
       "  -0.17424745857715607,\n",
       "  0.3336132764816284,\n",
       "  0.0720054879784584,\n",
       "  0.292131245136261,\n",
       "  -0.25375884771347046,\n",
       "  0.4446958899497986,\n",
       "  0.9157384634017944,\n",
       "  0.5574973225593567,\n",
       "  -0.1398192197084427,\n",
       "  -5.82922887802124,\n",
       "  -0.045258693397045135,\n",
       "  0.35842588543891907,\n",
       "  0.14620035886764526,\n",
       "  -0.8364124298095703,\n",
       "  -0.5217216610908508,\n",
       "  -0.5331365466117859,\n",
       "  0.13079833984375,\n",
       "  -0.15475454926490784,\n",
       "  0.29766902327537537,\n",
       "  0.06785891205072403,\n",
       "  -0.0846044272184372,\n",
       "  -1.9525965452194214,\n",
       "  0.24297551810741425,\n",
       "  0.11948400735855103,\n",
       "  0.6690183877944946,\n",
       "  0.5192065238952637,\n",
       "  -1.1659365892410278,\n",
       "  -0.5990879535675049,\n",
       "  0.8111360669136047,\n",
       "  -0.21907074749469757,\n",
       "  -0.5254995226860046,\n",
       "  -0.214351087808609,\n",
       "  0.7400760054588318,\n",
       "  0.18120376765727997,\n",
       "  0.6845988035202026,\n",
       "  0.49964624643325806,\n",
       "  0.15193597972393036,\n",
       "  0.11906148493289948,\n",
       "  -0.1492103487253189,\n",
       "  -0.43357613682746887,\n",
       "  0.7699229717254639,\n",
       "  0.5317104458808899,\n",
       "  -0.5645238161087036,\n",
       "  -0.5658005475997925,\n",
       "  -0.5053988099098206,\n",
       "  -0.2273419201374054,\n",
       "  -0.2618153393268585,\n",
       "  0.29793187975883484,\n",
       "  -0.9586483836174011,\n",
       "  -0.04971204698085785,\n",
       "  -0.2857999801635742,\n",
       "  0.4206007122993469,\n",
       "  -0.09131112694740295,\n",
       "  -0.20850005745887756,\n",
       "  0.011016717180609703,\n",
       "  -0.6367856860160828,\n",
       "  -1.2836624383926392,\n",
       "  0.7263840436935425,\n",
       "  -0.2704424262046814,\n",
       "  0.5462461113929749,\n",
       "  0.061021286994218826,\n",
       "  -0.2103026807308197,\n",
       "  -0.25895658135414124,\n",
       "  0.010129651054739952,\n",
       "  0.05656784027814865,\n",
       "  -0.11934813857078552,\n",
       "  0.7622962594032288,\n",
       "  0.9935018420219421,\n",
       "  -0.13441163301467896,\n",
       "  -0.7501094937324524,\n",
       "  -0.22641649842262268,\n",
       "  0.7660291194915771,\n",
       "  -0.2062537968158722,\n",
       "  -0.10971473157405853,\n",
       "  -0.3376818001270294,\n",
       "  -0.5439897179603577,\n",
       "  -0.14565956592559814,\n",
       "  0.18624714016914368,\n",
       "  -0.03164495527744293,\n",
       "  -0.08320894837379456,\n",
       "  -0.0973476991057396,\n",
       "  -0.06833042949438095,\n",
       "  -0.059110742062330246,\n",
       "  0.45225679874420166,\n",
       "  -0.4948614835739136,\n",
       "  -0.21817077696323395,\n",
       "  -0.08736510574817657,\n",
       "  -0.23762944340705872,\n",
       "  0.012431750074028969,\n",
       "  -0.4923018217086792,\n",
       "  -0.036932170391082764,\n",
       "  0.025076711550354958,\n",
       "  0.18249662220478058,\n",
       "  -0.26461148262023926,\n",
       "  -0.14472799003124237,\n",
       "  -0.7169675827026367,\n",
       "  0.580717921257019,\n",
       "  0.0831274539232254,\n",
       "  0.22194981575012207,\n",
       "  -0.04630638286471367,\n",
       "  -0.08150189369916916,\n",
       "  0.002924812724813819,\n",
       "  0.7969104647636414,\n",
       "  -0.6091328263282776,\n",
       "  0.6689724326133728,\n",
       "  -0.16345757246017456,\n",
       "  -0.07240181416273117,\n",
       "  -0.30025386810302734,\n",
       "  -0.3135923147201538,\n",
       "  0.09973736107349396,\n",
       "  0.5992611646652222,\n",
       "  -0.2740838825702667,\n",
       "  -0.10804227739572525,\n",
       "  -0.263589084148407,\n",
       "  0.10389507561922073,\n",
       "  2.5624639987945557,\n",
       "  0.05012338608503342,\n",
       "  -0.3246603310108185,\n",
       "  0.6125397682189941,\n",
       "  -0.29978999495506287,\n",
       "  0.6503714919090271,\n",
       "  -0.2642173171043396,\n",
       "  -0.029213087633252144,\n",
       "  1.3386735916137695,\n",
       "  0.4856244623661041,\n",
       "  -0.10397408902645111,\n",
       "  -0.7789874076843262,\n",
       "  0.815049409866333,\n",
       "  -0.49269813299179077,\n",
       "  -0.27448388934135437,\n",
       "  -0.11915802210569382,\n",
       "  -0.43815872073173523,\n",
       "  0.21803918480873108,\n",
       "  -0.037903137505054474,\n",
       "  -0.17559516429901123,\n",
       "  0.5319943428039551,\n",
       "  -0.3663312792778015,\n",
       "  0.12255378067493439,\n",
       "  -0.863267719745636,\n",
       "  -0.008006502874195576,\n",
       "  -0.2135714888572693,\n",
       "  -0.7591428756713867,\n",
       "  -0.5479644536972046,\n",
       "  0.48141151666641235,\n",
       "  -0.1961868554353714,\n",
       "  0.1025443896651268,\n",
       "  0.27947208285331726,\n",
       "  -0.013646438717842102,\n",
       "  0.1743445098400116,\n",
       "  -0.0027527243364602327,\n",
       "  0.8786623477935791,\n",
       "  -0.02264455147087574,\n",
       "  -0.48385322093963623,\n",
       "  0.6066804528236389,\n",
       "  -0.15369649231433868,\n",
       "  -0.14990010857582092,\n",
       "  -0.37642329931259155,\n",
       "  -1.308738112449646,\n",
       "  0.5794212222099304,\n",
       "  0.0035138966049999,\n",
       "  -0.24758805334568024,\n",
       "  -0.0642046108841896,\n",
       "  -0.10385331511497498,\n",
       "  0.20039966702461243,\n",
       "  -0.4644095003604889,\n",
       "  -0.10794268548488617,\n",
       "  -0.035109300166368484,\n",
       "  -0.8924391865730286,\n",
       "  1.2964131832122803,\n",
       "  0.8417264223098755,\n",
       "  -0.1474277377128601,\n",
       "  -0.40842172503471375,\n",
       "  -0.3388744592666626,\n",
       "  0.05154991149902344,\n",
       "  -0.3359699845314026,\n",
       "  -0.44935160875320435,\n",
       "  0.053809843957424164,\n",
       "  0.6039788722991943,\n",
       "  0.6217913031578064,\n",
       "  -0.5701568722724915,\n",
       "  -0.04618958383798599,\n",
       "  0.3967037498950958,\n",
       "  0.08323059231042862,\n",
       "  -0.08893710374832153,\n",
       "  -0.29205626249313354,\n",
       "  -0.30350545048713684,\n",
       "  -0.38018590211868286,\n",
       "  -1.9292819499969482,\n",
       "  -0.04475834220647812,\n",
       "  -0.10642554610967636,\n",
       "  0.8054044842720032,\n",
       "  -0.11286763846874237,\n",
       "  -0.032216742634773254,\n",
       "  0.5176336765289307,\n",
       "  -0.18916518986225128,\n",
       "  0.17813920974731445,\n",
       "  0.11741168051958084,\n",
       "  0.11468882113695145,\n",
       "  0.7750343084335327,\n",
       "  0.6036731004714966,\n",
       "  -0.5225254893302917,\n",
       "  -0.7191956043243408,\n",
       "  -0.18438021838665009,\n",
       "  0.18555089831352234,\n",
       "  -0.05841398239135742,\n",
       "  -0.08824952691793442,\n",
       "  0.3343813419342041,\n",
       "  -0.42103227972984314,\n",
       "  1.1620961427688599,\n",
       "  0.10917346179485321,\n",
       "  0.4245847165584564,\n",
       "  0.48953354358673096,\n",
       "  -0.16995631158351898,\n",
       "  -0.216983363032341,\n",
       "  0.4801773726940155,\n",
       "  -0.429280549287796,\n",
       "  0.9987749457359314,\n",
       "  -0.6667927503585815,\n",
       "  0.06503474712371826,\n",
       "  -0.11775443702936172]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

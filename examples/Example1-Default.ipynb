{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Default Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the default approach as described in the \"Basic Usage\" docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LRU_CACHE_CAPACITY'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through huggingfacevia below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "WARNING:datasets.builder:Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
      "WARNING:datasets.builder:Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "WARNING:datasets.builder:Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# getting newsgroups data from huggingface\n",
    "train_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"train\"))\n",
    "test_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"test\"))\n",
    "\n",
    "# setting up pytorch device\n",
    "if cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1 = self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :])  # use just the [CLS] output embedding\n",
    "        return output\n",
    "\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # text = text[text.index(\"\\n\\n\") + 2 :]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you have Weitek's address/phone number? I'd...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye) DN...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>Wouldn't this require a hyper-sphere. In 3-spa...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11014 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      I was wondering if anyone out there could enli...      7   \n",
       "1      A fair number of brave souls who upgraded thei...      4   \n",
       "2      well folks, my mac plus finally gave up the gh...      4   \n",
       "3      Do you have Weitek's address/phone number? I'd...      1   \n",
       "4      From article <C5owCB.n3p@world.std.com>, by to...     14   \n",
       "...                                                  ...    ...   \n",
       "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye) DN...     13   \n",
       "11310  I have a (very old) Mac 512k and a Mac Plus, b...      4   \n",
       "11311  I just installed a DX2-66 CPU in a clone mothe...      3   \n",
       "11312  Wouldn't this require a hyper-sphere. In 3-spa...      1   \n",
       "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...      8   \n",
       "\n",
       "                     label_text  \n",
       "0                     rec.autos  \n",
       "1         comp.sys.mac.hardware  \n",
       "2         comp.sys.mac.hardware  \n",
       "3                 comp.graphics  \n",
       "4                     sci.space  \n",
       "...                         ...  \n",
       "11309                   sci.med  \n",
       "11310     comp.sys.mac.hardware  \n",
       "11311  comp.sys.ibm.pc.hardware  \n",
       "11312             comp.graphics  \n",
       "11313           rec.motorcycles  \n",
       "\n",
       "[11014 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean long white space or extensive character returns\n",
    "train_data.text = train_data.text.apply(lambda x: clean_text(x))\n",
    "test_data.text = test_data.text.apply(lambda x: clean_text(x))\n",
    "\n",
    "# remove empty entries or trivially short ones\n",
    "train_cleaned = train_data[train_data[\"text\"].str.len() > 1]\n",
    "test_cleaned = test_data[test_data[\"text\"].str.len() > 1]\n",
    "train_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11014\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.data.label[index], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "train_cleaned.reset_index(drop=True, inplace=True)\n",
    "test_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_set = EncodedSet(train_cleaned, tokenizer, 256)\n",
    "test_set = EncodedSet(test_cleaned[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {\"batch_size\": 16, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "test_params = {\"batch_size\": 2, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    for _, data in tqdm(\n",
    "        enumerate(train_loader, start=0), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "    ):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _ % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss:  {loss.item()}\")\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780a2e7f3dbd4184a1ec20c932527c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  2.9448401927948\n",
      "Epoch: 0, Loss:  2.3907535076141357\n",
      "Epoch: 0, Loss:  1.3893176317214966\n",
      "Epoch: 0, Loss:  1.1983692646026611\n",
      "Epoch: 0, Loss:  1.0487241744995117\n",
      "Epoch: 0, Loss:  0.9903286695480347\n",
      "Epoch: 0, Loss:  0.7322931289672852\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = (\n",
    "    train_cleaned[[\"label\", \"label_text\"]]\n",
    "    .groupby([\"label_text\"])\n",
    "    .apply(lambda x: x[\"label\"].tolist()[0])\n",
    "    .to_dict()\n",
    ")\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tx2.dashboard import Dashboard\n",
    "from tx2.wrapper import Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # shouldn't be necessary since done in wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cache path found\n",
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:Running classifier...\n",
      "INFO:root:Saving predictions...\n",
      "INFO:root:Writing to data/predictions.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:Embedding training and testing datasets\n",
      "INFO:root:Saving embeddings...\n",
      "INFO:root:Writing to data/embedding_training.json\n",
      "INFO:root:Writing to data/embedding_testing.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:Training projector...\n",
      "INFO:root:Applying projector to test dataset...\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Writing to data/projections_training.json\n",
      "INFO:root:Writing to data/projections_testing.json\n",
      "INFO:root:Writing to data/projector.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:Computing salience maps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7c7b6cbc954e3384aa356d48109651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving salience maps...\n",
      "INFO:root:Writing to data/salience.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:Clustering projections...\n",
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_texts=train_cleaned.text,\n",
    "    train_labels=train_cleaned.label,\n",
    "    test_texts=test_cleaned.text[:2000],\n",
    "    test_labels=test_cleaned.label[:2000],\n",
    "    encodings=encodings,\n",
    "    classifier=model,\n",
    "    language_model=model.l1,\n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True,\n",
    ")\n",
    "wrapper.batch_size = 16\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper._compute_all_salience_maps_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a859281f59544a5199924ea1a0087d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(childâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dash = Dashboard(wrapper)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Clustering projections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "wrapper.recompute_visual_clusterings(\"KMeans\", clustering_args=dict(n_clusters=20))\n",
    "# wrapper.recompute_visual_clusterings(\"OPTICS\", clustering_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.6407073140144348,\n",
       "  0.209745854139328,\n",
       "  -0.554280698299408,\n",
       "  -0.10038193315267563,\n",
       "  -0.585742712020874,\n",
       "  -0.5102531313896179,\n",
       "  0.7485013604164124,\n",
       "  2.611030101776123,\n",
       "  2.2923128604888916,\n",
       "  -0.4392798840999603,\n",
       "  0.2102343738079071,\n",
       "  -0.9190913438796997,\n",
       "  0.2795550525188446,\n",
       "  -0.559229850769043,\n",
       "  0.6471983194351196,\n",
       "  -0.800150990486145,\n",
       "  -0.3223843574523926,\n",
       "  -1.3777291774749756,\n",
       "  -0.5429782271385193,\n",
       "  -1.2546855211257935]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.739083468914032,\n",
       "  -0.45587679743766785,\n",
       "  0.15782608091831207,\n",
       "  -0.4461686909198761,\n",
       "  -0.6502417325973511,\n",
       "  1.2214070558547974,\n",
       "  1.05491304397583,\n",
       "  0.21597401797771454,\n",
       "  -0.195156067609787,\n",
       "  -1.5591607093811035,\n",
       "  -0.3290790319442749,\n",
       "  -0.119004987180233,\n",
       "  -0.36744827032089233,\n",
       "  0.8441649675369263,\n",
       "  -1.232364296913147,\n",
       "  -0.0953817144036293,\n",
       "  -0.0764038935303688,\n",
       "  0.4676598906517029,\n",
       "  1.46743905544281,\n",
       "  0.14781242609024048,\n",
       "  -0.3107600510120392,\n",
       "  -0.18111953139305115,\n",
       "  0.24683499336242676,\n",
       "  -0.2195391058921814,\n",
       "  -0.8971104621887207,\n",
       "  -0.2875467836856842,\n",
       "  0.471092164516449,\n",
       "  1.3381378650665283,\n",
       "  0.8087002635002136,\n",
       "  0.34628111124038696,\n",
       "  -0.7713687419891357,\n",
       "  0.4110075533390045,\n",
       "  -0.14410705864429474,\n",
       "  -0.1669405847787857,\n",
       "  -0.04637010395526886,\n",
       "  -0.39001956582069397,\n",
       "  0.6669414043426514,\n",
       "  0.7004964351654053,\n",
       "  0.1998409926891327,\n",
       "  0.5631886124610901,\n",
       "  -0.13620173931121826,\n",
       "  0.14272961020469666,\n",
       "  0.08659176528453827,\n",
       "  0.7045535445213318,\n",
       "  -0.24780936539173126,\n",
       "  0.2548193633556366,\n",
       "  -0.17255185544490814,\n",
       "  -0.5450559258460999,\n",
       "  -0.22377406060695648,\n",
       "  0.6177277565002441,\n",
       "  0.3301754593849182,\n",
       "  -0.011795378290116787,\n",
       "  0.5153635144233704,\n",
       "  0.09256120026111603,\n",
       "  0.13281682133674622,\n",
       "  0.08251546323299408,\n",
       "  -0.5345514416694641,\n",
       "  0.08208012580871582,\n",
       "  -0.7729136347770691,\n",
       "  0.5059892535209656,\n",
       "  -0.6291453242301941,\n",
       "  -0.22133147716522217,\n",
       "  -0.22990940511226654,\n",
       "  0.1857132762670517,\n",
       "  0.051375750452280045,\n",
       "  0.1383553147315979,\n",
       "  1.0276861190795898,\n",
       "  -1.0133967399597168,\n",
       "  -0.07638897001743317,\n",
       "  -0.7624238133430481,\n",
       "  0.7227465510368347,\n",
       "  0.07228013128042221,\n",
       "  0.0005892014596611261,\n",
       "  0.4116687774658203,\n",
       "  0.5377890467643738,\n",
       "  0.4709469974040985,\n",
       "  -0.48302212357521057,\n",
       "  0.24764074385166168,\n",
       "  -0.18578967452049255,\n",
       "  0.12200506776571274,\n",
       "  1.0278947353363037,\n",
       "  0.3711254596710205,\n",
       "  -0.43178093433380127,\n",
       "  0.24586603045463562,\n",
       "  0.9432300925254822,\n",
       "  0.35715043544769287,\n",
       "  0.2990468442440033,\n",
       "  0.780769944190979,\n",
       "  -0.18663427233695984,\n",
       "  -1.3949546813964844,\n",
       "  -0.4063863754272461,\n",
       "  0.1378602683544159,\n",
       "  -0.7533087730407715,\n",
       "  0.09222718328237534,\n",
       "  -0.6208351254463196,\n",
       "  -0.3689382076263428,\n",
       "  -0.9358952045440674,\n",
       "  0.5921306610107422,\n",
       "  1.2906925678253174,\n",
       "  0.11655134707689285,\n",
       "  -0.7200593948364258,\n",
       "  -0.9030694365501404,\n",
       "  1.3785649538040161,\n",
       "  0.835281491279602,\n",
       "  -0.42623457312583923,\n",
       "  0.3423868417739868,\n",
       "  -0.4075051248073578,\n",
       "  0.30602091550827026,\n",
       "  -0.9026070237159729,\n",
       "  -0.26162809133529663,\n",
       "  1.5208693742752075,\n",
       "  0.5875042676925659,\n",
       "  0.2319537103176117,\n",
       "  0.7804784178733826,\n",
       "  0.022468630224466324,\n",
       "  -0.7196251749992371,\n",
       "  0.48828810453414917,\n",
       "  -0.5515792965888977,\n",
       "  0.6819950342178345,\n",
       "  0.18047812581062317,\n",
       "  0.6883574724197388,\n",
       "  0.28145790100097656,\n",
       "  1.330668568611145,\n",
       "  -0.8435501456260681,\n",
       "  -0.3710922598838806,\n",
       "  0.38269686698913574,\n",
       "  0.36387011408805847,\n",
       "  0.8561081886291504,\n",
       "  0.3541882634162903,\n",
       "  -0.4079808294773102,\n",
       "  -0.9007359743118286,\n",
       "  0.3240962028503418,\n",
       "  -0.8273479342460632,\n",
       "  1.2449755668640137,\n",
       "  -0.5644068717956543,\n",
       "  -0.07906273007392883,\n",
       "  -1.3011873960494995,\n",
       "  0.22178222239017487,\n",
       "  -3.1989874839782715,\n",
       "  0.8131459951400757,\n",
       "  -0.8315516710281372,\n",
       "  0.7598476409912109,\n",
       "  -0.9019002914428711,\n",
       "  0.08003837615251541,\n",
       "  -0.6253294348716736,\n",
       "  0.611099898815155,\n",
       "  0.2585217356681824,\n",
       "  -0.6374205946922302,\n",
       "  0.6663632988929749,\n",
       "  -0.9191279411315918,\n",
       "  0.697249174118042,\n",
       "  -0.8130282759666443,\n",
       "  -0.036017462611198425,\n",
       "  -0.7561700344085693,\n",
       "  -0.18651798367500305,\n",
       "  -0.568330705165863,\n",
       "  -0.19768866896629333,\n",
       "  -0.4804614186286926,\n",
       "  0.726030707359314,\n",
       "  0.6671953201293945,\n",
       "  0.47973620891571045,\n",
       "  0.3738539218902588,\n",
       "  -0.6821510791778564,\n",
       "  0.42562195658683777,\n",
       "  1.3580565452575684,\n",
       "  -0.9067630171775818,\n",
       "  0.6947579979896545,\n",
       "  0.33452939987182617,\n",
       "  -0.2242135852575302,\n",
       "  0.41885530948638916,\n",
       "  -0.5175719857215881,\n",
       "  -0.31509077548980713,\n",
       "  0.5365334749221802,\n",
       "  0.17477911710739136,\n",
       "  -0.29239600896835327,\n",
       "  0.3974874019622803,\n",
       "  0.9028829336166382,\n",
       "  0.925237774848938,\n",
       "  -0.4760664403438568,\n",
       "  -0.12197218090295792,\n",
       "  -0.9006994962692261,\n",
       "  0.9240732192993164,\n",
       "  0.48851704597473145,\n",
       "  -0.8139504194259644,\n",
       "  0.5899215340614319,\n",
       "  0.17770139873027802,\n",
       "  0.07380597293376923,\n",
       "  -1.0964223146438599,\n",
       "  0.1265106201171875,\n",
       "  -1.095478892326355,\n",
       "  1.0512893199920654,\n",
       "  -1.1500067710876465,\n",
       "  -0.8391679525375366,\n",
       "  -0.6781086921691895,\n",
       "  0.7019282579421997,\n",
       "  -1.033984899520874,\n",
       "  -1.1066594123840332,\n",
       "  0.5546533465385437,\n",
       "  -0.35644757747650146,\n",
       "  -0.06082593649625778,\n",
       "  -0.09994757175445557,\n",
       "  0.49127018451690674,\n",
       "  -0.8255859613418579,\n",
       "  0.3465464115142822,\n",
       "  0.2063627690076828,\n",
       "  -0.01830253005027771,\n",
       "  -0.6648573279380798,\n",
       "  -0.3937731385231018,\n",
       "  0.36690738797187805,\n",
       "  -0.049489475786685944,\n",
       "  -0.8189683556556702,\n",
       "  -0.4005163908004761,\n",
       "  0.6077674627304077,\n",
       "  -0.23573024570941925,\n",
       "  -0.5635234117507935,\n",
       "  -0.7609124183654785,\n",
       "  -0.3570265471935272,\n",
       "  0.9872812628746033,\n",
       "  0.2661047577857971,\n",
       "  0.3889189064502716,\n",
       "  -1.224096417427063,\n",
       "  0.508344829082489,\n",
       "  -0.47648918628692627,\n",
       "  0.01733018271625042,\n",
       "  0.21331176161766052,\n",
       "  -0.4999612271785736,\n",
       "  0.03384900465607643,\n",
       "  0.2674081027507782,\n",
       "  0.3677043914794922,\n",
       "  0.11981958150863647,\n",
       "  0.9384635090827942,\n",
       "  1.0063515901565552,\n",
       "  -0.5893813371658325,\n",
       "  -0.8446053862571716,\n",
       "  0.4487493932247162,\n",
       "  0.0030019355472177267,\n",
       "  -0.7549432516098022,\n",
       "  0.9910100102424622,\n",
       "  -1.3147523403167725,\n",
       "  -0.9004968404769897,\n",
       "  0.05008247494697571,\n",
       "  -0.799476683139801,\n",
       "  0.3151145875453949,\n",
       "  1.0911593437194824,\n",
       "  -0.8889608979225159,\n",
       "  0.04705650731921196,\n",
       "  0.16769401729106903,\n",
       "  -0.11203713715076447,\n",
       "  1.112263798713684,\n",
       "  -0.8036606311798096,\n",
       "  1.1743677854537964,\n",
       "  0.5743102431297302,\n",
       "  -0.060978688299655914,\n",
       "  -0.2373894453048706,\n",
       "  1.1053016185760498,\n",
       "  0.48069846630096436,\n",
       "  -0.45338335633277893,\n",
       "  -0.08186111599206924,\n",
       "  1.7956640720367432,\n",
       "  -0.11115193367004395,\n",
       "  0.3401692807674408,\n",
       "  -1.148877501487732,\n",
       "  0.36243367195129395,\n",
       "  -0.32871460914611816,\n",
       "  -0.706660807132721,\n",
       "  -0.24627655744552612,\n",
       "  -0.41987648606300354,\n",
       "  0.4069618284702301,\n",
       "  0.7556734681129456,\n",
       "  -0.04095989465713501,\n",
       "  1.6690711975097656,\n",
       "  0.6750460863113403,\n",
       "  -0.06305984407663345,\n",
       "  -0.7886049747467041,\n",
       "  0.4118591845035553,\n",
       "  0.9215267300605774,\n",
       "  0.7456295490264893,\n",
       "  -0.4261796474456787,\n",
       "  0.7653692364692688,\n",
       "  0.4366588592529297,\n",
       "  -0.8772632479667664,\n",
       "  -0.3367532789707184,\n",
       "  -0.7008823156356812,\n",
       "  1.066973090171814,\n",
       "  -0.1716364175081253,\n",
       "  0.25376537442207336,\n",
       "  1.5672324895858765,\n",
       "  -0.4511273205280304,\n",
       "  1.2011195421218872,\n",
       "  0.33118700981140137,\n",
       "  1.0058256387710571,\n",
       "  -0.5208330750465393,\n",
       "  -0.5141329169273376,\n",
       "  -1.3915563821792603,\n",
       "  -0.13594087958335876,\n",
       "  -0.2161797434091568,\n",
       "  -0.05450893193483353,\n",
       "  0.054625291377305984,\n",
       "  0.3315366804599762,\n",
       "  -0.05890658497810364,\n",
       "  0.45541077852249146,\n",
       "  -0.40342018008232117,\n",
       "  -0.29298219084739685,\n",
       "  -0.30050456523895264,\n",
       "  -1.108303189277649,\n",
       "  -0.6425211429595947,\n",
       "  1.0462971925735474,\n",
       "  -0.005612167529761791,\n",
       "  0.35378098487854004,\n",
       "  -0.8452750444412231,\n",
       "  0.039798565208911896,\n",
       "  -0.060647085309028625,\n",
       "  -1.0186856985092163,\n",
       "  0.5574763417243958,\n",
       "  -0.22902034223079681,\n",
       "  -1.017868161201477,\n",
       "  -0.503932535648346,\n",
       "  -0.19667036831378937,\n",
       "  -0.7414564490318298,\n",
       "  1.3453829288482666,\n",
       "  0.6461226344108582,\n",
       "  -0.17202702164649963,\n",
       "  -0.6603708863258362,\n",
       "  -0.800703763961792,\n",
       "  -0.39194777607917786,\n",
       "  0.5983331203460693,\n",
       "  -0.48559701442718506,\n",
       "  -0.43646588921546936,\n",
       "  -1.2910048961639404,\n",
       "  0.7646777629852295,\n",
       "  -0.30448275804519653,\n",
       "  -0.3392069935798645,\n",
       "  0.3097110092639923,\n",
       "  0.6113647818565369,\n",
       "  -0.5204833149909973,\n",
       "  -0.24569003283977509,\n",
       "  0.2910819947719574,\n",
       "  1.1874616146087646,\n",
       "  -0.6399099826812744,\n",
       "  1.122009515762329,\n",
       "  1.119223713874817,\n",
       "  1.0061142444610596,\n",
       "  0.3946511149406433,\n",
       "  -0.9341253042221069,\n",
       "  -0.3081476092338562,\n",
       "  -0.24826748669147491,\n",
       "  -0.06057647243142128,\n",
       "  -0.19173206388950348,\n",
       "  0.6587311029434204,\n",
       "  0.06243572011590004,\n",
       "  0.5296726822853088,\n",
       "  -0.5961606502532959,\n",
       "  0.22790871560573578,\n",
       "  -0.40444880723953247,\n",
       "  0.2981458008289337,\n",
       "  -0.4299063980579376,\n",
       "  -0.5822990536689758,\n",
       "  -0.30471503734588623,\n",
       "  -0.4451543390750885,\n",
       "  0.25257790088653564,\n",
       "  -0.8087599277496338,\n",
       "  -0.9448127150535583,\n",
       "  1.0295737981796265,\n",
       "  -0.4779801666736603,\n",
       "  0.49665260314941406,\n",
       "  -0.07973881810903549,\n",
       "  0.2864142954349518,\n",
       "  -0.10387922078371048,\n",
       "  -0.22094757854938507,\n",
       "  0.28527289628982544,\n",
       "  0.37103039026260376,\n",
       "  -0.49824029207229614,\n",
       "  -0.10642942786216736,\n",
       "  0.2698834538459778,\n",
       "  0.6545553207397461,\n",
       "  -0.9429289698600769,\n",
       "  0.10419563204050064,\n",
       "  0.48981863260269165,\n",
       "  0.05066295713186264,\n",
       "  -0.4893799126148224,\n",
       "  -0.23651926219463348,\n",
       "  0.6034565567970276,\n",
       "  1.0519216060638428,\n",
       "  0.08795783668756485,\n",
       "  -0.45375698804855347,\n",
       "  0.27399420738220215,\n",
       "  0.4721094071865082,\n",
       "  1.1278746128082275,\n",
       "  -0.7765169739723206,\n",
       "  0.08135460317134857,\n",
       "  0.2731243968009949,\n",
       "  -1.1855882406234741,\n",
       "  0.4175264239311218,\n",
       "  -0.847087025642395,\n",
       "  -0.452149897813797,\n",
       "  0.23712797462940216,\n",
       "  -0.3852161169052124,\n",
       "  0.6180095672607422,\n",
       "  -0.47879746556282043,\n",
       "  -0.4303987920284271,\n",
       "  -1.5300692319869995,\n",
       "  -1.0956335067749023,\n",
       "  -0.7124793529510498,\n",
       "  -0.7489128708839417,\n",
       "  0.2636483907699585,\n",
       "  1.1047656536102295,\n",
       "  -0.8158431053161621,\n",
       "  0.5909566879272461,\n",
       "  -0.6808719635009766,\n",
       "  -0.7228853106498718,\n",
       "  0.326365202665329,\n",
       "  0.267739474773407,\n",
       "  -0.6078634262084961,\n",
       "  -0.29532304406166077,\n",
       "  0.6661060452461243,\n",
       "  -0.28802770376205444,\n",
       "  -0.09973237663507462,\n",
       "  0.5658314228057861,\n",
       "  -0.3833862245082855,\n",
       "  0.29385823011398315,\n",
       "  -0.7688650488853455,\n",
       "  -0.048771463334560394,\n",
       "  0.6660775542259216,\n",
       "  -0.04757600277662277,\n",
       "  0.018811607733368874,\n",
       "  -0.860559344291687,\n",
       "  0.03725617751479149,\n",
       "  -0.2851080298423767,\n",
       "  -0.02251492813229561,\n",
       "  -0.03224415332078934,\n",
       "  -0.5348847508430481,\n",
       "  0.7483922243118286,\n",
       "  0.5590836405754089,\n",
       "  0.6005733013153076,\n",
       "  -0.057622045278549194,\n",
       "  -0.4090636074542999,\n",
       "  1.0065579414367676,\n",
       "  0.5477921962738037,\n",
       "  0.03890872001647949,\n",
       "  0.840997040271759,\n",
       "  -0.6961568593978882,\n",
       "  -0.847157895565033,\n",
       "  0.3312219977378845,\n",
       "  -0.9062102437019348,\n",
       "  0.25731274485588074,\n",
       "  0.8450514078140259,\n",
       "  -0.32232803106307983,\n",
       "  0.5948536396026611,\n",
       "  0.16025732457637787,\n",
       "  -1.4899132251739502,\n",
       "  0.2506224513053894,\n",
       "  0.49685755372047424,\n",
       "  -0.975021481513977,\n",
       "  -0.03245670348405838,\n",
       "  1.4975628852844238,\n",
       "  -0.039730481803417206,\n",
       "  -0.3045809268951416,\n",
       "  0.07514934986829758,\n",
       "  -0.9598419070243835,\n",
       "  0.00011042822734452784,\n",
       "  -0.6842315196990967,\n",
       "  0.776492178440094,\n",
       "  0.29123106598854065,\n",
       "  0.6803150773048401,\n",
       "  1.1447216272354126,\n",
       "  0.1327897012233734,\n",
       "  -0.3318437337875366,\n",
       "  0.11654876172542572,\n",
       "  -1.0104345083236694,\n",
       "  -1.4847956895828247,\n",
       "  -0.48607200384140015,\n",
       "  -1.3604042530059814,\n",
       "  0.028611933812499046,\n",
       "  0.5126003623008728,\n",
       "  -0.18004442751407623,\n",
       "  -0.46123453974723816,\n",
       "  0.2951836585998535,\n",
       "  0.17651930451393127,\n",
       "  0.11063437163829803,\n",
       "  1.4007108211517334,\n",
       "  -0.4533829689025879,\n",
       "  -0.58841872215271,\n",
       "  -1.5479929447174072,\n",
       "  -0.22657328844070435,\n",
       "  0.464139848947525,\n",
       "  0.93827223777771,\n",
       "  0.8069935441017151,\n",
       "  -0.5008352398872375,\n",
       "  0.4674433767795563,\n",
       "  0.010833545587956905,\n",
       "  0.25830700993537903,\n",
       "  0.5800597071647644,\n",
       "  -0.6765587329864502,\n",
       "  0.186518132686615,\n",
       "  -0.4424237310886383,\n",
       "  -1.140777587890625,\n",
       "  -0.17037616670131683,\n",
       "  -0.8414126038551331,\n",
       "  -0.9857707619667053,\n",
       "  -0.8511242866516113,\n",
       "  -0.0440661795437336,\n",
       "  -0.22224363684654236,\n",
       "  0.11353342235088348,\n",
       "  0.32657259702682495,\n",
       "  -1.0024205446243286,\n",
       "  0.8849754333496094,\n",
       "  -0.25512033700942993,\n",
       "  -1.0927479267120361,\n",
       "  -0.6347844004631042,\n",
       "  -0.4915158450603485,\n",
       "  0.9208207726478577,\n",
       "  -0.28841304779052734,\n",
       "  -0.293444961309433,\n",
       "  0.830220103263855,\n",
       "  -1.1352750062942505,\n",
       "  1.1176410913467407,\n",
       "  0.38030391931533813,\n",
       "  0.2565937042236328,\n",
       "  -0.8338273763656616,\n",
       "  0.2762157917022705,\n",
       "  0.44578495621681213,\n",
       "  0.5352004170417786,\n",
       "  -0.3688127100467682,\n",
       "  0.7100729942321777,\n",
       "  -0.5194775462150574,\n",
       "  1.502468466758728,\n",
       "  0.1445828527212143,\n",
       "  0.8102482557296753,\n",
       "  0.020967071875929832,\n",
       "  -0.44789639115333557,\n",
       "  0.906713604927063,\n",
       "  1.2033153772354126,\n",
       "  -1.1256616115570068,\n",
       "  -0.10577996075153351,\n",
       "  0.023087479174137115,\n",
       "  -0.41141483187675476,\n",
       "  0.26609620451927185,\n",
       "  0.5962762236595154,\n",
       "  0.0070093851536512375,\n",
       "  -0.766327977180481,\n",
       "  0.535698413848877,\n",
       "  -0.24594421684741974,\n",
       "  0.7532090544700623,\n",
       "  0.47961392998695374,\n",
       "  -0.03365631401538849,\n",
       "  0.3465096354484558,\n",
       "  -0.12940365076065063,\n",
       "  0.4036988317966461,\n",
       "  0.3780900537967682,\n",
       "  0.030493291094899178,\n",
       "  -0.7033408284187317,\n",
       "  0.6764556169509888,\n",
       "  -0.7541640996932983,\n",
       "  -0.1922055184841156,\n",
       "  0.5189377069473267,\n",
       "  1.5090878009796143,\n",
       "  -0.6341007947921753,\n",
       "  -4.206207275390625,\n",
       "  0.1471232920885086,\n",
       "  1.084617257118225,\n",
       "  -0.9298677444458008,\n",
       "  -0.2921348810195923,\n",
       "  -0.7631025910377502,\n",
       "  0.8107345700263977,\n",
       "  0.2673141658306122,\n",
       "  -0.5396984815597534,\n",
       "  0.19836726784706116,\n",
       "  -0.025270557031035423,\n",
       "  -0.6607959866523743,\n",
       "  0.0331387035548687,\n",
       "  0.27335110306739807,\n",
       "  -0.02455495297908783,\n",
       "  1.3285329341888428,\n",
       "  0.5160353183746338,\n",
       "  -1.658387541770935,\n",
       "  -0.48925474286079407,\n",
       "  0.8160191774368286,\n",
       "  0.5390037298202515,\n",
       "  -0.03490043431520462,\n",
       "  0.019761059433221817,\n",
       "  0.9335135221481323,\n",
       "  0.8735260963439941,\n",
       "  0.30049628019332886,\n",
       "  0.5431286692619324,\n",
       "  -0.24904392659664154,\n",
       "  -0.4820902943611145,\n",
       "  -0.5207327008247375,\n",
       "  -0.19272345304489136,\n",
       "  0.7190473675727844,\n",
       "  -1.2802691459655762,\n",
       "  -1.3840610980987549,\n",
       "  0.1425422877073288,\n",
       "  -1.7949583530426025,\n",
       "  1.0226160287857056,\n",
       "  0.46199843287467957,\n",
       "  0.5109989643096924,\n",
       "  -1.1970781087875366,\n",
       "  0.5063442587852478,\n",
       "  -0.6173166632652283,\n",
       "  0.7410649061203003,\n",
       "  0.6084242463111877,\n",
       "  0.22465723752975464,\n",
       "  -1.099271535873413,\n",
       "  0.5735957622528076,\n",
       "  0.6015742421150208,\n",
       "  1.0619351863861084,\n",
       "  -0.21048790216445923,\n",
       "  2.5063045024871826,\n",
       "  -0.0327216312289238,\n",
       "  -0.4036559760570526,\n",
       "  -1.6254897117614746,\n",
       "  -0.7536231279373169,\n",
       "  -0.7524072527885437,\n",
       "  0.13521529734134674,\n",
       "  0.11251548677682877,\n",
       "  0.4516110420227051,\n",
       "  -0.3912004828453064,\n",
       "  -0.18070095777511597,\n",
       "  0.356610506772995,\n",
       "  -0.016657987609505653,\n",
       "  0.4101269245147705,\n",
       "  -0.1529715657234192,\n",
       "  0.47054094076156616,\n",
       "  0.16491055488586426,\n",
       "  -0.5584322810173035,\n",
       "  -0.5236709117889404,\n",
       "  0.21881933510303497,\n",
       "  0.14342544972896576,\n",
       "  0.08607537299394608,\n",
       "  -0.07084770500659943,\n",
       "  0.5988802909851074,\n",
       "  0.7267254590988159,\n",
       "  -1.5361487865447998,\n",
       "  -1.1691174507141113,\n",
       "  -0.06591876596212387,\n",
       "  -0.37168368697166443,\n",
       "  -0.047336261719465256,\n",
       "  -0.28544050455093384,\n",
       "  0.13055431842803955,\n",
       "  0.42925459146499634,\n",
       "  0.7274695634841919,\n",
       "  0.24553069472312927,\n",
       "  -0.10443482547998428,\n",
       "  -0.709028422832489,\n",
       "  0.5286082029342651,\n",
       "  0.06043221428990364,\n",
       "  -0.0614020861685276,\n",
       "  -1.0370728969573975,\n",
       "  -0.7794317603111267,\n",
       "  -0.5214768648147583,\n",
       "  0.8917846083641052,\n",
       "  0.16838254034519196,\n",
       "  0.718243420124054,\n",
       "  0.40740853548049927,\n",
       "  -0.09655328840017319,\n",
       "  -0.3683165907859802,\n",
       "  -0.11676755547523499,\n",
       "  -0.7263312935829163,\n",
       "  0.9523398876190186,\n",
       "  -0.7891892790794373,\n",
       "  0.7917033433914185,\n",
       "  0.654273271560669,\n",
       "  0.34835150837898254,\n",
       "  0.9762778878211975,\n",
       "  -0.17065905034542084,\n",
       "  -0.47686728835105896,\n",
       "  0.36163797974586487,\n",
       "  1.0596551895141602,\n",
       "  0.5349482893943787,\n",
       "  0.23500695824623108,\n",
       "  1.3491419553756714,\n",
       "  -0.5618994235992432,\n",
       "  -0.06115293130278587,\n",
       "  -0.060993507504463196,\n",
       "  -0.6251287460327148,\n",
       "  0.7091960310935974,\n",
       "  0.49566397070884705,\n",
       "  -1.320820689201355,\n",
       "  -0.04286355525255203,\n",
       "  -0.783510684967041,\n",
       "  -1.1011102199554443,\n",
       "  0.6581992506980896,\n",
       "  0.9823074340820312,\n",
       "  0.20970910787582397,\n",
       "  -1.409715175628662,\n",
       "  0.3681687116622925,\n",
       "  -0.8480188250541687,\n",
       "  -0.04133940115571022,\n",
       "  -0.42910030484199524,\n",
       "  -0.4234941005706787,\n",
       "  0.2269577980041504,\n",
       "  0.34006786346435547,\n",
       "  -0.7444579005241394,\n",
       "  -0.05434863641858101,\n",
       "  -0.21233706176280975,\n",
       "  0.5915663242340088,\n",
       "  0.3679901361465454,\n",
       "  -0.5355650186538696,\n",
       "  0.9854304790496826,\n",
       "  1.804577112197876,\n",
       "  -0.10726025700569153,\n",
       "  1.1824092864990234,\n",
       "  -0.030294092372059822,\n",
       "  -0.6270188093185425,\n",
       "  -1.2314587831497192,\n",
       "  -0.6726229786872864,\n",
       "  0.20784935355186462,\n",
       "  -0.1928214430809021,\n",
       "  -0.5211597681045532,\n",
       "  -0.05746195837855339,\n",
       "  -0.7753714919090271,\n",
       "  -0.037038035690784454,\n",
       "  -0.6837877035140991,\n",
       "  -0.06164861097931862,\n",
       "  0.47251635789871216,\n",
       "  -1.3901668787002563,\n",
       "  1.5836042165756226,\n",
       "  0.9214999079704285,\n",
       "  0.07511985301971436,\n",
       "  0.9496416449546814,\n",
       "  -0.18083463609218597,\n",
       "  -0.30747494101524353,\n",
       "  0.4717440605163574,\n",
       "  -0.6511670351028442,\n",
       "  -0.740210771560669,\n",
       "  0.15569886565208435,\n",
       "  2.11200213432312,\n",
       "  0.1459711343050003,\n",
       "  -0.6751086115837097,\n",
       "  1.005374550819397,\n",
       "  1.0111781358718872,\n",
       "  -0.09058384597301483,\n",
       "  -1.773445963859558,\n",
       "  -0.5711519718170166,\n",
       "  -0.16742391884326935,\n",
       "  0.04398275539278984,\n",
       "  0.335426926612854,\n",
       "  0.09509056806564331,\n",
       "  0.2505696713924408,\n",
       "  -0.733492910861969,\n",
       "  0.06241590529680252,\n",
       "  1.0783966779708862,\n",
       "  -0.02303161658346653,\n",
       "  0.12814973294734955,\n",
       "  0.022468941286206245,\n",
       "  0.24493993818759918,\n",
       "  -0.3287544250488281,\n",
       "  -0.3273882269859314,\n",
       "  0.2718452215194702,\n",
       "  -0.5129795670509338,\n",
       "  -0.185267835855484,\n",
       "  -0.09101680666208267,\n",
       "  0.7299382090568542,\n",
       "  -0.48371848464012146,\n",
       "  0.2779104709625244,\n",
       "  -0.8000940680503845,\n",
       "  0.5884727835655212,\n",
       "  -0.2120285928249359,\n",
       "  0.06700733304023743,\n",
       "  0.4305567741394043,\n",
       "  -0.7808477878570557,\n",
       "  -0.6691640019416809,\n",
       "  -1.3909753561019897,\n",
       "  -0.12213436514139175,\n",
       "  0.28680935502052307,\n",
       "  -0.7587831020355225,\n",
       "  -0.02878742851316929,\n",
       "  -0.17871038615703583]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tx2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c8ebf232dea6577f6ceae92998c8d758983422845017e0e774d4bf7f66f7ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

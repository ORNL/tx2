{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - Default Approach\n",
    "\n",
    "This notebook demonstrates how to use the TX2 dashboard with a sequence classification transformer using the default approach as described in the \"Basic Usage\" docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd -q ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LRU_CACHE_CAPACITY'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable logging to view the output from `wrapper.prepare()` further down in the notebook. (It's a long running function, and logs which step it's on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example notebook, we use the 20 newsgroups dataset, which can be downloaded through huggingfacevia below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
      "Using custom data configuration SetFit--20_newsgroups-f9362e018b6adf67\n",
      "Reusing dataset json (/home/s6t/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# getting newsgroups data from huggingface\n",
    "train_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"train\"))\n",
    "test_data = pd.DataFrame(data=load_dataset(\"SetFit/20_newsgroups\", split=\"test\"))\n",
    "\n",
    "# setting up pytorch device\n",
    "if cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Defined below is a simple sequence classification model with a variable for the language model itself `l1`. Since it is a BERT model, we take the sequence embedding from the `[CLS]` token (via `output_1[0][:, 0, :])`) and pipe that into the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.l2 = torch.nn.Linear(768, 20)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        output_1 = self.l1(ids, mask)\n",
    "        output = self.l2(output_1[0][:, 0, :])  # use just the [CLS] output embedding\n",
    "        return output\n",
    "\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simplistic data cleaning, and putting all data into dataframes for the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # text = text[text.index(\"\\n\\n\") + 2 :]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    text = text.replace(\"   \", \" \")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you have Weitek's address/phone number? I'd...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye) DN...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>I have a (very old) Mac 512k and a Mac Plus, b...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>Wouldn't this require a hyper-sphere. In 3-spa...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>Stolen from Pasadena between 4:30 and 6:30 pm ...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      I was wondering if anyone out there could enli...      7   \n",
       "1      A fair number of brave souls who upgraded thei...      4   \n",
       "2      well folks, my mac plus finally gave up the gh...      4   \n",
       "3      Do you have Weitek's address/phone number? I'd...      1   \n",
       "4      From article <C5owCB.n3p@world.std.com>, by to...     14   \n",
       "...                                                  ...    ...   \n",
       "11309  DN> From: nyeda@cnsvax.uwec.edu (David Nye) DN...     13   \n",
       "11310  I have a (very old) Mac 512k and a Mac Plus, b...      4   \n",
       "11311  I just installed a DX2-66 CPU in a clone mothe...      3   \n",
       "11312  Wouldn't this require a hyper-sphere. In 3-spa...      1   \n",
       "11313  Stolen from Pasadena between 4:30 and 6:30 pm ...      8   \n",
       "\n",
       "                     label_text  \n",
       "0                     rec.autos  \n",
       "1         comp.sys.mac.hardware  \n",
       "2         comp.sys.mac.hardware  \n",
       "3                 comp.graphics  \n",
       "4                     sci.space  \n",
       "...                         ...  \n",
       "11309                   sci.med  \n",
       "11310     comp.sys.mac.hardware  \n",
       "11311  comp.sys.ibm.pc.hardware  \n",
       "11312             comp.graphics  \n",
       "11313           rec.motorcycles  \n",
       "\n",
       "[11014 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean long white space or extensive character returns\n",
    "train_data.text = train_data.text.apply(lambda x: clean_text(x))\n",
    "test_data.text = test_data.text.apply(lambda x: clean_text(x))\n",
    "\n",
    "# remove empty entries or trivially short ones\n",
    "train_cleaned = train_data[train_data[\"text\"].str.len() > 1]\n",
    "test_cleaned = test_data[test_data[\"text\"].str.len() > 1]\n",
    "train_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This section minimally trains the classification and language model - nothing fancy here, just to give the dashboard demo something to work with. Most of this is similar to the huggingface tutorial notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11014\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "class EncodedSet(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.data.label[index], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "train_cleaned.reset_index(drop=True, inplace=True)\n",
    "test_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_set = EncodedSet(train_cleaned, tokenizer, 256)\n",
    "test_set = EncodedSet(test_cleaned[:1000], tokenizer, 256)\n",
    "\n",
    "train_params = {\"batch_size\": 16, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "test_params = {\"batch_size\": 2, \"shuffle\": True, \"num_workers\": 0}\n",
    "\n",
    "# put everything into data loaders\n",
    "train_loader = DataLoader(train_set, **train_params)\n",
    "test_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    for _, data in tqdm(\n",
    "        enumerate(train_loader, start=0), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "    ):\n",
    "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        if _ % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss:  {loss.item()}\")\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         torch.cuda.empty_cache()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5743c205a23045e4b83b7ff67fe07eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  3.277553081512451\n",
      "Epoch: 0, Loss:  2.2027804851531982\n",
      "Epoch: 0, Loss:  1.2036110162734985\n",
      "Epoch: 0, Loss:  0.8762399554252625\n",
      "Epoch: 0, Loss:  1.244920253753662\n",
      "Epoch: 0, Loss:  1.211954116821289\n",
      "Epoch: 0, Loss:  0.7518831491470337\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):\n",
    "    losses.extend(train(epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper uses an `encodings` dictionary for various labels/visualizations, and can be set up with something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt.atheism': 0,\n",
       " 'comp.graphics': 1,\n",
       " 'comp.os.ms-windows.misc': 2,\n",
       " 'comp.sys.ibm.pc.hardware': 3,\n",
       " 'comp.sys.mac.hardware': 4,\n",
       " 'comp.windows.x': 5,\n",
       " 'misc.forsale': 6,\n",
       " 'rec.autos': 7,\n",
       " 'rec.motorcycles': 8,\n",
       " 'rec.sport.baseball': 9,\n",
       " 'rec.sport.hockey': 10,\n",
       " 'sci.crypt': 11,\n",
       " 'sci.electronics': 12,\n",
       " 'sci.med': 13,\n",
       " 'sci.space': 14,\n",
       " 'soc.religion.christian': 15,\n",
       " 'talk.politics.guns': 16,\n",
       " 'talk.politics.mideast': 17,\n",
       " 'talk.politics.misc': 18,\n",
       " 'talk.religion.misc': 19}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = (\n",
    "    train_cleaned[[\"label\", \"label_text\"]]\n",
    "    .groupby([\"label_text\"])\n",
    "    .apply(lambda x: x[\"label\"].tolist()[0])\n",
    "    .to_dict()\n",
    ")\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TX2\n",
    "\n",
    "This section shows how to put everything into the TX2 wrapper to get the dashboard widget displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tx2.dashboard import Dashboard\n",
    "from tx2.wrapper import Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # shouldn't be necessary since done in wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cache path found\n",
      "INFO:root:Checking for cached predictions...\n",
      "INFO:root:Running classifier...\n",
      "INFO:root:Saving predictions...\n",
      "INFO:root:Writing to data/predictions.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached embeddings...\n",
      "INFO:root:Embedding training and testing datasets\n",
      "INFO:root:Saving embeddings...\n",
      "INFO:root:Writing to data/embedding_training.json\n",
      "INFO:root:Writing to data/embedding_testing.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached projections...\n",
      "INFO:root:Training projector...\n",
      "INFO:root:Applying projector to test dataset...\n",
      "INFO:root:Saving projections...\n",
      "INFO:root:Writing to data/projections_training.json\n",
      "INFO:root:Writing to data/projections_testing.json\n",
      "INFO:root:Writing to data/projector.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached salience maps...\n",
      "INFO:root:Computing salience maps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd488eac668461495237f62bfa93bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving salience maps...\n",
      "INFO:root:Writing to data/salience.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Checking for cached cluster profiles...\n",
      "INFO:root:Clustering projections...\n",
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "wrapper = Wrapper(\n",
    "    train_texts=train_cleaned.text,\n",
    "    train_labels=train_cleaned.label,\n",
    "    test_texts=test_cleaned.text[:2000],\n",
    "    test_labels=test_cleaned.label[:2000],\n",
    "    encodings=encodings,\n",
    "    classifier=model,\n",
    "    language_model=model.l1,\n",
    "    tokenizer=tokenizer,\n",
    "    overwrite=True,\n",
    ")\n",
    "wrapper.batch_size = 16\n",
    "wrapper.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper._compute_all_salience_maps_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9b21a9c2c045eb90dd0b74c2118de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>UMAP Embedding Graph</h3>'), HBox(children=(Output(), VBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dash = Dashboard(wrapper)\n",
    "dash.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with different UMAP and DBSCAN arguments without having to recompute the entire `prepare()` function, you can use `recompute_projections` (which will recompute both the projections and visual clusterings) or `recompute_visual_clusterings` (which will only redo the clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Clustering projections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving cluster profiles...\n",
      "INFO:root:Writing to data/clusters.json\n",
      "INFO:root:Writing to data/cluster_profiles.pkl.gz\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster labels...\n",
      "INFO:root:Writing to data/cluster_labels.json\n",
      "INFO:root:Done!\n",
      "INFO:root:Saving cluster word counts...\n",
      "INFO:root:Writing to data/cluster_words.json\n",
      "INFO:root:Writing to data/cluster_class_words.json\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "wrapper.recompute_visual_clusterings(\"KMeans\", clustering_args=dict(n_clusters=20))\n",
    "# wrapper.recompute_visual_clusterings(\"OPTICS\", clustering_args=dict())\n",
    "# wrapper.recompute_projections(umap_args=dict(min_dist=.2), dbscan_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test or debug the classification model/see what raw outputs the viusualizations are getting, or create your own visualization tools, you can manually call the `classify()`, `soft_classify()`, `embed()` functions, or get access to any of the cached data as seen in the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.28060370683670044,\n",
       "  0.5031059980392456,\n",
       "  0.10724742710590363,\n",
       "  -0.5331698656082153,\n",
       "  -0.005571577697992325,\n",
       "  -0.4515875577926636,\n",
       "  0.08022506535053253,\n",
       "  1.9202200174331665,\n",
       "  1.8315465450286865,\n",
       "  0.6037108898162842,\n",
       "  0.5645972490310669,\n",
       "  -0.8079800009727478,\n",
       "  0.9349990487098694,\n",
       "  -0.9529551863670349,\n",
       "  0.289066344499588,\n",
       "  -0.8200767636299133,\n",
       "  -0.11846178025007248,\n",
       "  -0.9223289489746094,\n",
       "  -0.37361016869544983,\n",
       "  -0.6064767241477966]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.soft_classify([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0026363704819232225,\n",
       "  0.7441421747207642,\n",
       "  0.5824121236801147,\n",
       "  -0.4399600923061371,\n",
       "  -1.1622353792190552,\n",
       "  0.5489736795425415,\n",
       "  0.39393553137779236,\n",
       "  0.12440522015094757,\n",
       "  0.36614254117012024,\n",
       "  -2.0240416526794434,\n",
       "  1.0050026178359985,\n",
       "  -0.5519238114356995,\n",
       "  0.7607991695404053,\n",
       "  0.24020686745643616,\n",
       "  -1.0344598293304443,\n",
       "  0.5434224009513855,\n",
       "  0.3787517547607422,\n",
       "  0.328351229429245,\n",
       "  0.23373237252235413,\n",
       "  -0.054982177913188934,\n",
       "  -0.010498465970158577,\n",
       "  0.24423882365226746,\n",
       "  0.5864130854606628,\n",
       "  -0.8905940651893616,\n",
       "  -0.07740083336830139,\n",
       "  -0.19848847389221191,\n",
       "  0.4967444837093353,\n",
       "  1.0696762800216675,\n",
       "  -0.7025133967399597,\n",
       "  -1.1233991384506226,\n",
       "  -0.6221385598182678,\n",
       "  0.10273131728172302,\n",
       "  -0.1847362518310547,\n",
       "  0.2831178605556488,\n",
       "  0.7091114521026611,\n",
       "  -1.0009865760803223,\n",
       "  0.7327744960784912,\n",
       "  0.48003923892974854,\n",
       "  -0.937484860420227,\n",
       "  0.5187717080116272,\n",
       "  -0.7470951676368713,\n",
       "  -0.015629947185516357,\n",
       "  0.2555854320526123,\n",
       "  -0.25177907943725586,\n",
       "  -0.21360531449317932,\n",
       "  0.18297071754932404,\n",
       "  0.2208726853132248,\n",
       "  -0.15477436780929565,\n",
       "  -0.27074694633483887,\n",
       "  0.9076789617538452,\n",
       "  -0.4654757082462311,\n",
       "  0.7910577058792114,\n",
       "  0.11995242536067963,\n",
       "  0.7393882870674133,\n",
       "  -0.5720201134681702,\n",
       "  0.06833329796791077,\n",
       "  0.15713126957416534,\n",
       "  -0.14461801946163177,\n",
       "  -0.6489165425300598,\n",
       "  -0.2518835961818695,\n",
       "  -0.1695500761270523,\n",
       "  -0.9711695313453674,\n",
       "  -0.0010717927943915129,\n",
       "  0.18900196254253387,\n",
       "  -0.07129485160112381,\n",
       "  -0.15073148906230927,\n",
       "  0.37877586483955383,\n",
       "  -1.3142281770706177,\n",
       "  0.16554518043994904,\n",
       "  -0.015081673860549927,\n",
       "  1.0093994140625,\n",
       "  0.3654014766216278,\n",
       "  1.1801761388778687,\n",
       "  0.3850822150707245,\n",
       "  -0.043918248265981674,\n",
       "  -1.0087069272994995,\n",
       "  -0.43304383754730225,\n",
       "  -0.360103964805603,\n",
       "  -0.419109046459198,\n",
       "  0.9272658824920654,\n",
       "  0.4483959376811981,\n",
       "  0.32333076000213623,\n",
       "  -1.0316020250320435,\n",
       "  -0.27602165937423706,\n",
       "  0.3798182010650635,\n",
       "  -0.255353182554245,\n",
       "  0.594174861907959,\n",
       "  0.5866139531135559,\n",
       "  -0.630872905254364,\n",
       "  -0.45201486349105835,\n",
       "  0.3224993944168091,\n",
       "  0.48508962988853455,\n",
       "  -0.5444204807281494,\n",
       "  0.39302724599838257,\n",
       "  0.470566987991333,\n",
       "  0.5232910513877869,\n",
       "  -0.5155043005943298,\n",
       "  0.5342039465904236,\n",
       "  5.105998992919922,\n",
       "  0.43527013063430786,\n",
       "  0.05401286482810974,\n",
       "  -0.5655466914176941,\n",
       "  0.7946337461471558,\n",
       "  0.26992785930633545,\n",
       "  0.16369841992855072,\n",
       "  -0.5755679607391357,\n",
       "  -0.187538281083107,\n",
       "  -1.0488193035125732,\n",
       "  0.41212254762649536,\n",
       "  -0.35803502798080444,\n",
       "  0.7023214101791382,\n",
       "  0.30516982078552246,\n",
       "  0.8116440176963806,\n",
       "  -0.2482663244009018,\n",
       "  0.08776246011257172,\n",
       "  -0.3637399673461914,\n",
       "  0.19606243073940277,\n",
       "  0.1426113396883011,\n",
       "  0.14704352617263794,\n",
       "  0.21949619054794312,\n",
       "  -0.09537477046251297,\n",
       "  0.6317766308784485,\n",
       "  1.4630814790725708,\n",
       "  0.36908870935440063,\n",
       "  -1.0481479167938232,\n",
       "  -0.18829070031642914,\n",
       "  0.9332637786865234,\n",
       "  0.3471837341785431,\n",
       "  0.7574231028556824,\n",
       "  0.45426368713378906,\n",
       "  -0.7638721466064453,\n",
       "  -0.23641839623451233,\n",
       "  -0.13334648311138153,\n",
       "  0.16172674298286438,\n",
       "  -0.36781182885169983,\n",
       "  0.3818832039833069,\n",
       "  -1.8902809619903564,\n",
       "  0.858162522315979,\n",
       "  -2.306562900543213,\n",
       "  -0.21031710505485535,\n",
       "  -0.4469999074935913,\n",
       "  0.14298681914806366,\n",
       "  -0.30655524134635925,\n",
       "  -0.5480979681015015,\n",
       "  -0.6535319685935974,\n",
       "  2.0644636154174805,\n",
       "  0.5716520547866821,\n",
       "  5.3122264944249764e-05,\n",
       "  0.19574828445911407,\n",
       "  -1.3082150220870972,\n",
       "  0.42954427003860474,\n",
       "  -0.010785260237753391,\n",
       "  0.3379467725753784,\n",
       "  -0.32120707631111145,\n",
       "  0.35457950830459595,\n",
       "  -0.49443745613098145,\n",
       "  0.15120258927345276,\n",
       "  -1.11650812625885,\n",
       "  0.02390911616384983,\n",
       "  0.7611486911773682,\n",
       "  0.25612038373947144,\n",
       "  0.41225147247314453,\n",
       "  0.27764129638671875,\n",
       "  0.1414860039949417,\n",
       "  0.6676384806632996,\n",
       "  -0.32271239161491394,\n",
       "  0.7338669896125793,\n",
       "  -0.1315123587846756,\n",
       "  -0.10929962992668152,\n",
       "  0.43510451912879944,\n",
       "  -0.47271493077278137,\n",
       "  0.09310809522867203,\n",
       "  -0.903874397277832,\n",
       "  0.5262529850006104,\n",
       "  0.39922380447387695,\n",
       "  0.5257302522659302,\n",
       "  0.35527920722961426,\n",
       "  1.275718092918396,\n",
       "  -0.18680982291698456,\n",
       "  -0.5722688436508179,\n",
       "  -1.0869059562683105,\n",
       "  1.144865870475769,\n",
       "  0.41612571477890015,\n",
       "  -0.3592813313007355,\n",
       "  0.217953622341156,\n",
       "  -0.11341879516839981,\n",
       "  -0.1558084487915039,\n",
       "  -0.8283035159111023,\n",
       "  -0.16689056158065796,\n",
       "  -0.4918852746486664,\n",
       "  -0.37201932072639465,\n",
       "  -1.0765910148620605,\n",
       "  -0.2619776725769043,\n",
       "  -0.34849950671195984,\n",
       "  1.1642508506774902,\n",
       "  -0.684528648853302,\n",
       "  -0.9915048480033875,\n",
       "  0.004586750641465187,\n",
       "  -0.9145968556404114,\n",
       "  0.05635766312479973,\n",
       "  0.4298854172229767,\n",
       "  -0.37593361735343933,\n",
       "  -0.4866985082626343,\n",
       "  0.13279981911182404,\n",
       "  1.3322428464889526,\n",
       "  -0.15317881107330322,\n",
       "  0.39811280369758606,\n",
       "  -0.3687675893306732,\n",
       "  0.8926600813865662,\n",
       "  -0.43330326676368713,\n",
       "  -0.6576056480407715,\n",
       "  -0.7625918388366699,\n",
       "  0.16211934387683868,\n",
       "  -0.7577545642852783,\n",
       "  -0.5713220238685608,\n",
       "  -1.3521627187728882,\n",
       "  -0.4218146502971649,\n",
       "  0.7273794412612915,\n",
       "  -0.3676173686981201,\n",
       "  0.7581994533538818,\n",
       "  -1.2301478385925293,\n",
       "  -0.15222015976905823,\n",
       "  -0.7011511325836182,\n",
       "  -0.4188694953918457,\n",
       "  0.7727345824241638,\n",
       "  -0.004839577712118626,\n",
       "  0.6709232330322266,\n",
       "  0.23465123772621155,\n",
       "  0.41944757103919983,\n",
       "  -0.20417538285255432,\n",
       "  0.19393755495548248,\n",
       "  0.6796863079071045,\n",
       "  -1.2698167562484741,\n",
       "  -0.6499478220939636,\n",
       "  0.28258514404296875,\n",
       "  -0.30521202087402344,\n",
       "  -1.603227972984314,\n",
       "  0.8070324063301086,\n",
       "  -0.10703380405902863,\n",
       "  -0.4768510162830353,\n",
       "  -0.39913156628608704,\n",
       "  -0.7586442232131958,\n",
       "  0.8296442031860352,\n",
       "  0.22412936389446259,\n",
       "  0.2412339597940445,\n",
       "  -0.37532979249954224,\n",
       "  0.2122182548046112,\n",
       "  0.05031575262546539,\n",
       "  0.4893164336681366,\n",
       "  -0.12931060791015625,\n",
       "  0.9987657070159912,\n",
       "  0.10045988857746124,\n",
       "  -0.4505711495876312,\n",
       "  -0.8631659746170044,\n",
       "  0.7771056294441223,\n",
       "  0.6810179948806763,\n",
       "  0.7684137225151062,\n",
       "  -1.2365891933441162,\n",
       "  0.6155276298522949,\n",
       "  -1.0052796602249146,\n",
       "  0.4673391580581665,\n",
       "  -3.0283994674682617,\n",
       "  0.059552524238824844,\n",
       "  0.329312264919281,\n",
       "  -0.3711716830730438,\n",
       "  0.3646317422389984,\n",
       "  0.21647383272647858,\n",
       "  0.739954948425293,\n",
       "  -0.24970874190330505,\n",
       "  -0.49245795607566833,\n",
       "  0.818917453289032,\n",
       "  -0.25509634613990784,\n",
       "  -0.605699360370636,\n",
       "  -0.6056990027427673,\n",
       "  0.13547050952911377,\n",
       "  0.5234695672988892,\n",
       "  0.5130316615104675,\n",
       "  -0.5461247563362122,\n",
       "  -0.3444458842277527,\n",
       "  -0.6515705585479736,\n",
       "  0.47998884320259094,\n",
       "  0.4497968256473541,\n",
       "  -0.7164435982704163,\n",
       "  0.11222344636917114,\n",
       "  -0.5404496192932129,\n",
       "  0.5556848049163818,\n",
       "  0.3911333978176117,\n",
       "  -0.9197978973388672,\n",
       "  0.3768206834793091,\n",
       "  2.47454571723938,\n",
       "  1.942779302597046,\n",
       "  -0.9786689281463623,\n",
       "  0.2113630771636963,\n",
       "  -1.1599477529525757,\n",
       "  0.3900783658027649,\n",
       "  -0.011951869353652,\n",
       "  -0.33668002486228943,\n",
       "  -0.1332942545413971,\n",
       "  -0.7498466372489929,\n",
       "  -0.18843480944633484,\n",
       "  0.4373280704021454,\n",
       "  -0.018822021782398224,\n",
       "  -0.5657829642295837,\n",
       "  -0.1206299215555191,\n",
       "  -0.36050769686698914,\n",
       "  -1.3583906888961792,\n",
       "  -0.2176433950662613,\n",
       "  0.6740683913230896,\n",
       "  -0.8558049201965332,\n",
       "  -0.00010073481826111674,\n",
       "  -0.49528178572654724,\n",
       "  0.4320629835128784,\n",
       "  -0.43890902400016785,\n",
       "  0.8955596685409546,\n",
       "  -0.9491971731185913,\n",
       "  -1.3773372173309326,\n",
       "  -0.3535398840904236,\n",
       "  -0.17257387936115265,\n",
       "  -0.3235725164413452,\n",
       "  -0.07293959707021713,\n",
       "  0.27341577410697937,\n",
       "  -0.01203965861350298,\n",
       "  -0.09834364056587219,\n",
       "  -0.2876206338405609,\n",
       "  -0.751481831073761,\n",
       "  -0.08450642973184586,\n",
       "  -0.6641907095909119,\n",
       "  -0.17945683002471924,\n",
       "  -0.8655230402946472,\n",
       "  0.3813508450984955,\n",
       "  -0.7897543907165527,\n",
       "  0.2535836696624756,\n",
       "  -1.327684998512268,\n",
       "  0.8569644093513489,\n",
       "  -0.7162301540374756,\n",
       "  -0.8527883291244507,\n",
       "  0.2620305120944977,\n",
       "  0.07842139899730682,\n",
       "  -1.0891834497451782,\n",
       "  -0.5237951278686523,\n",
       "  0.2125031054019928,\n",
       "  0.49994757771492004,\n",
       "  -0.30627191066741943,\n",
       "  -0.7726003527641296,\n",
       "  -0.020041685551404953,\n",
       "  0.08928292989730835,\n",
       "  -0.007767749950289726,\n",
       "  -0.1412065476179123,\n",
       "  1.3790981769561768,\n",
       "  0.3041634261608124,\n",
       "  0.4601295590400696,\n",
       "  1.2915949821472168,\n",
       "  -0.3825753331184387,\n",
       "  0.19809360802173615,\n",
       "  -0.4022902548313141,\n",
       "  -0.6544130444526672,\n",
       "  -0.19569538533687592,\n",
       "  -0.042023856192827225,\n",
       "  -0.3707467019557953,\n",
       "  0.6080325245857239,\n",
       "  -0.274289608001709,\n",
       "  -1.731554388999939,\n",
       "  0.6715879440307617,\n",
       "  0.7152014970779419,\n",
       "  0.4350813329219818,\n",
       "  0.5049181580543518,\n",
       "  0.23156824707984924,\n",
       "  0.06494278460741043,\n",
       "  0.5049942135810852,\n",
       "  0.3493807315826416,\n",
       "  0.4754422903060913,\n",
       "  0.4506000876426697,\n",
       "  0.5417848229408264,\n",
       "  -0.6575701236724854,\n",
       "  0.802304744720459,\n",
       "  -0.7420283555984497,\n",
       "  -0.18928243219852448,\n",
       "  1.4182298183441162,\n",
       "  -0.4798329174518585,\n",
       "  0.14407823979854584,\n",
       "  -0.23597070574760437,\n",
       "  0.10013319551944733,\n",
       "  1.570353388786316,\n",
       "  -0.47119998931884766,\n",
       "  -0.223782017827034,\n",
       "  -0.19694358110427856,\n",
       "  0.8474540710449219,\n",
       "  -0.1373928040266037,\n",
       "  -0.6621061563491821,\n",
       "  -0.36571457982063293,\n",
       "  0.2516985833644867,\n",
       "  -0.9574050903320312,\n",
       "  0.513796865940094,\n",
       "  -0.5752779245376587,\n",
       "  0.10740581154823303,\n",
       "  0.6347612142562866,\n",
       "  0.360819011926651,\n",
       "  0.30956804752349854,\n",
       "  0.6274176836013794,\n",
       "  0.1977720558643341,\n",
       "  -0.69281405210495,\n",
       "  -0.41631877422332764,\n",
       "  -0.7089714407920837,\n",
       "  0.364623486995697,\n",
       "  0.14197459816932678,\n",
       "  1.5734243392944336,\n",
       "  -0.14734840393066406,\n",
       "  0.43877550959587097,\n",
       "  -2.21636700630188,\n",
       "  0.10653054714202881,\n",
       "  0.5791124105453491,\n",
       "  -0.1073884665966034,\n",
       "  0.14169751107692719,\n",
       "  -0.3685411214828491,\n",
       "  0.5606775283813477,\n",
       "  -0.49390748143196106,\n",
       "  0.1501549780368805,\n",
       "  0.1471923589706421,\n",
       "  -0.43338480591773987,\n",
       "  0.031143808737397194,\n",
       "  -0.029866743832826614,\n",
       "  0.1228753849864006,\n",
       "  0.6076945662498474,\n",
       "  -0.3158988356590271,\n",
       "  0.512089192867279,\n",
       "  -0.4655151963233948,\n",
       "  -0.11054672300815582,\n",
       "  -0.9273145794868469,\n",
       "  0.7938318252563477,\n",
       "  0.6137349009513855,\n",
       "  -1.4242427349090576,\n",
       "  0.4751015603542328,\n",
       "  0.22325271368026733,\n",
       "  0.5290440320968628,\n",
       "  -0.4696020483970642,\n",
       "  -0.2793191969394684,\n",
       "  -0.2077418565750122,\n",
       "  -1.04618501663208,\n",
       "  -0.09236541390419006,\n",
       "  4.16103458404541,\n",
       "  -0.6666349172592163,\n",
       "  0.06967957317829132,\n",
       "  -0.22076277434825897,\n",
       "  -0.04473461955785751,\n",
       "  -0.3891131281852722,\n",
       "  -0.1465357542037964,\n",
       "  -1.2207077741622925,\n",
       "  0.5054364204406738,\n",
       "  -0.28906214237213135,\n",
       "  -0.7149348855018616,\n",
       "  -0.04000486060976982,\n",
       "  0.45244714617729187,\n",
       "  0.5198891758918762,\n",
       "  0.5798922777175903,\n",
       "  1.3860236406326294,\n",
       "  -0.7410112619400024,\n",
       "  -0.01502999197691679,\n",
       "  -0.859950840473175,\n",
       "  -0.8393484950065613,\n",
       "  1.2057209014892578,\n",
       "  -1.110448956489563,\n",
       "  0.8812150955200195,\n",
       "  -0.565190851688385,\n",
       "  0.422743558883667,\n",
       "  0.47991740703582764,\n",
       "  -0.3085382580757141,\n",
       "  0.02662692591547966,\n",
       "  -0.6914080381393433,\n",
       "  -0.2756742537021637,\n",
       "  -0.2603461444377899,\n",
       "  -0.39808523654937744,\n",
       "  0.4887728691101074,\n",
       "  -0.862373948097229,\n",
       "  0.2987501621246338,\n",
       "  -0.1499234288930893,\n",
       "  -0.23564420640468597,\n",
       "  -0.21205800771713257,\n",
       "  -0.4081459641456604,\n",
       "  0.3540235459804535,\n",
       "  0.39122483134269714,\n",
       "  0.5496484637260437,\n",
       "  0.9312937259674072,\n",
       "  -0.6679080724716187,\n",
       "  0.39037176966667175,\n",
       "  -0.4586640000343323,\n",
       "  0.6480569243431091,\n",
       "  -0.18704143166542053,\n",
       "  -0.9603462219238281,\n",
       "  0.039901476353406906,\n",
       "  0.8586363196372986,\n",
       "  0.21443793177604675,\n",
       "  0.9269801378250122,\n",
       "  -0.3764827847480774,\n",
       "  -0.5074746012687683,\n",
       "  0.6536716818809509,\n",
       "  -0.4308583438396454,\n",
       "  -0.5046201348304749,\n",
       "  -0.6632967591285706,\n",
       "  0.19507603347301483,\n",
       "  0.3161529004573822,\n",
       "  -0.5282096862792969,\n",
       "  1.0443859100341797,\n",
       "  -0.5574413537979126,\n",
       "  0.7964830994606018,\n",
       "  -0.15707814693450928,\n",
       "  0.2848125398159027,\n",
       "  -0.41098737716674805,\n",
       "  -0.15583263337612152,\n",
       "  0.1843939870595932,\n",
       "  -0.3928409814834595,\n",
       "  0.6546974778175354,\n",
       "  -0.32187992334365845,\n",
       "  0.9976159334182739,\n",
       "  0.6478789448738098,\n",
       "  -0.6833112239837646,\n",
       "  0.1444237232208252,\n",
       "  0.10322097688913345,\n",
       "  -0.5746797323226929,\n",
       "  0.13161608576774597,\n",
       "  -1.0087478160858154,\n",
       "  0.2139674723148346,\n",
       "  0.6047424674034119,\n",
       "  -0.23739631474018097,\n",
       "  1.1844007968902588,\n",
       "  -0.13109655678272247,\n",
       "  0.29106372594833374,\n",
       "  -0.2695540487766266,\n",
       "  -0.06855807453393936,\n",
       "  -0.38863620162010193,\n",
       "  0.8296135663986206,\n",
       "  0.11283984780311584,\n",
       "  -0.10931774228811264,\n",
       "  -0.5016387104988098,\n",
       "  0.4930151700973511,\n",
       "  0.373173326253891,\n",
       "  -0.2976105809211731,\n",
       "  0.17419341206550598,\n",
       "  -0.014488596469163895,\n",
       "  0.16160903871059418,\n",
       "  -0.7646825909614563,\n",
       "  0.35238105058670044,\n",
       "  -0.43851327896118164,\n",
       "  -0.32615092396736145,\n",
       "  -0.15669149160385132,\n",
       "  0.3877356946468353,\n",
       "  0.5301096439361572,\n",
       "  0.059541165828704834,\n",
       "  -0.04622675105929375,\n",
       "  0.13522659242153168,\n",
       "  1.189355492591858,\n",
       "  -0.26314717531204224,\n",
       "  0.23482997715473175,\n",
       "  -0.5433183312416077,\n",
       "  0.31061264872550964,\n",
       "  0.7635578513145447,\n",
       "  0.8897888660430908,\n",
       "  -0.6013981103897095,\n",
       "  -3.5432138442993164,\n",
       "  -0.474951833486557,\n",
       "  0.7719078660011292,\n",
       "  0.13822492957115173,\n",
       "  -0.9086863994598389,\n",
       "  -1.7950527667999268,\n",
       "  0.5621689558029175,\n",
       "  0.32913291454315186,\n",
       "  0.18593965470790863,\n",
       "  0.44660988450050354,\n",
       "  -0.5258122682571411,\n",
       "  -0.38336971402168274,\n",
       "  -1.3890639543533325,\n",
       "  0.2779104709625244,\n",
       "  0.22792436182498932,\n",
       "  0.9065536856651306,\n",
       "  0.05217347294092178,\n",
       "  -0.029100745916366577,\n",
       "  -0.22179996967315674,\n",
       "  0.13113008439540863,\n",
       "  0.14094920456409454,\n",
       "  0.20952582359313965,\n",
       "  -0.01691490411758423,\n",
       "  0.11020518839359283,\n",
       "  0.0317227728664875,\n",
       "  0.20544226467609406,\n",
       "  0.3603578209877014,\n",
       "  -0.20428267121315002,\n",
       "  -0.3081955313682556,\n",
       "  -0.3296871483325958,\n",
       "  -0.17712824046611786,\n",
       "  0.786920428276062,\n",
       "  0.37258249521255493,\n",
       "  -0.9532531499862671,\n",
       "  -0.4103173613548279,\n",
       "  -1.255228877067566,\n",
       "  1.3399732112884521,\n",
       "  -0.15847381949424744,\n",
       "  -0.013051473535597324,\n",
       "  -0.9878419637680054,\n",
       "  0.08285224437713623,\n",
       "  -0.47259578108787537,\n",
       "  0.7371739149093628,\n",
       "  -0.011927857995033264,\n",
       "  -0.41981157660484314,\n",
       "  -0.7665438652038574,\n",
       "  -0.04395478218793869,\n",
       "  -1.0556575059890747,\n",
       "  1.019712209701538,\n",
       "  -0.020038478076457977,\n",
       "  1.8185148239135742,\n",
       "  -0.724476158618927,\n",
       "  -0.2450123280286789,\n",
       "  -0.3068181276321411,\n",
       "  -0.386674165725708,\n",
       "  -0.6760514974594116,\n",
       "  1.3044037818908691,\n",
       "  -0.16056117415428162,\n",
       "  0.7497411370277405,\n",
       "  -0.5309123992919922,\n",
       "  -1.5016038417816162,\n",
       "  0.8493679165840149,\n",
       "  1.0895137786865234,\n",
       "  0.749552309513092,\n",
       "  -0.2520168423652649,\n",
       "  0.0942903533577919,\n",
       "  -0.34583935141563416,\n",
       "  -0.6996520757675171,\n",
       "  0.253618061542511,\n",
       "  0.7452512979507446,\n",
       "  -0.6182090044021606,\n",
       "  -0.16532355546951294,\n",
       "  -0.9694309234619141,\n",
       "  0.052315231412649155,\n",
       "  0.1667608916759491,\n",
       "  -0.9742806553840637,\n",
       "  -0.6005958914756775,\n",
       "  -0.09588605910539627,\n",
       "  0.6181309223175049,\n",
       "  0.34229934215545654,\n",
       "  -0.0563129261136055,\n",
       "  -0.13712777197360992,\n",
       "  0.6070481538772583,\n",
       "  0.030161499977111816,\n",
       "  0.4538669288158417,\n",
       "  0.0426262766122818,\n",
       "  -0.38741427659988403,\n",
       "  1.0495355129241943,\n",
       "  0.24813774228096008,\n",
       "  0.5385233163833618,\n",
       "  -0.3933638036251068,\n",
       "  -0.2339733988046646,\n",
       "  -0.9316674470901489,\n",
       "  1.188890814781189,\n",
       "  -0.18452471494674683,\n",
       "  0.48619508743286133,\n",
       "  -0.6514583230018616,\n",
       "  -0.12214121967554092,\n",
       "  0.4952220916748047,\n",
       "  -0.411018431186676,\n",
       "  -1.3372806310653687,\n",
       "  0.3331352770328522,\n",
       "  -0.11730608344078064,\n",
       "  0.4394437372684479,\n",
       "  -0.0405149832367897,\n",
       "  0.12021083384752274,\n",
       "  2.434962511062622,\n",
       "  -0.6648941040039062,\n",
       "  -0.4736272096633911,\n",
       "  0.22171808779239655,\n",
       "  0.18896624445915222,\n",
       "  -0.6026129722595215,\n",
       "  -0.45654061436653137,\n",
       "  0.23693649470806122,\n",
       "  0.6091161370277405,\n",
       "  1.2922016382217407,\n",
       "  -0.7496978044509888,\n",
       "  0.03466775640845299,\n",
       "  0.15948212146759033,\n",
       "  -0.2183762788772583,\n",
       "  -0.28321680426597595,\n",
       "  0.49557429552078247,\n",
       "  0.24931594729423523,\n",
       "  -0.11057325452566147,\n",
       "  1.0742814540863037,\n",
       "  0.07463827729225159,\n",
       "  0.5088364481925964,\n",
       "  -0.456804484128952,\n",
       "  0.428196519613266,\n",
       "  -1.1321544647216797,\n",
       "  0.2096087485551834,\n",
       "  -0.7977246046066284,\n",
       "  -0.16758635640144348,\n",
       "  0.535618007183075,\n",
       "  0.10857054591178894,\n",
       "  -0.48061418533325195,\n",
       "  -0.052960313856601715,\n",
       "  -0.30623042583465576,\n",
       "  0.008661839179694653,\n",
       "  1.0110759735107422,\n",
       "  -0.5079790353775024,\n",
       "  0.8742189407348633,\n",
       "  0.09799487143754959,\n",
       "  -0.3142439126968384,\n",
       "  0.10305052995681763,\n",
       "  -0.17106443643569946,\n",
       "  -0.17521794140338898,\n",
       "  -0.5922989249229431,\n",
       "  -0.8074019551277161,\n",
       "  0.17455795407295227,\n",
       "  0.5903110504150391,\n",
       "  -0.8086756467819214,\n",
       "  -0.11807967722415924,\n",
       "  -0.2931821942329407,\n",
       "  -0.0004284108290448785,\n",
       "  -0.8390202522277832,\n",
       "  -0.2911221385002136,\n",
       "  0.6681102514266968,\n",
       "  -1.4010369777679443,\n",
       "  1.4995288848876953,\n",
       "  1.4327332973480225,\n",
       "  0.8660986423492432,\n",
       "  -0.22961443662643433,\n",
       "  -0.5841013789176941,\n",
       "  -0.9409380555152893,\n",
       "  -0.9805386662483215,\n",
       "  -1.6530102491378784,\n",
       "  -0.6460572481155396,\n",
       "  0.03319624438881874,\n",
       "  1.7558245658874512,\n",
       "  -0.4501768946647644,\n",
       "  -0.05266396328806877,\n",
       "  -0.1556587517261505,\n",
       "  0.703151285648346,\n",
       "  0.9123957753181458,\n",
       "  -0.6667776107788086,\n",
       "  -0.4168824851512909,\n",
       "  -0.3539588749408722,\n",
       "  -1.307481050491333,\n",
       "  1.0372138023376465,\n",
       "  0.6340735554695129,\n",
       "  1.2048923969268799,\n",
       "  0.10104013979434967,\n",
       "  -0.2349366396665573,\n",
       "  0.6814184784889221,\n",
       "  -0.7727004289627075,\n",
       "  -0.5821892023086548,\n",
       "  0.7830025553703308,\n",
       "  -0.30609822273254395,\n",
       "  0.8518140912055969,\n",
       "  0.26617830991744995,\n",
       "  -0.08131290227174759,\n",
       "  -1.1530388593673706,\n",
       "  -0.04695504903793335,\n",
       "  -0.5846274495124817,\n",
       "  -0.7309154272079468,\n",
       "  -0.2121995985507965,\n",
       "  0.4743034243583679,\n",
       "  -0.16365014016628265,\n",
       "  1.0301170349121094,\n",
       "  -0.16053128242492676,\n",
       "  0.16996575891971588,\n",
       "  0.4903523325920105,\n",
       "  -0.35665345191955566,\n",
       "  -0.027812045067548752,\n",
       "  0.37977737188339233,\n",
       "  0.6205717325210571,\n",
       "  1.2362849712371826,\n",
       "  -1.165265679359436,\n",
       "  0.6720974445343018,\n",
       "  -0.45566946268081665]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.embed([\"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cached data:\n",
    "# wrapper.embeddings_training\n",
    "# wrapper.embeddings_testing\n",
    "# wrapper.projector\n",
    "# wrapper.projections_training\n",
    "# wrapper.projections_testing\n",
    "# wrapper.salience_maps\n",
    "# wrapper.clusters\n",
    "# wrapper.cluster_profiles\n",
    "# wrapper.cluster_words_freqs\n",
    "# wrapper.cluster_class_word_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tx2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c8ebf232dea6577f6ceae92998c8d758983422845017e0e774d4bf7f66f7ab9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
